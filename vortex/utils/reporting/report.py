from datetime import datetime
import torch
import onnx
from vortex.utils.profiler.resource import get_uname, get_cpu_info,get_cpu_scaling, get_gpu_info
from pathlib import Path
from typing import Tuple

## top-level template
report_template = \
"""
## Validation Report
This report is autogenerated from {uname}
- Experiment name : {experiment_name}
- Dataset Info : 
    - Dataset Name : {dataset_name}
    - Dataset Split : {dataset_split}
- **Predictor class** : `{predictor_classname}`
- Test date : {test_date}

Note :  

- CPU resource was monitored using [psutil](https://psutil.readthedocs.io/en/latest/)
- GPU reources was monitored using [gpustat](https://pypi.python.org/pypi/gpustat)
- Monitoring session was performed on the whole evaluation loop while `timedata` was measured only for each `predict`
- It is not guaranteed that CPU and GPU was only used by this process, this may or may not satisfy your requirements


## Environment
### Libraries
- `torch` : {torch_version}
- `onnx` : {onnx_version}
- `vortex` : **TODO**
### CPU   
{cpu_info}

{cpu_scaling_governance}

### GPU   
{gpu_info}

## Validation Args.
| Name | Value |
| ---- | :---: |
{validation_args}

<div style="page-break-after: always;"></div>

## Summary
{summary_report}

<div style="page-break-after: always;"></div>

## Metrics
{metrics_report}
"""

device_summary_report_template = \
"""
### [eval @{device_name}](#Metrics-@{device_name})
| | |
| -- | :--: |
{device_summary_report}
"""

## per-device metric report, including assets and resource
## for top-level report
device_metric_report_template = \
"""
### Metrics @{device_name}
| Metric Name              | Value  |
| ------------------------ | :----: |
{metric_report}

### Assets @{device_name}
{metric_assets}

<div style="page-break-after: always;"></div>

### Resource @{device_name}
{resource_report}

<div style="page-break-after: always;"></div>

"""

## metric report for per-device metric report
metric_report_template = \
"""| {metric_name} | {metric_value} |"""

## metric asset for per-device metric report
metric_assets_template = \
"""
- {metric_name}
  {metric_asset}
"""

## resource report for per-device metric report
resource_template_report = \
"""
- {resource_name}
  {resource_path}
"""

## assuming all images in the same directory, and md file will be put in the same directory
to_md_image = lambda image : "![{image}](assets/{image})".format_map(dict(image=Path(image).name))

def generate_reports(eval_results : dict, output_directory, experiment_name,dataset_info : Tuple, metric_assets : dict, resource_usage : dict, resources : dict, validation_args : dict, predictor_classname="TODO", filename_suffix="_validation") :
    now = str(datetime.now())
    validation_args = '\n'.join(
        metric_report_template.format_map(dict(metric_name=name,metric_value=value)) \
            for name, value in validation_args.items()
    )

    cpu_scaling = get_cpu_scaling()
    cpu_scaling_governance = '| {} |\n | {} |\n | {} |'.format(
        ' | '.join('`cpu{}`'.format(i) for i in range(len(cpu_scaling))),
        ' | '.join(':--:' for _ in range(len(cpu_scaling))),
        ' | '.join('`{}`'.format(scale) for scale in cpu_scaling)
    )
    env = dict(
        test_date=now,
        experiment_name=experiment_name,
        dataset_name=dataset_info[1],
        dataset_split=dataset_info[0],
        torch_version=torch.__version__,
        onnx_version=onnx.__version__,
        cpu_info=get_cpu_info(),
        gpu_info=get_gpu_info(),
        uname=get_uname(),
        predictor_classname=predictor_classname,
        validation_args=validation_args,
        cpu_scaling_governance=cpu_scaling_governance,
    )
    metric_reports = []
    summary_reports = []

    for device, device_metrics in eval_results.items() :
        device_metric_report = '\n'.join(
            metric_report_template.format_map(dict(
                metric_name=name, metric_value=value
            )) for name, value in device_metrics.items()
        )
        assert device in metric_assets
        device_assets_report = '\n'.join(
            metric_assets_template.format_map(dict(
                metric_name=name, metric_asset=to_md_image(asset)
            )) for name, asset in metric_assets[device].items()
        )
        assert device in resources
        device_resource_report = '\n'.join(
            resource_template_report.format_map(dict(
                resource_name=name, resource_path=to_md_image(path)
            )) for name, path in resources[device].items()
        )
    
        def resource_dict_to_str(d) :
            format_dict = lambda x : '{name} : {value}'.format_map(dict(name=x['name'], value=x['memory']))
            formatted_str = ', '.join(map(format_dict, d)) if isinstance(d, list) else str(d)
            return formatted_str

        device_summary_report = '\n'.join([
            *[metric_report_template.format_map(dict(
                metric_name=name, metric_value=value
            )) for name, value in device_metrics.items()],
            *[metric_report_template.format_map(dict(
                metric_name=name, metric_value=resource_dict_to_str(value)
            )) for name, value in resource_usage[device].items()]
        ])
        
        metric_reports.append(device_metric_report_template.format_map(dict(
            device_name=device, 
            metric_report=device_metric_report,
            metric_assets=device_assets_report, 
            resource_report=device_resource_report,
        )))
        summary_reports.append(device_summary_report_template.format_map(dict(
            device_name=device,
            device_summary_report=device_summary_report,
        )))
    report = report_template.format_map(dict(
        metrics_report='\n'.join(metric_reports),
        summary_report='\n'.join(summary_reports), **env   
    )).replace('\n', '  \n')
    output_file = Path(output_directory).joinpath('{}{}.md'.format(experiment_name, filename_suffix))
    with open(output_file,'w+') as f:
        f.write(report)