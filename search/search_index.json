{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Vortex Documentation \u00b6 A Deep Learning Model Development Framework for Computer Vision Version : 0.2.1 Overview \u00b6 Vortex (a.k.a Visual Cortex) is a computer deep learning framework based on Pytorch that provides end-to-end model development. It utilizes a single experiment file in YAML format (and an optional YAML file for hyperparameter optimization) to navigate all of the pipelines and provides complete development environment which consists of the following pipelines : Training Validation Prediction Hyperparameters Optimization Additionally, it also support exporting Pytorch model into graph Intermediate Representation (IR) and utilize it on specific runtime by the following process : Graph Export IR Validation IR Prediction Currently we support deep learning model development on computer vision of the following task: Image Classification Object Detection ( UNVERIFIED YET, MAY PRODUCE BAD RESULTS ) Multiple Object Detection Single Object Detection with Landmarks Highlights \u00b6 Easy CLI usage Modular design, reusable components Various architecture support 50 + infamous backbone networks Classification and Detection architecture support Integration with image augmentation library ( Albumentations ) Integration with hyperparameter optimization library ( Optuna ) Integration with 3rd party experiment logger ( Comet.ml ) Graph export to Torchscript and ONNX Visual report of model's performance and resource usage, see this example Installation \u00b6 Vortex consists of 2 packages: Vortex development package ( vortex.development ) This package contain full vortex pipelines and main development features with all of it's dependencies Vortex runtime package ( vortex.runtime ) This package only contain minimal dependencies to run the exported vortex model (IR graph). There are two IR graph format supported by vortex, torchscript or onnx. You could choose to install only minimal dependencies packages if you want to support only one of them, or you could also install all of them with extra dependency option: - all - torchscript - onnxruntime Vortex is tested and developed using Python3.6 Vortex Development \u00b6 With Docker \u00b6 Make sure you have installed nvidia-docker2 if you want to use your nvidia gpus, if you haven't follow this guide . You pull the official image from our docker hub: docker pull nodefluxio/vortex:latest Or if you want to build your self: docker build --target=development -t vortex:dev -f dockerfiles/vortex.dockerfile . From Source \u00b6 Install package dependencies: apt update apt install -y libsm6 libxext6 libxrender-dev ffmpeg \\ x264 libx264-dev libsm6 git sqlite3 \\ libsqlite3-dev graphviz pciutils Make sure you have Python3.6 installed, or if you use Ubuntu 18.04 you can install them by: apt install -y python3.6 or, you can download the Python 3.6 release here . Then clone vortex repo to your local directory: git clone https://github.com/nodefluxio/vortex.git cd vortex git checkout v0.2.0 You could choose either one of the following environment to install vortex: Using pip \u00b6 Make sure you have pip installed for your python executable, if not follow this guide . It's important to be noted that vortex.development package also depends on vortex.runtime package, so you need to install both packages: pip install ./src/runtime[all] pip install ./src/development Or if you want to install vortex with additional optuna visualization support: pip install 'src/development[optuna_vis]' To check whether the installation is succesful, you can run: python3.6 -c 'import vortex.development' python3.6 -c 'import vortex.runtime' Using conda \u00b6 Make sure you have conda installed, if not follow this guide . Create new environment and activate it: conda create --name vortex python=3.6 conda activate vortex Install pytorch and the supporting package: conda install -c pytorch -y pytorch=1.6 torchvision=0.7 cudatoolkit=10.2 Then you could install vortex with: pip install ./src/runtime[all] ./src/development Check the installation with: python -c 'import vortex.development' python -c 'import vortex.runtime' Vortex Runtime \u00b6 There three option for the runtime package installation to support either one of the IR graph runtime or to support both of them. The options are: - onnxruntime - torchscript - all With Docker \u00b6 Make sure you have installed nvidia-docker2 if you want to use your nvidia gpus, if you haven't follow this guide . You pull the official image from our docker hub for runtime: docker pull nodefluxio/vortex:runtime-all-0.2.1 There are runtime image tags you can choose from the repository, all the tags that starts with runtime- is the image for vortex runtime. For example if you choose to get the image with onnxruntime support: docker pull nodefluxio/vortex:runtime-onnxruntime-0.2.1 Or if you want to build your self: docker build --target=runtime -t vortex:runtime -f dockerfiles/vortex.dockerfile . You could also specify the extra dependency to support specific IR graph using the build argument RUNTIME_TYPE , for example with onnxruntime : docker build --target=runtime --build-arg RUNTIME_TYPE=onnxruntime -t vortex:runtime-onnx -f dockerfiles/vortex.dockerfile . From Source \u00b6 Install package dependencies: apt update apt install -y libsm6 libxext6 libxrender-dev ffmpeg \\ x264 libx264-dev libsm6 git sqlite3 \\ libsqlite3-dev graphviz pciutils Make sure you have Python3.6 installed, or if you use Ubuntu 18.04 you can install them by: apt install -y python3.6 or, you can download the Python 3.6 release here . Then clone vortex repo to your local directory: git clone https://github.com/nodefluxio/vortex.git && cd vortex git checkout v0.2.0 You could choose either one of the following environment to install vortex: Using pip \u00b6 Choose one dependency options that suits your need, for example if you want to install specific dependencies for onnxruntime : pip install ./src/runtime[onnxruntime] Or, if you want have support for all available runtime: pip install ./src/runtime[all] To check the installation, you can run: python3.6 -c 'import vortex.runtime' Using conda \u00b6 Make sure you have conda installed, if not follow this guide . Create new environment and activate it: conda create --name vortex-rt python=3.6 conda activate vortex-rt Install cudatoolkit package: conda install -c pytorch -y cudatoolkit=10.2 Choose one of the dependency that suits your need, for example if you only want to support onnxruntime : pip install ./src/runtime[onnxruntime] Or, if you want to support all available runtime: pip install ./src/runtime[all] Check the installation with: python -c 'import vortex.runtime' Getting Started \u00b6 Vortex utilizes a certain standard to allow seamless integration between pipelines. In this guide, we will show you how to integrate your dataset/use the built-in ones , how to build the experiment file , and how to utilize and propagate both items to all of Vortex pipelines. Developing Vortex Model \u00b6 The first step is dataset integration, it is recommended for you to check the built-in datasets section in order to find suitable setting for your dataset. For example, you can use torchvision's ImageFolder to integrate a classification dataset. However, if you didn't find any suitable internal integration, you can follow dataset integration section to make your own integration point Next, we need to build the experiment file , please follow the experiment file configuration section At this point, you should've already prepared your experiment file and your dataset . You can now run the training pipeline. See training pipeline section for further instructions. After receiving Vortex model from training pipeline, you can either do : measure your model's performance using validation pipeline , or directly use the model in your script using prediction pipeline API , or further optimize your model by converting it into Intermediate Representation using graph export pipeline If you choose to export your model, once you have the Vortex IR model, you can either do : measure your IR model's performance using IR validation pipeline , or directly use the IR model in your script using IR prediction pipeline API develop the production-level inferencing code by installing only the Vortex runtime package and using runtime API to make prediction. Hyperparameter Optimization \u00b6 Now, once you've accustomed with Vortex pipelines, you can explore the use of hypopt pipeline to find the best hyperparameter setting for your model. Basically To do that, you can follow the guide below : Prepare hyperparameter configuration file. You can check hypopt config file section to create it Make sure all requirement related to objective is already met. For example, if you want to use ValidationObjective , you need to check whether you've already prepared the requirements of validation pipeline Run the hypopt pipeline, see hypopt pipeline section Once you get the optimal hyperparameters value, you can use it with the corresponding pipeline","title":"Home"},{"location":"#vortex-documentation","text":"A Deep Learning Model Development Framework for Computer Vision Version : 0.2.1","title":"Vortex Documentation"},{"location":"#overview","text":"Vortex (a.k.a Visual Cortex) is a computer deep learning framework based on Pytorch that provides end-to-end model development. It utilizes a single experiment file in YAML format (and an optional YAML file for hyperparameter optimization) to navigate all of the pipelines and provides complete development environment which consists of the following pipelines : Training Validation Prediction Hyperparameters Optimization Additionally, it also support exporting Pytorch model into graph Intermediate Representation (IR) and utilize it on specific runtime by the following process : Graph Export IR Validation IR Prediction Currently we support deep learning model development on computer vision of the following task: Image Classification Object Detection ( UNVERIFIED YET, MAY PRODUCE BAD RESULTS ) Multiple Object Detection Single Object Detection with Landmarks","title":"Overview"},{"location":"#highlights","text":"Easy CLI usage Modular design, reusable components Various architecture support 50 + infamous backbone networks Classification and Detection architecture support Integration with image augmentation library ( Albumentations ) Integration with hyperparameter optimization library ( Optuna ) Integration with 3rd party experiment logger ( Comet.ml ) Graph export to Torchscript and ONNX Visual report of model's performance and resource usage, see this example","title":"Highlights"},{"location":"#installation","text":"Vortex consists of 2 packages: Vortex development package ( vortex.development ) This package contain full vortex pipelines and main development features with all of it's dependencies Vortex runtime package ( vortex.runtime ) This package only contain minimal dependencies to run the exported vortex model (IR graph). There are two IR graph format supported by vortex, torchscript or onnx. You could choose to install only minimal dependencies packages if you want to support only one of them, or you could also install all of them with extra dependency option: - all - torchscript - onnxruntime Vortex is tested and developed using Python3.6","title":"Installation"},{"location":"#vortex-development","text":"","title":"Vortex Development"},{"location":"#with-docker","text":"Make sure you have installed nvidia-docker2 if you want to use your nvidia gpus, if you haven't follow this guide . You pull the official image from our docker hub: docker pull nodefluxio/vortex:latest Or if you want to build your self: docker build --target=development -t vortex:dev -f dockerfiles/vortex.dockerfile .","title":"With Docker"},{"location":"#from-source","text":"Install package dependencies: apt update apt install -y libsm6 libxext6 libxrender-dev ffmpeg \\ x264 libx264-dev libsm6 git sqlite3 \\ libsqlite3-dev graphviz pciutils Make sure you have Python3.6 installed, or if you use Ubuntu 18.04 you can install them by: apt install -y python3.6 or, you can download the Python 3.6 release here . Then clone vortex repo to your local directory: git clone https://github.com/nodefluxio/vortex.git cd vortex git checkout v0.2.0 You could choose either one of the following environment to install vortex:","title":"From Source"},{"location":"#using-pip","text":"Make sure you have pip installed for your python executable, if not follow this guide . It's important to be noted that vortex.development package also depends on vortex.runtime package, so you need to install both packages: pip install ./src/runtime[all] pip install ./src/development Or if you want to install vortex with additional optuna visualization support: pip install 'src/development[optuna_vis]' To check whether the installation is succesful, you can run: python3.6 -c 'import vortex.development' python3.6 -c 'import vortex.runtime'","title":"Using pip"},{"location":"#using-conda","text":"Make sure you have conda installed, if not follow this guide . Create new environment and activate it: conda create --name vortex python=3.6 conda activate vortex Install pytorch and the supporting package: conda install -c pytorch -y pytorch=1.6 torchvision=0.7 cudatoolkit=10.2 Then you could install vortex with: pip install ./src/runtime[all] ./src/development Check the installation with: python -c 'import vortex.development' python -c 'import vortex.runtime'","title":"Using conda"},{"location":"#vortex-runtime","text":"There three option for the runtime package installation to support either one of the IR graph runtime or to support both of them. The options are: - onnxruntime - torchscript - all","title":"Vortex Runtime"},{"location":"#with-docker_1","text":"Make sure you have installed nvidia-docker2 if you want to use your nvidia gpus, if you haven't follow this guide . You pull the official image from our docker hub for runtime: docker pull nodefluxio/vortex:runtime-all-0.2.1 There are runtime image tags you can choose from the repository, all the tags that starts with runtime- is the image for vortex runtime. For example if you choose to get the image with onnxruntime support: docker pull nodefluxio/vortex:runtime-onnxruntime-0.2.1 Or if you want to build your self: docker build --target=runtime -t vortex:runtime -f dockerfiles/vortex.dockerfile . You could also specify the extra dependency to support specific IR graph using the build argument RUNTIME_TYPE , for example with onnxruntime : docker build --target=runtime --build-arg RUNTIME_TYPE=onnxruntime -t vortex:runtime-onnx -f dockerfiles/vortex.dockerfile .","title":"With Docker"},{"location":"#from-source_1","text":"Install package dependencies: apt update apt install -y libsm6 libxext6 libxrender-dev ffmpeg \\ x264 libx264-dev libsm6 git sqlite3 \\ libsqlite3-dev graphviz pciutils Make sure you have Python3.6 installed, or if you use Ubuntu 18.04 you can install them by: apt install -y python3.6 or, you can download the Python 3.6 release here . Then clone vortex repo to your local directory: git clone https://github.com/nodefluxio/vortex.git && cd vortex git checkout v0.2.0 You could choose either one of the following environment to install vortex:","title":"From Source"},{"location":"#using-pip_1","text":"Choose one dependency options that suits your need, for example if you want to install specific dependencies for onnxruntime : pip install ./src/runtime[onnxruntime] Or, if you want have support for all available runtime: pip install ./src/runtime[all] To check the installation, you can run: python3.6 -c 'import vortex.runtime'","title":"Using pip"},{"location":"#using-conda_1","text":"Make sure you have conda installed, if not follow this guide . Create new environment and activate it: conda create --name vortex-rt python=3.6 conda activate vortex-rt Install cudatoolkit package: conda install -c pytorch -y cudatoolkit=10.2 Choose one of the dependency that suits your need, for example if you only want to support onnxruntime : pip install ./src/runtime[onnxruntime] Or, if you want to support all available runtime: pip install ./src/runtime[all] Check the installation with: python -c 'import vortex.runtime'","title":"Using conda"},{"location":"#getting-started","text":"Vortex utilizes a certain standard to allow seamless integration between pipelines. In this guide, we will show you how to integrate your dataset/use the built-in ones , how to build the experiment file , and how to utilize and propagate both items to all of Vortex pipelines.","title":"Getting Started"},{"location":"#developing-vortex-model","text":"The first step is dataset integration, it is recommended for you to check the built-in datasets section in order to find suitable setting for your dataset. For example, you can use torchvision's ImageFolder to integrate a classification dataset. However, if you didn't find any suitable internal integration, you can follow dataset integration section to make your own integration point Next, we need to build the experiment file , please follow the experiment file configuration section At this point, you should've already prepared your experiment file and your dataset . You can now run the training pipeline. See training pipeline section for further instructions. After receiving Vortex model from training pipeline, you can either do : measure your model's performance using validation pipeline , or directly use the model in your script using prediction pipeline API , or further optimize your model by converting it into Intermediate Representation using graph export pipeline If you choose to export your model, once you have the Vortex IR model, you can either do : measure your IR model's performance using IR validation pipeline , or directly use the IR model in your script using IR prediction pipeline API develop the production-level inferencing code by installing only the Vortex runtime package and using runtime API to make prediction.","title":"Developing Vortex Model"},{"location":"#hyperparameter-optimization","text":"Now, once you've accustomed with Vortex pipelines, you can explore the use of hypopt pipeline to find the best hyperparameter setting for your model. Basically To do that, you can follow the guide below : Prepare hyperparameter configuration file. You can check hypopt config file section to create it Make sure all requirement related to objective is already met. For example, if you want to use ValidationObjective , you need to check whether you've already prepared the requirements of validation pipeline Run the hypopt pipeline, see hypopt pipeline section Once you get the optimal hyperparameters value, you can use it with the corresponding pipeline","title":"Hyperparameter Optimization"},{"location":"api/vortex.development.core.factory/","text":"vortex.development.core.factory \u00b6 Functions \u00b6 create_model \u00b6 Function to create model and it's signature components. E.g. loss function, collate function, etc def create_model( model_config : easydict.EasyDict, state_dict : typing.Union[str, dict, pathlib.Path] = None, stage : str = 'train', ) Arguments : model_config EasyDict - Experiment file configuration at model section, as EasyDict object state_dict Union[str, dict, Path], optional - [description]. model Pytorch state dictionary or commonly known as weight, can be provided as the path to the file, or the returned dictionary object from torch.load . If this param is provided, it will override checkpoint specified in the experiment file. Defaults to None. stage str, optional - If set to 'train', this will enforce that the model must have loss and collate_fn attributes, hence it will make sure model can be used for training stage. If set to 'validate' it will ignore those requirements but cannot be used in training pipeline, but may still valid for other pipelines. Defaults to 'train'. Returns : EasyDict - The dictionary containing the model's components Raises : TypeError - Raises if the provided stage not in 'train' or 'validate' Examples : The dictionary returned will contain several keys : network : Pytorch model's object which inherit torch.nn.Module class. preprocess : model's preprocessing module postprocess : model's postprocessing module loss : if provided, module for model's loss function collate_fn : if provided, module to be embedded to dataloader's collate_fn function to modify dataset label's format into desirable format that can be accepted by loss components from vortex.development.core.factory import create_model from easydict import EasyDict model_config = EasyDict({ 'name': 'softmax', 'network_args': { 'backbone': 'efficientnet_b0', 'n_classes': 10, 'pretrained_backbone': True, }, 'preprocess_args': { 'input_size': 32, 'input_normalization': { 'mean': [0.4914, 0.4822, 0.4465], 'std': [0.2023, 0.1994, 0.2010], 'scaler': 255, } }, 'loss_args': { 'reduction': 'mean' } }) model_components = create_model( model_config = model_config ) print(model_components.keys()) create_dataloader \u00b6 Function to create iterable data loader object def create_dataloader( dataloader_config : easydict.EasyDict, dataset_config : easydict.EasyDict, preprocess_config : easydict.EasyDict, stage : str = 'train', collate_fn : typing.Union[typing.Callable, str, NoneType] = None, ) Arguments : dataloader_config EasyDict - Experiment file configuration at dataloader section, as EasyDict object dataset_config EasyDict - Experiment file configuration at dataset section, as EasyDict object preprocess_config EasyDict - Experiment file configuration at model.preprocess_args section, as EasyDict object stage str, optional - Specify the experiment stage, either 'train' or 'validate'. Defaults to 'train'. collate_fn Union[Callable,str,None], optional - Collate function to reformat batch data serving. Defaults to None. Returns : Type[Iterable] - Iterable dataloader object which served batch of data in every iteration Raises : TypeError - Raises if provided collate_fn type is neither 'str' (registered in Vortex), Callable (custom function), or None RuntimeError - Raises if specified 'dataloader' module is not registered Examples : from vortex.development.core.factory import create_dataloader from easydict import EasyDict dataloader_config = EasyDict({ 'module': 'PytorchDataLoader', 'args': { 'num_workers': 1, 'batch_size': 4, 'shuffle': True, }, }) dataset_config = EasyDict({ 'train': { 'dataset': 'ImageFolder', 'args': { 'root': 'tests/test_dataset/classification/train' }, 'augmentations': [{ 'module': 'albumentations', 'args': { 'transforms': [ { 'transform' : 'RandomBrightnessContrast', 'args' : { 'p' : 0.5, 'brightness_by_max': False, 'brightness_limit': 0.1, 'contrast_limit': 0.1, } }, {'transform': 'HueSaturationValue', 'args': {}}, {'transform' : 'HorizontalFlip', 'args' : {'p' : 0.5}}, ] } }] }, }) preprocess_config = EasyDict({ 'input_size' : 224, 'input_normalization' : { 'mean' : [0.5,0.5,0.5], 'std' : [0.5, 0.5, 0.5], 'scaler' : 255 }, }) dataloader = create_dataloader(dataloader_config=dataloader_config, dataset_config=dataset_config, preprocess_config = preprocess_config, collate_fn=None) for data in dataloader: images,labels = data","title":"vortex.development.core.factory"},{"location":"api/vortex.development.core.factory/#vortexdevelopmentcorefactory","text":"","title":"vortex.development.core.factory"},{"location":"api/vortex.development.core.factory/#functions","text":"","title":"Functions"},{"location":"api/vortex.development.core.factory/#create_model","text":"Function to create model and it's signature components. E.g. loss function, collate function, etc def create_model( model_config : easydict.EasyDict, state_dict : typing.Union[str, dict, pathlib.Path] = None, stage : str = 'train', ) Arguments : model_config EasyDict - Experiment file configuration at model section, as EasyDict object state_dict Union[str, dict, Path], optional - [description]. model Pytorch state dictionary or commonly known as weight, can be provided as the path to the file, or the returned dictionary object from torch.load . If this param is provided, it will override checkpoint specified in the experiment file. Defaults to None. stage str, optional - If set to 'train', this will enforce that the model must have loss and collate_fn attributes, hence it will make sure model can be used for training stage. If set to 'validate' it will ignore those requirements but cannot be used in training pipeline, but may still valid for other pipelines. Defaults to 'train'. Returns : EasyDict - The dictionary containing the model's components Raises : TypeError - Raises if the provided stage not in 'train' or 'validate' Examples : The dictionary returned will contain several keys : network : Pytorch model's object which inherit torch.nn.Module class. preprocess : model's preprocessing module postprocess : model's postprocessing module loss : if provided, module for model's loss function collate_fn : if provided, module to be embedded to dataloader's collate_fn function to modify dataset label's format into desirable format that can be accepted by loss components from vortex.development.core.factory import create_model from easydict import EasyDict model_config = EasyDict({ 'name': 'softmax', 'network_args': { 'backbone': 'efficientnet_b0', 'n_classes': 10, 'pretrained_backbone': True, }, 'preprocess_args': { 'input_size': 32, 'input_normalization': { 'mean': [0.4914, 0.4822, 0.4465], 'std': [0.2023, 0.1994, 0.2010], 'scaler': 255, } }, 'loss_args': { 'reduction': 'mean' } }) model_components = create_model( model_config = model_config ) print(model_components.keys())","title":"create_model"},{"location":"api/vortex.development.core.factory/#create_dataloader","text":"Function to create iterable data loader object def create_dataloader( dataloader_config : easydict.EasyDict, dataset_config : easydict.EasyDict, preprocess_config : easydict.EasyDict, stage : str = 'train', collate_fn : typing.Union[typing.Callable, str, NoneType] = None, ) Arguments : dataloader_config EasyDict - Experiment file configuration at dataloader section, as EasyDict object dataset_config EasyDict - Experiment file configuration at dataset section, as EasyDict object preprocess_config EasyDict - Experiment file configuration at model.preprocess_args section, as EasyDict object stage str, optional - Specify the experiment stage, either 'train' or 'validate'. Defaults to 'train'. collate_fn Union[Callable,str,None], optional - Collate function to reformat batch data serving. Defaults to None. Returns : Type[Iterable] - Iterable dataloader object which served batch of data in every iteration Raises : TypeError - Raises if provided collate_fn type is neither 'str' (registered in Vortex), Callable (custom function), or None RuntimeError - Raises if specified 'dataloader' module is not registered Examples : from vortex.development.core.factory import create_dataloader from easydict import EasyDict dataloader_config = EasyDict({ 'module': 'PytorchDataLoader', 'args': { 'num_workers': 1, 'batch_size': 4, 'shuffle': True, }, }) dataset_config = EasyDict({ 'train': { 'dataset': 'ImageFolder', 'args': { 'root': 'tests/test_dataset/classification/train' }, 'augmentations': [{ 'module': 'albumentations', 'args': { 'transforms': [ { 'transform' : 'RandomBrightnessContrast', 'args' : { 'p' : 0.5, 'brightness_by_max': False, 'brightness_limit': 0.1, 'contrast_limit': 0.1, } }, {'transform': 'HueSaturationValue', 'args': {}}, {'transform' : 'HorizontalFlip', 'args' : {'p' : 0.5}}, ] } }] }, }) preprocess_config = EasyDict({ 'input_size' : 224, 'input_normalization' : { 'mean' : [0.5,0.5,0.5], 'std' : [0.5, 0.5, 0.5], 'scaler' : 255 }, }) dataloader = create_dataloader(dataloader_config=dataloader_config, dataset_config=dataset_config, preprocess_config = preprocess_config, collate_fn=None) for data in dataloader: images,labels = data","title":"create_dataloader"},{"location":"api/vortex.development.core.pipelines/","text":"vortex.development.core.pipelines \u00b6 Classes \u00b6 GraphExportPipeline \u00b6 Vortex Graph Export Pipeline API __init__ \u00b6 def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. Examples : from vortex.development.utils.parser import load_config from vortex.development.core.pipelines import GraphExportPipeline # Parse config config = load_config('experiments/config/example.yml') graph_exporter = GraphExportPipeline(config=config, weights='experiments/outputs/example/example.pth') run \u00b6 def run( self, example_input : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : example_input Union[str,Path], optional - path to example input image to help graph tracing. Defaults to None. Returns : EasyDict - dictionary containing status of the export process Examples : example_input = 'image1.jpg' graph_exporter = GraphExportPipeline(config=config, weights='experiments/outputs/example/example.pth') result = graph_exporter.run(example_input=example_input) HypOptPipeline \u00b6 Vortex Hyperparameters Optimization Pipeline API __init__ \u00b6 def __init__( self, config : easydict.EasyDict, optconfig : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file optconfig EasyDict - dictionary parsed from Vortex hypopt configuration file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Only used for ValidationObjective. Defaults to None. Examples : from vortex.development.core.pipelines import HypOptPipeline from vortex.development.utils.parser.loader import Loader import yaml # Parse config config_path = 'experiments/config/example.yml' optconfig_path = 'experiments/hypopt/learning_rate_search.yml' with open(config_path) as f: config_data = yaml.load(f, Loader=Loader) with open(optconfig_path) as f: optconfig_data = yaml.load(f, Loader=Loader) graph_exporter = HypOptPipeline(config=config, optconfig=optconfig) run \u00b6 def run( self, ) Returns : EasyDict - dictionary containing result of the hypopt process Examples : graph_exporter = HypOptPipeline(config=config, optconfig=optconfig) results = graph_exporter.run() PytorchPredictionPipeline \u00b6 Vortex Prediction Pipeline API for Vortex model __init__ \u00b6 def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, device : typing.Union[str, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. device Union[str,None], optional - selected device for model's computation. If None, it will use the device described in experiment file . Defaults to None. Raises : FileNotFoundError - raise error if selected 'weights' file is not found Examples : from vortex.development.core.pipelines import PytorchPredictionPipeline from vortex.development.utils.parser import load_config # Parse config config_path = 'experiments/config/example.yml' config = load_config(config_path) weights_file = 'experiments/outputs/example/example.pth' device = 'cuda' vortex_predictor = PytorchPredictionPipeline(config = config, weights = weights_file, device = device) run \u00b6 def run( self, images : typing.Union[typing.List[str], numpy.ndarray], output_coordinate_format: str = \"relative\", visualize : bool = False, dump_visual : bool = False, output_dir : typing.Union[str, pathlib.Path] = '.', **kwargs, ) Arguments : images Union[List[str],np.ndarray] - list of images path or array of image output_coordinate_format str, optional - output coordinate format, especially usefull for models that returns coordinates in the input, e.g. bounding box, landmark, etc. Available: 'relative' : the coordinate is relative to input size (have range of [0, 1]), so to visualize the output needs to be multplied by input size; 'absolute' : the coordinate is absolute to input size (range of [widht, height]). Default 'relative' . visualize bool, optional - option to return prediction visualization. Defaults to False. dump_visual bool, optional - option to dump prediction visualization. Defaults to False. output_dir Union[str,Path], optional - directory path to dump visualization. Defaults to '.' . kwargs optional - forwarded to model's forward pass, so this kwargs is placement for additional input parameters, make sure to have this if your model needs an additional inputs, e.g. score_threshold , etc. Returns : EasyDict - dictionary of prediction result Raises : TypeError - raise error if provided 'images' is not list of image path or array of images Examples : # Initialize prediction pipeline vortex_predictor=PytorchPredictionPipeline(config = config, weights = weights_file, device = device) ## OR vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime) # You can get model's required parameter by extracting model's 'input_specs' attributes input_shape = vortex_predictor.model.input_specs['input']['shape'] ## `input_specs['input']['shape']` will provide (batch_size,height,width,channel) dimension ## NOTES : PytorchPredictionPipeline can accept flexible batch size, ## however the `input_specs['input']['shape']` of the batch_size dimension ## will always set to 1, ignore this # Extract additional run() input parameters specific for each model additional_run_params = [key for key in vortex_predictor.model.input_specs.keys() if key!='input'] print(additional_run_params) ## Assume that the model is detection model ## ['score_threshold', 'iou_threshold'] << this parameter must be provided in run() arguments # Prepare batched input from image files path batch_input = ['image1.jpg','image2.jpg'] ## OR import cv2 input_size = input_shape[1] # Assume square input image1 = cv2.resize(cv2.imread('image1.jpg'), (input_size,input_size)) image2 = cv2.resize(cv2.imread('image2.jpg'), (input_size,input_size)) batch_input = np.array([image1,image2]) results = vortex_predictor.run(images=batch_input, score_threshold=0.9, iou_threshold=0.2) # Additional process : obtain class_names from model class_names = vortex_predictor.model.class_names print(class_names) IRPredictionPipeline \u00b6 Vortex Prediction Pipeline API for Vortex IR model __init__ \u00b6 def __init__( self, model : typing.Union[str, pathlib.Path], runtime : str = 'cpu', ) Arguments : model Union[str,Path] - path to Vortex IR model, file with extension '.onnx' or '.pt' runtime str, optional - backend runtime to be selected for model's computation. Defaults to 'cpu'. Examples : from vortex.development.core.pipelines import IRPredictionPipeline from vortex.development.utils.parser import load_config # Parse config model_file = 'experiments/outputs/example/example.pt' # Model file with extension '.onnx' or '.pt' runtime = 'cpu' vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime) run \u00b6 def run( self, images : typing.Union[typing.List[str], numpy.ndarray], output_coordinate_format: str = \"relative\", visualize : bool = False, dump_visual : bool = False, output_dir : typing.Union[str, pathlib.Path] = '.', **kwargs, ) Arguments : images Union[List[str],np.ndarray] - list of images path or array of image output_coordinate_format str, optional - output coordinate format, especially usefull for models that returns coordinates in the input, e.g. bounding box, landmark, etc. Available: 'relative' : the coordinate is relative to input size (have range of [0, 1]), so to visualize the output needs to be multplied by input size; 'absolute' : the coordinate is absolute to input size (range of [widht, height]). Default 'relative' . visualize bool, optional - option to return prediction visualization. Defaults to False. dump_visual bool, optional - option to dump prediction visualization. Defaults to False. output_dir Union[str,Path], optional - directory path to dump visualization. Defaults to '.' . kwargs optional - forwarded to model's forward pass, so this kwargs is placement for additional input parameters, make sure to have this if your model needs an additional inputs, e.g. score_threshold , etc. Returns : EasyDict - dictionary of prediction result Raises : TypeError - raise error if provided 'images' is not list of image path or array of images Examples : # Initialize prediction pipeline vortex_predictor=PytorchPredictionPipeline(config = config, weights = weights_file, device = device) ## OR vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime) # You can get model's required parameter by extracting model's 'input_specs' attributes input_shape = vortex_predictor.model.input_specs['input']['shape'] ## `input_specs['input']['shape']` will provide (batch_size,height,width,channel) dimension ## NOTES : PytorchPredictionPipeline can accept flexible batch size, ## however the `input_specs['input']['shape']` of the batch_size dimension ## will always set to 1, ignore this # Extract additional run() input parameters specific for each model additional_run_params = [key for key in vortex_predictor.model.input_specs.keys() if key!='input'] print(additional_run_params) ## Assume that the model is detection model ## ['score_threshold', 'iou_threshold'] << this parameter must be provided in run() arguments # Prepare batched input from image files path batch_input = ['image1.jpg','image2.jpg'] ## OR import cv2 input_size = input_shape[1] # Assume square input image1 = cv2.resize(cv2.imread('image1.jpg'), (input_size,input_size)) image2 = cv2.resize(cv2.imread('image2.jpg'), (input_size,input_size)) batch_input = np.array([image1,image2]) results = vortex_predictor.run(images=batch_input, score_threshold=0.9, iou_threshold=0.2) # Additional process : obtain class_names from model class_names = vortex_predictor.model.class_names print(class_names) TrainingPipeline \u00b6 Vortex Training Pipeline API __init__ \u00b6 def __init__( self, config : easydict.EasyDict, config_path : typing.Union[str, pathlib.Path, NoneType] = None, hypopt : bool = False, resume : bool = False, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file config_path Union[str,Path,None], optional - path to experiment file. Need to be provided for backup experiment file . Defaults to None. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. resume bool, optional - flag to resume training. Defaults to False. Raises : Exception - raise undocumented error if exist Examples : from vortex.development.utils.parser import load_config from vortex.development.core.pipelines import TrainingPipeline # Parse config config_path = 'experiments/config/example.yml' config = load_config(config_path) train_executor = TrainingPipeline(config=config, config_path=config_path, hypopt=False) run \u00b6 def run( self, save_model : bool = True, ) Arguments : save_model bool, optional - dump model's checkpoint. Defaults to True. Returns : EasyDict - dictionary containing loss, val results and learning rates history Examples : train_executor = TrainingPipeline(config=config, config_path=config_path, hypopt=False) outputs = train_executor.run() PytorchValidationPipeline \u00b6 Vortex Validation Pipeline API for Vortex model __init__ \u00b6 def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, backends : typing.Union[list, str] = [], generate_report : bool = True, hypopt : bool = False, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. backends Union[list,str], optional - device(s) to be used for validation process. If not provided, it will use the device described in experiment file . Defaults to []. generate_report bool, optional - if enabled will generate validation report in markdown format. Defaults to True. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. Examples : from vortex.development.utils.parser import load_config from vortex.development.core.pipelines import PytorchValidationPipeline # Parse config config_path = 'experiments/config/example.yml' weights_file = 'experiments/outputs/example/example.pth' backends = ['cpu','cuda'] config = load_config(config_path) validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True) run \u00b6 def run( self, batch_size : int = 1, ) Arguments : batch_size int, optional - size of validation input batch. Defaults to 1. Returns : EasyDict - dictionary containing validation metrics result Examples : # Initialize validation pipeline validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True) ## OR validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True) # Run validation process results = validation_executor.run(batch_size = 2) ## OR (for IRValidationPipeline only, PytorchValidationPipeline can accept flexible batch size) ## 'batch_size' information is embedded in model.input_specs['input']['shape'][0] batch_size = validation_executor.model.input_specs['input']['shape'][0] results = validation_executor.run(batch_size = batch_size) IRValidationPipeline \u00b6 Vortex Validation Pipeline API for Vortex IR model __init__ \u00b6 def __init__( self, config : easydict.EasyDict, model : typing.Union[str, pathlib.Path, NoneType], backends : typing.Union[list, str] = ['cpu'], generate_report : bool = True, hypopt : bool = False, ) Arguments : config EasyDict - ictionary parsed from Vortex experiment file model Union[str,Path,None] - path to Vortex IR model, file with extension '.onnx' or '.pt' backends Union[list,str], optional - runtime(s) to be used for validation process. Defaults to ['cpu']. generate_report bool, optional - if enabled will generate validation report in markdown format. Defaults to True. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. Raises : RuntimeError - raise error if the provided model file's extension is not ' .onnx' or ' .pt' Examples : from vortex.development.utils.parser import load_config from vortex.development.core.pipelines import IRValidationPipeline # Parse config config_path = 'experiments/config/example.yml' model_file = 'experiments/outputs/example/example.pt' backends = ['cpu','cuda'] config = load_config(config_path) validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True) run \u00b6 def run( self, batch_size : int = 1, ) Arguments : batch_size int, optional - size of validation input batch. Defaults to 1. Returns : EasyDict - dictionary containing validation metrics result Examples : # Initialize validation pipeline validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True) ## OR validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True) # Run validation process results = validation_executor.run(batch_size = 2) ## OR (for IRValidationPipeline only, PytorchValidationPipeline can accept flexible batch size) ## 'batch_size' information is embedded in model.input_specs['input']['shape'][0] batch_size = validation_executor.model.input_specs['input']['shape'][0] results = validation_executor.run(batch_size = batch_size)","title":"vortex.development.core.pipelines"},{"location":"api/vortex.development.core.pipelines/#vortexdevelopmentcorepipelines","text":"","title":"vortex.development.core.pipelines"},{"location":"api/vortex.development.core.pipelines/#classes","text":"","title":"Classes"},{"location":"api/vortex.development.core.pipelines/#graphexportpipeline","text":"Vortex Graph Export Pipeline API","title":"GraphExportPipeline"},{"location":"api/vortex.development.core.pipelines/#__init__","text":"def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. Examples : from vortex.development.utils.parser import load_config from vortex.development.core.pipelines import GraphExportPipeline # Parse config config = load_config('experiments/config/example.yml') graph_exporter = GraphExportPipeline(config=config, weights='experiments/outputs/example/example.pth')","title":"__init__"},{"location":"api/vortex.development.core.pipelines/#run","text":"def run( self, example_input : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : example_input Union[str,Path], optional - path to example input image to help graph tracing. Defaults to None. Returns : EasyDict - dictionary containing status of the export process Examples : example_input = 'image1.jpg' graph_exporter = GraphExportPipeline(config=config, weights='experiments/outputs/example/example.pth') result = graph_exporter.run(example_input=example_input)","title":"run"},{"location":"api/vortex.development.core.pipelines/#hypoptpipeline","text":"Vortex Hyperparameters Optimization Pipeline API","title":"HypOptPipeline"},{"location":"api/vortex.development.core.pipelines/#__init___1","text":"def __init__( self, config : easydict.EasyDict, optconfig : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file optconfig EasyDict - dictionary parsed from Vortex hypopt configuration file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Only used for ValidationObjective. Defaults to None. Examples : from vortex.development.core.pipelines import HypOptPipeline from vortex.development.utils.parser.loader import Loader import yaml # Parse config config_path = 'experiments/config/example.yml' optconfig_path = 'experiments/hypopt/learning_rate_search.yml' with open(config_path) as f: config_data = yaml.load(f, Loader=Loader) with open(optconfig_path) as f: optconfig_data = yaml.load(f, Loader=Loader) graph_exporter = HypOptPipeline(config=config, optconfig=optconfig)","title":"__init__"},{"location":"api/vortex.development.core.pipelines/#run_1","text":"def run( self, ) Returns : EasyDict - dictionary containing result of the hypopt process Examples : graph_exporter = HypOptPipeline(config=config, optconfig=optconfig) results = graph_exporter.run()","title":"run"},{"location":"api/vortex.development.core.pipelines/#pytorchpredictionpipeline","text":"Vortex Prediction Pipeline API for Vortex model","title":"PytorchPredictionPipeline"},{"location":"api/vortex.development.core.pipelines/#__init___2","text":"def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, device : typing.Union[str, NoneType] = None, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. device Union[str,None], optional - selected device for model's computation. If None, it will use the device described in experiment file . Defaults to None. Raises : FileNotFoundError - raise error if selected 'weights' file is not found Examples : from vortex.development.core.pipelines import PytorchPredictionPipeline from vortex.development.utils.parser import load_config # Parse config config_path = 'experiments/config/example.yml' config = load_config(config_path) weights_file = 'experiments/outputs/example/example.pth' device = 'cuda' vortex_predictor = PytorchPredictionPipeline(config = config, weights = weights_file, device = device)","title":"__init__"},{"location":"api/vortex.development.core.pipelines/#run_2","text":"def run( self, images : typing.Union[typing.List[str], numpy.ndarray], output_coordinate_format: str = \"relative\", visualize : bool = False, dump_visual : bool = False, output_dir : typing.Union[str, pathlib.Path] = '.', **kwargs, ) Arguments : images Union[List[str],np.ndarray] - list of images path or array of image output_coordinate_format str, optional - output coordinate format, especially usefull for models that returns coordinates in the input, e.g. bounding box, landmark, etc. Available: 'relative' : the coordinate is relative to input size (have range of [0, 1]), so to visualize the output needs to be multplied by input size; 'absolute' : the coordinate is absolute to input size (range of [widht, height]). Default 'relative' . visualize bool, optional - option to return prediction visualization. Defaults to False. dump_visual bool, optional - option to dump prediction visualization. Defaults to False. output_dir Union[str,Path], optional - directory path to dump visualization. Defaults to '.' . kwargs optional - forwarded to model's forward pass, so this kwargs is placement for additional input parameters, make sure to have this if your model needs an additional inputs, e.g. score_threshold , etc. Returns : EasyDict - dictionary of prediction result Raises : TypeError - raise error if provided 'images' is not list of image path or array of images Examples : # Initialize prediction pipeline vortex_predictor=PytorchPredictionPipeline(config = config, weights = weights_file, device = device) ## OR vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime) # You can get model's required parameter by extracting model's 'input_specs' attributes input_shape = vortex_predictor.model.input_specs['input']['shape'] ## `input_specs['input']['shape']` will provide (batch_size,height,width,channel) dimension ## NOTES : PytorchPredictionPipeline can accept flexible batch size, ## however the `input_specs['input']['shape']` of the batch_size dimension ## will always set to 1, ignore this # Extract additional run() input parameters specific for each model additional_run_params = [key for key in vortex_predictor.model.input_specs.keys() if key!='input'] print(additional_run_params) ## Assume that the model is detection model ## ['score_threshold', 'iou_threshold'] << this parameter must be provided in run() arguments # Prepare batched input from image files path batch_input = ['image1.jpg','image2.jpg'] ## OR import cv2 input_size = input_shape[1] # Assume square input image1 = cv2.resize(cv2.imread('image1.jpg'), (input_size,input_size)) image2 = cv2.resize(cv2.imread('image2.jpg'), (input_size,input_size)) batch_input = np.array([image1,image2]) results = vortex_predictor.run(images=batch_input, score_threshold=0.9, iou_threshold=0.2) # Additional process : obtain class_names from model class_names = vortex_predictor.model.class_names print(class_names)","title":"run"},{"location":"api/vortex.development.core.pipelines/#irpredictionpipeline","text":"Vortex Prediction Pipeline API for Vortex IR model","title":"IRPredictionPipeline"},{"location":"api/vortex.development.core.pipelines/#__init___3","text":"def __init__( self, model : typing.Union[str, pathlib.Path], runtime : str = 'cpu', ) Arguments : model Union[str,Path] - path to Vortex IR model, file with extension '.onnx' or '.pt' runtime str, optional - backend runtime to be selected for model's computation. Defaults to 'cpu'. Examples : from vortex.development.core.pipelines import IRPredictionPipeline from vortex.development.utils.parser import load_config # Parse config model_file = 'experiments/outputs/example/example.pt' # Model file with extension '.onnx' or '.pt' runtime = 'cpu' vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime)","title":"__init__"},{"location":"api/vortex.development.core.pipelines/#run_3","text":"def run( self, images : typing.Union[typing.List[str], numpy.ndarray], output_coordinate_format: str = \"relative\", visualize : bool = False, dump_visual : bool = False, output_dir : typing.Union[str, pathlib.Path] = '.', **kwargs, ) Arguments : images Union[List[str],np.ndarray] - list of images path or array of image output_coordinate_format str, optional - output coordinate format, especially usefull for models that returns coordinates in the input, e.g. bounding box, landmark, etc. Available: 'relative' : the coordinate is relative to input size (have range of [0, 1]), so to visualize the output needs to be multplied by input size; 'absolute' : the coordinate is absolute to input size (range of [widht, height]). Default 'relative' . visualize bool, optional - option to return prediction visualization. Defaults to False. dump_visual bool, optional - option to dump prediction visualization. Defaults to False. output_dir Union[str,Path], optional - directory path to dump visualization. Defaults to '.' . kwargs optional - forwarded to model's forward pass, so this kwargs is placement for additional input parameters, make sure to have this if your model needs an additional inputs, e.g. score_threshold , etc. Returns : EasyDict - dictionary of prediction result Raises : TypeError - raise error if provided 'images' is not list of image path or array of images Examples : # Initialize prediction pipeline vortex_predictor=PytorchPredictionPipeline(config = config, weights = weights_file, device = device) ## OR vortex_predictor=IRPredictionPipeline(model = model_file, runtime = runtime) # You can get model's required parameter by extracting model's 'input_specs' attributes input_shape = vortex_predictor.model.input_specs['input']['shape'] ## `input_specs['input']['shape']` will provide (batch_size,height,width,channel) dimension ## NOTES : PytorchPredictionPipeline can accept flexible batch size, ## however the `input_specs['input']['shape']` of the batch_size dimension ## will always set to 1, ignore this # Extract additional run() input parameters specific for each model additional_run_params = [key for key in vortex_predictor.model.input_specs.keys() if key!='input'] print(additional_run_params) ## Assume that the model is detection model ## ['score_threshold', 'iou_threshold'] << this parameter must be provided in run() arguments # Prepare batched input from image files path batch_input = ['image1.jpg','image2.jpg'] ## OR import cv2 input_size = input_shape[1] # Assume square input image1 = cv2.resize(cv2.imread('image1.jpg'), (input_size,input_size)) image2 = cv2.resize(cv2.imread('image2.jpg'), (input_size,input_size)) batch_input = np.array([image1,image2]) results = vortex_predictor.run(images=batch_input, score_threshold=0.9, iou_threshold=0.2) # Additional process : obtain class_names from model class_names = vortex_predictor.model.class_names print(class_names)","title":"run"},{"location":"api/vortex.development.core.pipelines/#trainingpipeline","text":"Vortex Training Pipeline API","title":"TrainingPipeline"},{"location":"api/vortex.development.core.pipelines/#__init___4","text":"def __init__( self, config : easydict.EasyDict, config_path : typing.Union[str, pathlib.Path, NoneType] = None, hypopt : bool = False, resume : bool = False, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file config_path Union[str,Path,None], optional - path to experiment file. Need to be provided for backup experiment file . Defaults to None. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. resume bool, optional - flag to resume training. Defaults to False. Raises : Exception - raise undocumented error if exist Examples : from vortex.development.utils.parser import load_config from vortex.development.core.pipelines import TrainingPipeline # Parse config config_path = 'experiments/config/example.yml' config = load_config(config_path) train_executor = TrainingPipeline(config=config, config_path=config_path, hypopt=False)","title":"__init__"},{"location":"api/vortex.development.core.pipelines/#run_4","text":"def run( self, save_model : bool = True, ) Arguments : save_model bool, optional - dump model's checkpoint. Defaults to True. Returns : EasyDict - dictionary containing loss, val results and learning rates history Examples : train_executor = TrainingPipeline(config=config, config_path=config_path, hypopt=False) outputs = train_executor.run()","title":"run"},{"location":"api/vortex.development.core.pipelines/#pytorchvalidationpipeline","text":"Vortex Validation Pipeline API for Vortex model","title":"PytorchValidationPipeline"},{"location":"api/vortex.development.core.pipelines/#__init___5","text":"def __init__( self, config : easydict.EasyDict, weights : typing.Union[str, pathlib.Path, NoneType] = None, backends : typing.Union[list, str] = [], generate_report : bool = True, hypopt : bool = False, ) Arguments : config EasyDict - dictionary parsed from Vortex experiment file weights Union[str,Path,None], optional - path to selected Vortex model's weight. If set to None, it will assume that final model weights exist in experiment directory . Defaults to None. backends Union[list,str], optional - device(s) to be used for validation process. If not provided, it will use the device described in experiment file . Defaults to []. generate_report bool, optional - if enabled will generate validation report in markdown format. Defaults to True. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. Examples : from vortex.development.utils.parser import load_config from vortex.development.core.pipelines import PytorchValidationPipeline # Parse config config_path = 'experiments/config/example.yml' weights_file = 'experiments/outputs/example/example.pth' backends = ['cpu','cuda'] config = load_config(config_path) validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True)","title":"__init__"},{"location":"api/vortex.development.core.pipelines/#run_5","text":"def run( self, batch_size : int = 1, ) Arguments : batch_size int, optional - size of validation input batch. Defaults to 1. Returns : EasyDict - dictionary containing validation metrics result Examples : # Initialize validation pipeline validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True) ## OR validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True) # Run validation process results = validation_executor.run(batch_size = 2) ## OR (for IRValidationPipeline only, PytorchValidationPipeline can accept flexible batch size) ## 'batch_size' information is embedded in model.input_specs['input']['shape'][0] batch_size = validation_executor.model.input_specs['input']['shape'][0] results = validation_executor.run(batch_size = batch_size)","title":"run"},{"location":"api/vortex.development.core.pipelines/#irvalidationpipeline","text":"Vortex Validation Pipeline API for Vortex IR model","title":"IRValidationPipeline"},{"location":"api/vortex.development.core.pipelines/#__init___6","text":"def __init__( self, config : easydict.EasyDict, model : typing.Union[str, pathlib.Path, NoneType], backends : typing.Union[list, str] = ['cpu'], generate_report : bool = True, hypopt : bool = False, ) Arguments : config EasyDict - ictionary parsed from Vortex experiment file model Union[str,Path,None] - path to Vortex IR model, file with extension '.onnx' or '.pt' backends Union[list,str], optional - runtime(s) to be used for validation process. Defaults to ['cpu']. generate_report bool, optional - if enabled will generate validation report in markdown format. Defaults to True. hypopt bool, optional - flag for hypopt, disable several pipeline process. Defaults to False. Raises : RuntimeError - raise error if the provided model file's extension is not ' .onnx' or ' .pt' Examples : from vortex.development.utils.parser import load_config from vortex.development.core.pipelines import IRValidationPipeline # Parse config config_path = 'experiments/config/example.yml' model_file = 'experiments/outputs/example/example.pt' backends = ['cpu','cuda'] config = load_config(config_path) validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True)","title":"__init__"},{"location":"api/vortex.development.core.pipelines/#run_6","text":"def run( self, batch_size : int = 1, ) Arguments : batch_size int, optional - size of validation input batch. Defaults to 1. Returns : EasyDict - dictionary containing validation metrics result Examples : # Initialize validation pipeline validation_executor = PytorchValidationPipeline(config=config, weights = weights_file, backends = backends, generate_report = True) ## OR validation_executor = IRValidationPipeline(config=config, model = model_file, backends = backends, generate_report = True) # Run validation process results = validation_executor.run(batch_size = 2) ## OR (for IRValidationPipeline only, PytorchValidationPipeline can accept flexible batch size) ## 'batch_size' information is embedded in model.input_specs['input']['shape'][0] batch_size = validation_executor.model.input_specs['input']['shape'][0] results = validation_executor.run(batch_size = batch_size)","title":"run"},{"location":"api/vortex.runtime/","text":"vortex.runtime \u00b6 Functions \u00b6 create_runtime_model \u00b6 Functions to create runtime model def create_runtime_model( model_path : typing.Union[str, pathlib.Path], runtime : str, output_name = ['output'], *args, **kwargs, ) Arguments : model_path Union[str, Path] - Path to Intermediate Representation (IR) model file runtime str - Backend runtime to be used, e.g. : 'cpu' or 'cuda' (Depends on available runtime options) output_name list, optional - Runtime output(s) variable name. Defaults to [\"output\"]. Returns : Type[BaseRuntime] - Runtime model objects based on IR file model's type and selected runtime Raises : RuntimeError - Raises if selected runtime is not available Examples : from vortex.runtime import create_runtime_model import numpy as np import cv2 model_path = 'tests/output_test/test_classification_pipelines/test_classification_pipelines.onnx' runtime_model = create_runtime_model( model_path = model_path, runtime = 'cpu' ) print(type(runtime_model)) ## Get model's input specifications and additional inferencing parameters print(runtime_model.input_specs) # Inferencing example input_shape = runtime_model.input_specs['input']['shape'] batch_imgs = np.array([cv2.resize(cv2.imread('tests/images/cat.jpg'),(input_shape[2],input_shape[1]))]) ## Make sure the shape of input data is equal to input specifications assert batch_imgs.shape == tuple(input_shape) ## Additional parameters can be inspected from input_specs, ## E.g. `score_threshold` or `iou_threshold` for object detection additional_input_parameters = {} ## Inference is done by utilizing __call__ method prediction_results = runtime_model(batch_imgs, **additional_input_parameters) print(prediction_results) check_available_runtime \u00b6 Function to check current environment's available runtime def check_available_runtime( ) Returns : dict - Dictionary containing status of available runtime Examples : from vortex.runtime import check_available_runtime available_runtime = check_available_runtime() print(available_runtime)","title":"vortex.runtime"},{"location":"api/vortex.runtime/#vortexruntime","text":"","title":"vortex.runtime"},{"location":"api/vortex.runtime/#functions","text":"","title":"Functions"},{"location":"api/vortex.runtime/#create_runtime_model","text":"Functions to create runtime model def create_runtime_model( model_path : typing.Union[str, pathlib.Path], runtime : str, output_name = ['output'], *args, **kwargs, ) Arguments : model_path Union[str, Path] - Path to Intermediate Representation (IR) model file runtime str - Backend runtime to be used, e.g. : 'cpu' or 'cuda' (Depends on available runtime options) output_name list, optional - Runtime output(s) variable name. Defaults to [\"output\"]. Returns : Type[BaseRuntime] - Runtime model objects based on IR file model's type and selected runtime Raises : RuntimeError - Raises if selected runtime is not available Examples : from vortex.runtime import create_runtime_model import numpy as np import cv2 model_path = 'tests/output_test/test_classification_pipelines/test_classification_pipelines.onnx' runtime_model = create_runtime_model( model_path = model_path, runtime = 'cpu' ) print(type(runtime_model)) ## Get model's input specifications and additional inferencing parameters print(runtime_model.input_specs) # Inferencing example input_shape = runtime_model.input_specs['input']['shape'] batch_imgs = np.array([cv2.resize(cv2.imread('tests/images/cat.jpg'),(input_shape[2],input_shape[1]))]) ## Make sure the shape of input data is equal to input specifications assert batch_imgs.shape == tuple(input_shape) ## Additional parameters can be inspected from input_specs, ## E.g. `score_threshold` or `iou_threshold` for object detection additional_input_parameters = {} ## Inference is done by utilizing __call__ method prediction_results = runtime_model(batch_imgs, **additional_input_parameters) print(prediction_results)","title":"create_runtime_model"},{"location":"api/vortex.runtime/#check_available_runtime","text":"Function to check current environment's available runtime def check_available_runtime( ) Returns : dict - Dictionary containing status of available runtime Examples : from vortex.runtime import check_available_runtime available_runtime = check_available_runtime() print(available_runtime)","title":"check_available_runtime"},{"location":"developer-guides/custom_backbone_integration/","text":"Implements Custom Backbone Module in Vortex \u00b6 This tutorial shows how to develop your own backbone module and integrate it to vortex . The tutorial consists of 4 steps: Vortex Backbone Module Registering builder function Pretrained weights and module restructure hack Integration with vortex CLI import torch import torch.nn as nn import torchvision as vision import warnings import vortex import vortex.development.networks as networks import vortex.development.networks.modules as vortex_modules 1. Vortex Backbone Module \u00b6 On computer vision tasks, it is often necessary to perform experiment on various backbone networks. Hence, it is useful to have adapter to multiple backbone, if desired. vortex has such adapter capability with its BackbonePoolConnector that provides interfaces to any registered backbone. The registered backbone itself should be type of Backbone class which actually an adapter for nn.Sequential to also returns its intermediate results . 2. Registering builder function \u00b6 Registering backbone is done by adding builder function to vortex . To register the function, we decorate our function with @vortex.development.networks.modules.backbones.register_backbone(model_name) def get_backbone(...) or we can call register_backbone_ directly. The function should be named get_backbone . When calling decorator, we can register multiple model on single function. And the function signature: get_backbone(model_name, pretrained, feature_type, **kwargs) -> Backbone where model_name is str representing the identifier of requested backbone model, pretrained may be str or path-like depending on the caller, and feature_type is str for customization point if necessary. In this case we will add SqueezeNet as backbone to vortex . For the sake of simplicity, we'll load existing network & pretrained from torchvision . register_backbone = vortex_modules.backbones.register_backbone @register_backbone(['squeezenetv1.1', 'squeezenetv1.0']) def get_backbone(model_name, pretrained=False, feature_type='tri_stage_fpn', n_classes=1000, **kwargs): \"\"\" create squeezenet (v1.0 & v1.1) backbone \"\"\" 3. Pretrained weights and module restructure hack \u00b6 In many cases, we do not want to develop from scratch but use existing module instead. But, this raises an issue since not all existing module are the same structure and dynamic nature of pytorch scripts make us difficult to reuse module and pretrained weights. To be able to reuse pretrained weights, we could first load the original structure of the network and then load the state_dict and then restructure filter parts of the network to nn.Sequential (as required by Backbone ). We do this hack on our builder function. Since we know that available pretrained model is trained on ImageNet, and we want to use pretrained weights, let's set the initial number of classes to 1000 before loading the model. # we use `n_classes` but torch vision uses `num_classes` kwargs.update({'num_classes': n_classes}) num_classes = kwargs.get('num_classes', 1000) if pretrained and num_classes != 1000: warnings.warn('temporarily change number of classes to get imagenet pretrained weights') kwargs.update({'num_classes': 1000}) if model_name == 'squeezenetv1.1': model = vision.models.squeezenet1_1(pretrained=pretrained, **kwargs) elif model_name == 'squeezenetv1.0': model = vision.models.squeezenet1_0(pretrained=pretrained, **kwargs) else: raise RuntimeError(\"model f{model_name} not supported\") After the model initialization, the next step is to prepare feature extractor. We can use feature_type as customization point if specific feature extractor are required, for example, you want to return feature from activation input instead of activation output, etc. Current implementation needs 'tri_stage_fpn' and 'classifier' to be defined. For 'tri_stage_fpn' we need to split the feature to five stages. For 'classifier' , we need to provide both feature and classifier . if feature_type == 'tri_stage_fpn': if model_name == 'squeezenetv1.1': features = [ model.features[0], model.features[1:3], model.features[3:7], model.features[7:12], model.features[12:], ] elif model_name == 'squeezenetv1.0': features = [ model.features[0], model.features[1:3], model.features[3:6], model.features[6:8], model.features[8:], ] features = vortex_modules.Backbone(nn.Sequential(*features)) For 'classifier' feature, we may need to restore original number of classes if we were loading from pretrained model. elif feature_type == 'classifier': # looking at squeezenet implementation, we need to reset this layer # also note that we need to add nn.Flatten() if pretrained and num_classes != 1000: model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1) classifier_feature = nn.Sequential( model.classifier, nn.Flatten() ) features = model.features features = vortex_modules.ClassifierFeature( features,classifier=classifier_feature ) Raise error here if given feature_type is not supported. else: raise RuntimeError(\"feature type f{feature_tyep} not supported by f{model_name}\") ret = dict( features=features ) if feature_type == 'classifier': ret.update(dict( classifier=classifier_feature )) return features 4. Integration with vortex CLI \u00b6 Finally, we make this module as entry-point for our next experiments to get more vortex' features in a configurable way: hyperparameter optimization training validation TorchScript & ONNX export (with batch inference support) TensorRT inference (using ONNX model) if __name__=='__main__': ## let's use vortex_cli to demonstrate vortex features ## this will be our entrypoint to supported experiments vortex.development.vortex_cli.main() Note that since our custom model is added outside the vortex distribution, it is unavailable from vortex command-line, to properly run experiments with our custom model registered we need to invoke python, for example python3 squeezenet.py train --config cifar10.yml","title":"Custom Backbone Integration"},{"location":"developer-guides/custom_backbone_integration/#implements-custom-backbone-module-in-vortex","text":"This tutorial shows how to develop your own backbone module and integrate it to vortex . The tutorial consists of 4 steps: Vortex Backbone Module Registering builder function Pretrained weights and module restructure hack Integration with vortex CLI import torch import torch.nn as nn import torchvision as vision import warnings import vortex import vortex.development.networks as networks import vortex.development.networks.modules as vortex_modules","title":"Implements Custom Backbone Module in Vortex"},{"location":"developer-guides/custom_backbone_integration/#1-vortex-backbone-module","text":"On computer vision tasks, it is often necessary to perform experiment on various backbone networks. Hence, it is useful to have adapter to multiple backbone, if desired. vortex has such adapter capability with its BackbonePoolConnector that provides interfaces to any registered backbone. The registered backbone itself should be type of Backbone class which actually an adapter for nn.Sequential to also returns its intermediate results .","title":"1. Vortex Backbone Module"},{"location":"developer-guides/custom_backbone_integration/#2-registering-builder-function","text":"Registering backbone is done by adding builder function to vortex . To register the function, we decorate our function with @vortex.development.networks.modules.backbones.register_backbone(model_name) def get_backbone(...) or we can call register_backbone_ directly. The function should be named get_backbone . When calling decorator, we can register multiple model on single function. And the function signature: get_backbone(model_name, pretrained, feature_type, **kwargs) -> Backbone where model_name is str representing the identifier of requested backbone model, pretrained may be str or path-like depending on the caller, and feature_type is str for customization point if necessary. In this case we will add SqueezeNet as backbone to vortex . For the sake of simplicity, we'll load existing network & pretrained from torchvision . register_backbone = vortex_modules.backbones.register_backbone @register_backbone(['squeezenetv1.1', 'squeezenetv1.0']) def get_backbone(model_name, pretrained=False, feature_type='tri_stage_fpn', n_classes=1000, **kwargs): \"\"\" create squeezenet (v1.0 & v1.1) backbone \"\"\"","title":"2. Registering builder function"},{"location":"developer-guides/custom_backbone_integration/#3-pretrained-weights-and-module-restructure-hack","text":"In many cases, we do not want to develop from scratch but use existing module instead. But, this raises an issue since not all existing module are the same structure and dynamic nature of pytorch scripts make us difficult to reuse module and pretrained weights. To be able to reuse pretrained weights, we could first load the original structure of the network and then load the state_dict and then restructure filter parts of the network to nn.Sequential (as required by Backbone ). We do this hack on our builder function. Since we know that available pretrained model is trained on ImageNet, and we want to use pretrained weights, let's set the initial number of classes to 1000 before loading the model. # we use `n_classes` but torch vision uses `num_classes` kwargs.update({'num_classes': n_classes}) num_classes = kwargs.get('num_classes', 1000) if pretrained and num_classes != 1000: warnings.warn('temporarily change number of classes to get imagenet pretrained weights') kwargs.update({'num_classes': 1000}) if model_name == 'squeezenetv1.1': model = vision.models.squeezenet1_1(pretrained=pretrained, **kwargs) elif model_name == 'squeezenetv1.0': model = vision.models.squeezenet1_0(pretrained=pretrained, **kwargs) else: raise RuntimeError(\"model f{model_name} not supported\") After the model initialization, the next step is to prepare feature extractor. We can use feature_type as customization point if specific feature extractor are required, for example, you want to return feature from activation input instead of activation output, etc. Current implementation needs 'tri_stage_fpn' and 'classifier' to be defined. For 'tri_stage_fpn' we need to split the feature to five stages. For 'classifier' , we need to provide both feature and classifier . if feature_type == 'tri_stage_fpn': if model_name == 'squeezenetv1.1': features = [ model.features[0], model.features[1:3], model.features[3:7], model.features[7:12], model.features[12:], ] elif model_name == 'squeezenetv1.0': features = [ model.features[0], model.features[1:3], model.features[3:6], model.features[6:8], model.features[8:], ] features = vortex_modules.Backbone(nn.Sequential(*features)) For 'classifier' feature, we may need to restore original number of classes if we were loading from pretrained model. elif feature_type == 'classifier': # looking at squeezenet implementation, we need to reset this layer # also note that we need to add nn.Flatten() if pretrained and num_classes != 1000: model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1) classifier_feature = nn.Sequential( model.classifier, nn.Flatten() ) features = model.features features = vortex_modules.ClassifierFeature( features,classifier=classifier_feature ) Raise error here if given feature_type is not supported. else: raise RuntimeError(\"feature type f{feature_tyep} not supported by f{model_name}\") ret = dict( features=features ) if feature_type == 'classifier': ret.update(dict( classifier=classifier_feature )) return features","title":"3. Pretrained weights and module restructure hack"},{"location":"developer-guides/custom_backbone_integration/#4-integration-with-vortex-cli","text":"Finally, we make this module as entry-point for our next experiments to get more vortex' features in a configurable way: hyperparameter optimization training validation TorchScript & ONNX export (with batch inference support) TensorRT inference (using ONNX model) if __name__=='__main__': ## let's use vortex_cli to demonstrate vortex features ## this will be our entrypoint to supported experiments vortex.development.vortex_cli.main() Note that since our custom model is added outside the vortex distribution, it is unavailable from vortex command-line, to properly run experiments with our custom model registered we need to invoke python, for example python3 squeezenet.py train --config cifar10.yml","title":"4. Integration with vortex CLI"},{"location":"developer-guides/custom_model_integration_classification/","text":"Integrating Custom Classification Model to Vortex \u00b6 This tutorial shows how to develop your own model and integrate it to vortex . The tutorial consists of 5 steps: Define model architecture Create post-process module Define loss function Register model's builder function Integration with vortex CLI import torch import torch.nn as nn import torchvision as vision import vortex.development import vortex.development.networks as networks import vortex.development.networks.modules as vortex_modules 1. Model Architecture \u00b6 We first define our model, in this case we will define AlexNet model that can be integrated to vortex . In order to do that, our model needs only to have task and output_format member variables. For the sake of simplicity, we will reuse AlexNet from torchvision by instantiate torchvision 's AlexNet and add the required member variable task and output_format . output_format will be used to slice tensor output from post-process module. output_format is a nested dict , with signature of Dict[str,dict] , with inner dict with signature of Dict[str,Union[list,int]] . The inner dictionary is a mapping from output name to arguments that will be used for tensor slicing. The tensor slicing operation will be performed using np.take , Onnx.Gather , or torch.index_select ; the arguments' naming use numpy's take, that is indices and axis ; check numpy, onnx, or torch docs for more details. For classification models, we need class_label and class_confidence . Note that this output_format will be used for single sample. The code will looks like: alexnet = vision.models.alexnet(...) alexnet.task = \"classification\" alexnet.output_format = dict( class_label=dict( indices=[0], axis=0 ), class_confidence=dict( indices=[1], axis=0 ) ) 2. Create PostProcess module \u00b6 Next, let's define our post-process module. This module will be responsible for slicing tensor output from model's output tensor, as well as some additional postprocess if necessary. For example, non-max suppression for detection model, or apply softmax for clasification model if necessary. Note that this module is used for evaluation only, skipped when training. Additional arguments may be supplied as attributes via initializer. class AlexNetPostProcess(nn.Module): def __init__(self): super(AlexNetPostProcess, self).__init__() self.softmax = nn.Softmax(dim=1) def forward(self, input: torch.Tensor) -> torch.Tensor: input = self.softmax(input) conf_label, cls_label = input.max(dim=1, keepdim=False) return torch.stack((cls_label.float(), conf_label), dim=1) 3. Defining Loss Function \u00b6 Since we do not include softmax to our model (performed in post-process instead), we will use cross entropy loss. Note that the function signature for loss function is forward(input,target) -> Tensor . Additional arguments may be supplied as attributes via initializer. class ClassificationLoss(nn.Module): def __init__(self, *args, **kwargs): super(ClassificationLoss,self).__init__() self.loss_fn = nn.CrossEntropyLoss(*args,**kwargs) def forward(self, input, target): target = target.squeeze() if target.size() == torch.Size([]): target = target.unsqueeze(0) return self.loss_fn(input, target) 4. Registering our model to vortex \u00b6 After defining necessary components, we need to define builder function that will create instances of necessary components, then we register it to vortex by decorating the function and providing the model name. This function serves as entry point and customization point for our loss, model, and post-process, and optionally preprocess and dataset collater. For example, you need to reuse parameter from networks to postprocess, you can do it here. The required components are preprocess , network , and postprocess . For training, we need additional components, loss and optional collate_fn . For preprocess, it is recommended to use modules from vortex_modules.preprocess to make sure it is exportable. note that the function signature is create_model_components(model_name, preprocess_args, network_args, loss_args, postprocess_args, stage) -> dict where model_name is a str holds the model name ( 'alexnet' in this case), preprocess_args , network_args , loss_args , postprocess_args are mapping ( dict ) containing parameters from configuration file, while stage is a string containing experiment stage (either 'train' or 'validate' ) given by vortex driver. Note that we simply unpack argument mapping using ** operator. There are two ways of registering builder function, using decorator register_model @vortex.development.networks.models.register_model(model_name='my_model_name') def create_model_components(...): or by directly calling register_model_ vortex.development.networks.models.register_model_('my_model_name',create_model_components) @networks.models.register_model(model_name='alexnet') def create_model_components( model_name, preprocess_args, network_args, loss_args, postprocess_args, stage) -> dict: ## let's get model from torchvision ## then add 'task' and 'output_format' to our AlexNet class, ## adding attributes is not necessary ## if your model already have 'task' and 'output_format' alexnet = vision.models.alexnet(**network_args) ## necessary attributes alexnet.task = \"classification\" alexnet.output_format = dict( class_label=dict( indices=[0], axis=0 ), class_confidence=dict( indices=[1], axis=0 ) ) postprocess = AlexNetPostProcess(**postprocess_args) loss_fn = ClassificationLoss(**loss_args) # using vortex' preprocess module preprocess = vortex_modules.preprocess.get_preprocess( 'normalizer', **preprocess_args ) ## wrap-up core components components = dict( preprocess=preprocess, network=alexnet, postprocess=postprocess, ) if stage == 'train': ## if we are training then append loss function components.update(dict( loss=loss_fn )) elif stage == 'validate': ## do nothing for now pass return components 5. Integration with vortex CLI \u00b6 Finally, we make this module as entry-point for our next experiments to get more vortex' features in a configurable way: hyperparameter optimization training validation TorchScript & ONNX export (with batch inference support) TensorRT inference (using ONNX model) if __name__=='__main__': ## let's use vortex_cli to demonstrate vortex features ## this will be our entrypoint to supported experiments vortex.development.vortex_cli.main() Note that since our custom model is added outside the vortex distribution, it is unavailable from vortex command-line, to properly run experiments with our custom model registered we need to invoke python, for example # run hyperparam optimization experiment python3 alexnet.py hypopt --config cifar10.yml --optconfig hyperparam.yml # run training experiment python3 alexnet.py train --config cifar10.yml","title":"Custom Model Integration : Classification"},{"location":"developer-guides/custom_model_integration_classification/#integrating-custom-classification-model-to-vortex","text":"This tutorial shows how to develop your own model and integrate it to vortex . The tutorial consists of 5 steps: Define model architecture Create post-process module Define loss function Register model's builder function Integration with vortex CLI import torch import torch.nn as nn import torchvision as vision import vortex.development import vortex.development.networks as networks import vortex.development.networks.modules as vortex_modules","title":"Integrating Custom Classification Model to Vortex"},{"location":"developer-guides/custom_model_integration_classification/#1-model-architecture","text":"We first define our model, in this case we will define AlexNet model that can be integrated to vortex . In order to do that, our model needs only to have task and output_format member variables. For the sake of simplicity, we will reuse AlexNet from torchvision by instantiate torchvision 's AlexNet and add the required member variable task and output_format . output_format will be used to slice tensor output from post-process module. output_format is a nested dict , with signature of Dict[str,dict] , with inner dict with signature of Dict[str,Union[list,int]] . The inner dictionary is a mapping from output name to arguments that will be used for tensor slicing. The tensor slicing operation will be performed using np.take , Onnx.Gather , or torch.index_select ; the arguments' naming use numpy's take, that is indices and axis ; check numpy, onnx, or torch docs for more details. For classification models, we need class_label and class_confidence . Note that this output_format will be used for single sample. The code will looks like: alexnet = vision.models.alexnet(...) alexnet.task = \"classification\" alexnet.output_format = dict( class_label=dict( indices=[0], axis=0 ), class_confidence=dict( indices=[1], axis=0 ) )","title":"1. Model Architecture"},{"location":"developer-guides/custom_model_integration_classification/#2-create-postprocess-module","text":"Next, let's define our post-process module. This module will be responsible for slicing tensor output from model's output tensor, as well as some additional postprocess if necessary. For example, non-max suppression for detection model, or apply softmax for clasification model if necessary. Note that this module is used for evaluation only, skipped when training. Additional arguments may be supplied as attributes via initializer. class AlexNetPostProcess(nn.Module): def __init__(self): super(AlexNetPostProcess, self).__init__() self.softmax = nn.Softmax(dim=1) def forward(self, input: torch.Tensor) -> torch.Tensor: input = self.softmax(input) conf_label, cls_label = input.max(dim=1, keepdim=False) return torch.stack((cls_label.float(), conf_label), dim=1)","title":"2. Create PostProcess module"},{"location":"developer-guides/custom_model_integration_classification/#3-defining-loss-function","text":"Since we do not include softmax to our model (performed in post-process instead), we will use cross entropy loss. Note that the function signature for loss function is forward(input,target) -> Tensor . Additional arguments may be supplied as attributes via initializer. class ClassificationLoss(nn.Module): def __init__(self, *args, **kwargs): super(ClassificationLoss,self).__init__() self.loss_fn = nn.CrossEntropyLoss(*args,**kwargs) def forward(self, input, target): target = target.squeeze() if target.size() == torch.Size([]): target = target.unsqueeze(0) return self.loss_fn(input, target)","title":"3. Defining Loss Function"},{"location":"developer-guides/custom_model_integration_classification/#4-registering-our-model-to-vortex","text":"After defining necessary components, we need to define builder function that will create instances of necessary components, then we register it to vortex by decorating the function and providing the model name. This function serves as entry point and customization point for our loss, model, and post-process, and optionally preprocess and dataset collater. For example, you need to reuse parameter from networks to postprocess, you can do it here. The required components are preprocess , network , and postprocess . For training, we need additional components, loss and optional collate_fn . For preprocess, it is recommended to use modules from vortex_modules.preprocess to make sure it is exportable. note that the function signature is create_model_components(model_name, preprocess_args, network_args, loss_args, postprocess_args, stage) -> dict where model_name is a str holds the model name ( 'alexnet' in this case), preprocess_args , network_args , loss_args , postprocess_args are mapping ( dict ) containing parameters from configuration file, while stage is a string containing experiment stage (either 'train' or 'validate' ) given by vortex driver. Note that we simply unpack argument mapping using ** operator. There are two ways of registering builder function, using decorator register_model @vortex.development.networks.models.register_model(model_name='my_model_name') def create_model_components(...): or by directly calling register_model_ vortex.development.networks.models.register_model_('my_model_name',create_model_components) @networks.models.register_model(model_name='alexnet') def create_model_components( model_name, preprocess_args, network_args, loss_args, postprocess_args, stage) -> dict: ## let's get model from torchvision ## then add 'task' and 'output_format' to our AlexNet class, ## adding attributes is not necessary ## if your model already have 'task' and 'output_format' alexnet = vision.models.alexnet(**network_args) ## necessary attributes alexnet.task = \"classification\" alexnet.output_format = dict( class_label=dict( indices=[0], axis=0 ), class_confidence=dict( indices=[1], axis=0 ) ) postprocess = AlexNetPostProcess(**postprocess_args) loss_fn = ClassificationLoss(**loss_args) # using vortex' preprocess module preprocess = vortex_modules.preprocess.get_preprocess( 'normalizer', **preprocess_args ) ## wrap-up core components components = dict( preprocess=preprocess, network=alexnet, postprocess=postprocess, ) if stage == 'train': ## if we are training then append loss function components.update(dict( loss=loss_fn )) elif stage == 'validate': ## do nothing for now pass return components","title":"4. Registering our model to vortex"},{"location":"developer-guides/custom_model_integration_classification/#5-integration-with-vortex-cli","text":"Finally, we make this module as entry-point for our next experiments to get more vortex' features in a configurable way: hyperparameter optimization training validation TorchScript & ONNX export (with batch inference support) TensorRT inference (using ONNX model) if __name__=='__main__': ## let's use vortex_cli to demonstrate vortex features ## this will be our entrypoint to supported experiments vortex.development.vortex_cli.main() Note that since our custom model is added outside the vortex distribution, it is unavailable from vortex command-line, to properly run experiments with our custom model registered we need to invoke python, for example # run hyperparam optimization experiment python3 alexnet.py hypopt --config cifar10.yml --optconfig hyperparam.yml # run training experiment python3 alexnet.py train --config cifar10.yml","title":"5. Integration with vortex CLI"},{"location":"developer-guides/custom_model_integration_detection/","text":"Integrating Custom Detection Model to Vortex \u00b6 This tutorial shows how to create custom detection model and integrate to vortex . In this particular case, we will integrate DE:TR (Detection Transformer) , and constrained to inference only. from PIL import Image import requests import matplotlib.pyplot as plt import torch import torch.nn as nn import torchvision as vision import torchvision.transforms as T from torchvision.models import resnet50 torch.set_grad_enabled(False) import cv2 import numpy as np import warnings warnings.filterwarnings(\"ignore\") import vortex.development import vortex.development.networks as networks import vortex.development.networks.modules as vortex_modules import vortex.development.exporter.utils.onnx.graph_ops as graph_ops Pipeline = vortex.development.core.pipelines.PytorchPredictionPipeline ExportPipeline = vortex.development.core.pipelines.GraphExportPipeline ONNXPipeline = vortex.development.core.pipelines.IRPredictionPipeline TorchScriptPipeline = vortex.development.core.pipelines.IRPredictionPipeline np.set_printoptions(precision=2, suppress=True) 1. Model Architecture \u00b6 DETR Demo taken from facebookresearch/detr 's google colab demo . Modified for vortex compatibility by adding task and output_format atrributes to our model. output_format is a nested dict , with signature of Dict[str,dict] , with inner dict with signature of Dict[str,Union[list,int]] . The inner dictionary is a mapping from output name to arguments that will be used for tensor slicing. The tensor slicing operation will be performed using np.take , Onnx.Gather , or torch.index_select ; the arguments' naming use numpy's take, that is indices and axis ; check numpy, onnx, or torch docs for more details. For detection models, we need class_label , class_confidence , and bounding_box . output_format will be used to slice single sample (image). In case of detection models, there might be multiple objects for single sample, so the result from single sample is 2D tensor. Hence, we take the tensor's values at axis=1 . Note that in the official DETR demo, the model's postprocess returned the full probability tensor but in our case, we want to take only 1D tensor, so we set indices of class_label to [4] and class_confidence to [5] , these values should be corresponding to our postprocess module's output. class DETRdemo(nn.Module): \"\"\" Demo DETR implementation. Demo implementation of DETR in minimal number of lines, with the following differences wrt DETR in the paper: * learned positional encoding (instead of sine) * positional encoding is passed at input (instead of attention) * fc bbox predictor (instead of MLP) The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100. Only batch size 1 supported. \"\"\" def __init__(self, num_classes, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6): super().__init__() # create ResNet-50 backbone self.backbone = resnet50() del self.backbone.fc # create conversion layer self.conv = nn.Conv2d(2048, hidden_dim, 1) # create a default PyTorch transformer self.transformer = nn.Transformer( hidden_dim, nheads, num_encoder_layers, num_decoder_layers) # prediction heads, one extra class for predicting non-empty slots # note that in baseline DETR linear_bbox layer is 3-layer MLP self.linear_class = nn.Linear(hidden_dim, num_classes + 1) self.linear_bbox = nn.Linear(hidden_dim, 4) # output positional encodings (object queries) self.query_pos = nn.Parameter(torch.rand(100, hidden_dim)) # spatial positional encodings # note that in baseline DETR we use sine positional encodings self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2)) self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2)) # attributes for vortex self.task = \"detection\" self.output_format = dict( class_label=dict( indices=[4], axis=1 ), class_confidence=dict( indices=[5], axis=1 ), bounding_box=dict( indices=[0,1,2,3], axis=1 ) ) def forward(self, inputs): # propagate inputs through ResNet-50 up to avg-pool layer x = self.backbone.conv1(inputs) x = self.backbone.bn1(x) x = self.backbone.relu(x) x = self.backbone.maxpool(x) x = self.backbone.layer1(x) x = self.backbone.layer2(x) x = self.backbone.layer3(x) x = self.backbone.layer4(x) # convert from 2048 to 256 feature planes for the transformer h = self.conv(x) # construct positional encodings H, W = h.shape[-2:] pos = torch.cat([ self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1), self.row_embed[:H].unsqueeze(1).repeat(1, W, 1), ], dim=-1).flatten(0, 1).unsqueeze(1) # propagate through the transformer # workaround to handle batched input src = pos + 0.1 * h.flatten(2).permute(2, 0, 1) tgt = self.query_pos.unsqueeze(1).repeat(1, src.size(1), 1) h = self.transformer(src,tgt).transpose(0, 1) # finally project transformer outputs to class labels and bounding boxes return {'pred_logits': self.linear_class(h), 'pred_boxes': self.linear_bbox(h).sigmoid()} helper function to get pretrained DETR on COCO def coco_detr(): detr = DETRdemo(num_classes=91) state_dict = torch.hub.load_state_dict_from_url( url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth', map_location='cpu', check_hash=True) detr.load_state_dict(state_dict) detr.eval() return detr # for output bounding box post-processing def box_cxcywh_to_xyxy(x): x_c, y_c, w, h = x.unbind(1) b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)] return torch.stack(b, dim=1) 2. Create PostProcess module \u00b6 Next, let's define our post-process module. As the name suggests, this module will be called after model inference. In this module we will format the model's output tensor to our format, as well as, decoding the model's output tensor to perform score thresholding, and bounding box conversion. Our postprocess module will take two input, the network output and additional score_threshold as input. By default, the network output will be forwarded to postprocess. For our score_threshold , we need to tell vortex that our postprocess requires additional input, by adding attribute additional_inputs to our postprocess. The signature is Tuple[Tuple[str,Tuple[int,...]],...] , with first elements of the sequences are the input names and the seconds are the input shapes. By doing this, we can change the value even after export to TorchScript or ONNX. Also note that we introduce workaround for torch==1.4 , onnx==1.6 and onnxruntime==1.2 . Onnx operator Greater can't have tensor(bool) as input, but torch onnx exporter won't complain. squeeze and nonzero also inserted to match appropriate dimension and type for index_select (for slicing). class DETRPostProcess(nn.Module): def __init__(self): super().__init__() # tell vortex that our postprocess has additional input # that is score threshold with shape of torch.Size([1]) self.additional_inputs = ( ('score_threshold', (1,)), ) def _forward(self, input: torch.Tensor, score_threshold: torch.Tensor) -> torch.Tensor: # keep only predictions > score_threshold probas = input['pred_logits'].softmax(-1)[:, :-1] # workaround for onnx inference (torch 1.4, onnx 1.6, onnxruntime 1.2) # op `Greater` can't have tensor(bool) as input values, indices = probas.max(-1, keepdim=True) keep = torch.gt(values, score_threshold).squeeze(-1).nonzero().squeeze(-1) # convert to xyxy out_bbox = input['pred_boxes'].index_select(0,keep) out_bbox = box_cxcywh_to_xyxy(out_bbox) # take confidence and classes and match bbox dim # probas = probas[keep] probas = probas.index_select(0, keep) conf_label, cls_label = probas.max(dim=1, keepdim=False) conf_label.unsqueeze_(-1) cls_label.unsqueeze_(-1) # concat input output = torch.cat((out_bbox,cls_label.float(),conf_label),-1) return output def forward(self, input: torch.Tensor, score_threshold: torch.Tensor) -> torch.Tensor: num_batch = input['pred_logits'].size(0) results = [] # workaround to handle batched input, # `num_batch` will be static after export for i in range(num_batch): pred_input = {key: value[i] for key, value in input.items()} result = self._forward(pred_input, score_threshold) results.append(result) return tuple(results) 3. Registering our model to vortex \u00b6 After defining necessary components, we need to define builder function that will create instances of necessary components, then we register it to vortex by decorating the function and providing the model name. This function serves as entry point and customization point for our loss, model, and post-process, and optionally preprocess and dataset collater. For example, you need to reuse parameter from networks to postprocess, you can do it here. The required components are preprocess , network , and postprocess . For training, we need additional components, loss and optional collate_fn . For preprocess, it is recommended to use modules from vortex_modules.preprocess to make sure it is exportable. Here, we will also perform some hack to keep pretrained weights. And note that since we will implements inference only (for now), we will just reject training stage. @networks.models.register_model(model_name='detr') def create_model_components( model_name, preprocess_args, network_args, loss_args, postprocess_args, stage) -> dict: pretrained = network_args.pop('pretrained',False) num_classes = network_args.pop('num_classes',91) if pretrained: # make sure our config match available pretrained assert network_args.get('nheads',8)==8 and \\ network_args.get('hidden_dim',256)==256 and \\ network_args.get('num_encoder_layers',6)==6 and \\ network_args.get('num_decoder_layers',6)==6, \\ \"pretrained not available\" # start from pretrained model = coco_detr() # looking at the implementation, we restore number of class if necessary if num_classes != 91: model.linear_class = nn.Linear(hidden_dim, num_classes + 1) # override preprocess to match with pretrained preprocess preprocess_args = { 'input_size': 800, 'input_normalization': { 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225], 'scaler': 255, } } else: model = DETRdemo(**network_args) # using vortex' preprocess module normalizer = 'normalizer' if pretrained: normalizer = 'flip_normalizer' # rgb2bgr preprocess = vortex_modules.preprocess.get_preprocess( normalizer, **preprocess_args ) # instantiate our DETR postprocess postprocess = DETRPostProcess(**postprocess_args) # wrap-up core components components = dict( preprocess=preprocess, network=model, postprocess=postprocess, ) # current implemenetation doesn't support training if stage == 'train': raise NotImplementedError(\"current implementation doesn't support training yet\") return components 4. Integration with vortex CLI \u00b6 We'll provide entry-point for our experiments to get more vortex' features in a configuratble way. Doing this is as simple as calling vortex_cli 's main function and we are all set. To separate this from demo program, let's wrap it on cli() and create another python scripts calling this cli() as our entry-point (see cli.py ) def cli(): vortex.development.vortex_cli.main() 5. Demo program \u00b6 Prepare some files run_demo = False if __name__ == '__main__': run_demo = True from pathlib import Path def download_image(url: str, filename: str): path = Path(filename) try: Image.open(path) except: with path.open('wb+') as f: f.write(requests.get(url).content) return filename images = [ dict( url='http://images.cocodataset.org/val2017/000000039769.jpg', filename='000000039769.jpg' ), dict( url='http://farm7.staticflickr.com/6153/6162098289_512fc331dc_z.jpg', filename='6162098289_512fc331dc_z.jpg' ), dict( url='http://farm9.staticflickr.com/8539/8640225449_73c750f686_z.jpg', filename='8640225449_73c750f686_z.jpg' ), dict( url='http://farm5.staticflickr.com/4008/4295843111_bee8c2f586_z.jpg', filename='4295843111_bee8c2f586_z.jpg' ) ] downloaded_images = list(map(lambda args: download_image(**args), images)) Create helper function for detr demo. You can run this demo using examples/custom_models/detr/detr.py or examples/custom_models/detr/detr.ipynb . def detr_demo(): def rescale_bboxes(out_bbox, size): img_w, img_h = size b = box_cxcywh_to_xyxy(out_bbox) b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32) return b def detect(im, model, transform): # mean-std normalize the input image (batch-size: 1) img = transform(im).unsqueeze(0) # propagate through the model outputs = model(img) # keep only predictions with 0.7+ confidence probas = outputs['pred_logits'].softmax(-1)[0, :, :-1] keep = probas.max(-1).values > 0.7 # convert boxes from [0; 1] to image scales bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size) return probas[keep], bboxes_scaled def plot_results(pil_img, prob, boxes): plt.figure(figsize=(16,10)) plt.imshow(pil_img) ax = plt.gca() for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100): ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3)) cl = p.argmax() text = f'{CLASSES[cl]}: {p[cl]:0.2f}' ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5)) plt.axis('off') plt.show() # COCO classes CLASSES = [ 'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush' ] # colors for visualization COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125], [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]] # standard PyTorch mean-std input image normalization transform = T.Compose([ T.Resize(800), T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) detr = coco_detr() url = 'http://images.cocodataset.org/val2017/000000039769.jpg' im = Image.open(requests.get(url, stream=True).raw) scores, boxes = detect(im, detr, transform) plot_results(im, scores, boxes) Create helper function for detr demo using vortex def load_config(cfg='detr.yml'): import yaml, os, pathlib, easydict with open(cfg) as f: config = easydict.EasyDict(yaml.load(f)) # prepare path TORCH_HOME = os.getenv('TORCH_HOME', os.environ['HOME'] + '/.cache/torch/') weights_file = pathlib.Path(TORCH_HOME) weights_file = weights_file / 'checkpoints' / 'detr_demo-da2a99e9.pth' return dict( config=config, weights_file=weights_file ) def visualize(results): for visualization, prediction in zip(results.visualization, results.prediction) : # visualization plt.figure() plt.imshow(cv2.cvtColor(visualization,cv2.COLOR_BGR2RGB)) # print results print('prediction results:') print(prediction) def vortex_demo(cfg='detr.yml',test_images='000000039769.jpg', score_threshold=0.7): \"\"\" helper function to run torch inference on vortex \"\"\" cfg = load_config(cfg) config = cfg['config'] weights_file = cfg['weights_file'] # create predictor predictor = Pipeline(config, weights_file, 'cuda' if torch.cuda.is_available() else 'cpu') # run prediction test_images = test_images if isinstance(test_images, str): test_images = [test_images] run_args = dict( dump_visual=True, visualize=True, score_threshold=score_threshold ) results = predictor.run(test_images, **run_args) visualize(results) def vortex_export(cfg='detr.yml', test_images='000000039769.jpg'): \"\"\" helper function to export detr to ONNX & TorchScript \"\"\" cfg = load_config(cfg) config = cfg['config'] weights_file = cfg['weights_file'] exporter = ExportPipeline(config, weights=weights_file) result = exporter.run(example_input=test_images) def torchscript_demo(model=None, runtime='cpu', test_images='000000039769.jpg', score_threshold=0.7): \"\"\" helper function to run torchscript inference on vortex \"\"\" if model is None: model = 'experiments/outputs/detr/detr-bs4.pt' # create predictor predictor = TorchScriptPipeline(model, runtime=runtime) # run prediction # run prediction test_images = test_images if isinstance(test_images, str): test_images = [test_images] run_args = dict( dump_visual=True, visualize=True, score_threshold=score_threshold ) results = predictor.run(test_images, **run_args) visualize(results) def onnx_demo(model=None, runtime='cpu', test_images='000000039769.jpg', score_threshold=0.7): \"\"\" helper function to run onnx inference on vortex \"\"\" if model is None: model = 'experiments/outputs/detr/detr.onnx' # create predictor predictor = ONNXPipeline(model, runtime=runtime) # run prediction test_images = test_images if isinstance(test_images, str): test_images = [test_images] run_args = dict( dump_visual=True, visualize=True, score_threshold=score_threshold, ) results = predictor.run(test_images, **run_args) visualize(results) DE:TR Demo using original demo code detr_demo() if run_demo else 'skip detr demo' DE:TR Demo using vortex vortex_demo(test_images=downloaded_images) if run_demo else 'skip detr demo' Export DE:TR to ONNX and TorchScript using vortex vortex_export() if run_demo else 'skip detr export' TorchScript Inference using exported graph torchscript_demo(test_images=downloaded_images) if run_demo else 'skip torchscript demo' ONNX Inference using exported graph # skip for now # onnx_demo() if run_demo else 'skip onnx demo'","title":"Custom Model Integration : Detection"},{"location":"developer-guides/custom_model_integration_detection/#integrating-custom-detection-model-to-vortex","text":"This tutorial shows how to create custom detection model and integrate to vortex . In this particular case, we will integrate DE:TR (Detection Transformer) , and constrained to inference only. from PIL import Image import requests import matplotlib.pyplot as plt import torch import torch.nn as nn import torchvision as vision import torchvision.transforms as T from torchvision.models import resnet50 torch.set_grad_enabled(False) import cv2 import numpy as np import warnings warnings.filterwarnings(\"ignore\") import vortex.development import vortex.development.networks as networks import vortex.development.networks.modules as vortex_modules import vortex.development.exporter.utils.onnx.graph_ops as graph_ops Pipeline = vortex.development.core.pipelines.PytorchPredictionPipeline ExportPipeline = vortex.development.core.pipelines.GraphExportPipeline ONNXPipeline = vortex.development.core.pipelines.IRPredictionPipeline TorchScriptPipeline = vortex.development.core.pipelines.IRPredictionPipeline np.set_printoptions(precision=2, suppress=True)","title":"Integrating Custom Detection Model to Vortex"},{"location":"developer-guides/custom_model_integration_detection/#1-model-architecture","text":"DETR Demo taken from facebookresearch/detr 's google colab demo . Modified for vortex compatibility by adding task and output_format atrributes to our model. output_format is a nested dict , with signature of Dict[str,dict] , with inner dict with signature of Dict[str,Union[list,int]] . The inner dictionary is a mapping from output name to arguments that will be used for tensor slicing. The tensor slicing operation will be performed using np.take , Onnx.Gather , or torch.index_select ; the arguments' naming use numpy's take, that is indices and axis ; check numpy, onnx, or torch docs for more details. For detection models, we need class_label , class_confidence , and bounding_box . output_format will be used to slice single sample (image). In case of detection models, there might be multiple objects for single sample, so the result from single sample is 2D tensor. Hence, we take the tensor's values at axis=1 . Note that in the official DETR demo, the model's postprocess returned the full probability tensor but in our case, we want to take only 1D tensor, so we set indices of class_label to [4] and class_confidence to [5] , these values should be corresponding to our postprocess module's output. class DETRdemo(nn.Module): \"\"\" Demo DETR implementation. Demo implementation of DETR in minimal number of lines, with the following differences wrt DETR in the paper: * learned positional encoding (instead of sine) * positional encoding is passed at input (instead of attention) * fc bbox predictor (instead of MLP) The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100. Only batch size 1 supported. \"\"\" def __init__(self, num_classes, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6): super().__init__() # create ResNet-50 backbone self.backbone = resnet50() del self.backbone.fc # create conversion layer self.conv = nn.Conv2d(2048, hidden_dim, 1) # create a default PyTorch transformer self.transformer = nn.Transformer( hidden_dim, nheads, num_encoder_layers, num_decoder_layers) # prediction heads, one extra class for predicting non-empty slots # note that in baseline DETR linear_bbox layer is 3-layer MLP self.linear_class = nn.Linear(hidden_dim, num_classes + 1) self.linear_bbox = nn.Linear(hidden_dim, 4) # output positional encodings (object queries) self.query_pos = nn.Parameter(torch.rand(100, hidden_dim)) # spatial positional encodings # note that in baseline DETR we use sine positional encodings self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2)) self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2)) # attributes for vortex self.task = \"detection\" self.output_format = dict( class_label=dict( indices=[4], axis=1 ), class_confidence=dict( indices=[5], axis=1 ), bounding_box=dict( indices=[0,1,2,3], axis=1 ) ) def forward(self, inputs): # propagate inputs through ResNet-50 up to avg-pool layer x = self.backbone.conv1(inputs) x = self.backbone.bn1(x) x = self.backbone.relu(x) x = self.backbone.maxpool(x) x = self.backbone.layer1(x) x = self.backbone.layer2(x) x = self.backbone.layer3(x) x = self.backbone.layer4(x) # convert from 2048 to 256 feature planes for the transformer h = self.conv(x) # construct positional encodings H, W = h.shape[-2:] pos = torch.cat([ self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1), self.row_embed[:H].unsqueeze(1).repeat(1, W, 1), ], dim=-1).flatten(0, 1).unsqueeze(1) # propagate through the transformer # workaround to handle batched input src = pos + 0.1 * h.flatten(2).permute(2, 0, 1) tgt = self.query_pos.unsqueeze(1).repeat(1, src.size(1), 1) h = self.transformer(src,tgt).transpose(0, 1) # finally project transformer outputs to class labels and bounding boxes return {'pred_logits': self.linear_class(h), 'pred_boxes': self.linear_bbox(h).sigmoid()} helper function to get pretrained DETR on COCO def coco_detr(): detr = DETRdemo(num_classes=91) state_dict = torch.hub.load_state_dict_from_url( url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth', map_location='cpu', check_hash=True) detr.load_state_dict(state_dict) detr.eval() return detr # for output bounding box post-processing def box_cxcywh_to_xyxy(x): x_c, y_c, w, h = x.unbind(1) b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)] return torch.stack(b, dim=1)","title":"1. Model Architecture"},{"location":"developer-guides/custom_model_integration_detection/#2-create-postprocess-module","text":"Next, let's define our post-process module. As the name suggests, this module will be called after model inference. In this module we will format the model's output tensor to our format, as well as, decoding the model's output tensor to perform score thresholding, and bounding box conversion. Our postprocess module will take two input, the network output and additional score_threshold as input. By default, the network output will be forwarded to postprocess. For our score_threshold , we need to tell vortex that our postprocess requires additional input, by adding attribute additional_inputs to our postprocess. The signature is Tuple[Tuple[str,Tuple[int,...]],...] , with first elements of the sequences are the input names and the seconds are the input shapes. By doing this, we can change the value even after export to TorchScript or ONNX. Also note that we introduce workaround for torch==1.4 , onnx==1.6 and onnxruntime==1.2 . Onnx operator Greater can't have tensor(bool) as input, but torch onnx exporter won't complain. squeeze and nonzero also inserted to match appropriate dimension and type for index_select (for slicing). class DETRPostProcess(nn.Module): def __init__(self): super().__init__() # tell vortex that our postprocess has additional input # that is score threshold with shape of torch.Size([1]) self.additional_inputs = ( ('score_threshold', (1,)), ) def _forward(self, input: torch.Tensor, score_threshold: torch.Tensor) -> torch.Tensor: # keep only predictions > score_threshold probas = input['pred_logits'].softmax(-1)[:, :-1] # workaround for onnx inference (torch 1.4, onnx 1.6, onnxruntime 1.2) # op `Greater` can't have tensor(bool) as input values, indices = probas.max(-1, keepdim=True) keep = torch.gt(values, score_threshold).squeeze(-1).nonzero().squeeze(-1) # convert to xyxy out_bbox = input['pred_boxes'].index_select(0,keep) out_bbox = box_cxcywh_to_xyxy(out_bbox) # take confidence and classes and match bbox dim # probas = probas[keep] probas = probas.index_select(0, keep) conf_label, cls_label = probas.max(dim=1, keepdim=False) conf_label.unsqueeze_(-1) cls_label.unsqueeze_(-1) # concat input output = torch.cat((out_bbox,cls_label.float(),conf_label),-1) return output def forward(self, input: torch.Tensor, score_threshold: torch.Tensor) -> torch.Tensor: num_batch = input['pred_logits'].size(0) results = [] # workaround to handle batched input, # `num_batch` will be static after export for i in range(num_batch): pred_input = {key: value[i] for key, value in input.items()} result = self._forward(pred_input, score_threshold) results.append(result) return tuple(results)","title":"2. Create PostProcess module"},{"location":"developer-guides/custom_model_integration_detection/#3-registering-our-model-to-vortex","text":"After defining necessary components, we need to define builder function that will create instances of necessary components, then we register it to vortex by decorating the function and providing the model name. This function serves as entry point and customization point for our loss, model, and post-process, and optionally preprocess and dataset collater. For example, you need to reuse parameter from networks to postprocess, you can do it here. The required components are preprocess , network , and postprocess . For training, we need additional components, loss and optional collate_fn . For preprocess, it is recommended to use modules from vortex_modules.preprocess to make sure it is exportable. Here, we will also perform some hack to keep pretrained weights. And note that since we will implements inference only (for now), we will just reject training stage. @networks.models.register_model(model_name='detr') def create_model_components( model_name, preprocess_args, network_args, loss_args, postprocess_args, stage) -> dict: pretrained = network_args.pop('pretrained',False) num_classes = network_args.pop('num_classes',91) if pretrained: # make sure our config match available pretrained assert network_args.get('nheads',8)==8 and \\ network_args.get('hidden_dim',256)==256 and \\ network_args.get('num_encoder_layers',6)==6 and \\ network_args.get('num_decoder_layers',6)==6, \\ \"pretrained not available\" # start from pretrained model = coco_detr() # looking at the implementation, we restore number of class if necessary if num_classes != 91: model.linear_class = nn.Linear(hidden_dim, num_classes + 1) # override preprocess to match with pretrained preprocess preprocess_args = { 'input_size': 800, 'input_normalization': { 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225], 'scaler': 255, } } else: model = DETRdemo(**network_args) # using vortex' preprocess module normalizer = 'normalizer' if pretrained: normalizer = 'flip_normalizer' # rgb2bgr preprocess = vortex_modules.preprocess.get_preprocess( normalizer, **preprocess_args ) # instantiate our DETR postprocess postprocess = DETRPostProcess(**postprocess_args) # wrap-up core components components = dict( preprocess=preprocess, network=model, postprocess=postprocess, ) # current implemenetation doesn't support training if stage == 'train': raise NotImplementedError(\"current implementation doesn't support training yet\") return components","title":"3. Registering our model to vortex"},{"location":"developer-guides/custom_model_integration_detection/#4-integration-with-vortex-cli","text":"We'll provide entry-point for our experiments to get more vortex' features in a configuratble way. Doing this is as simple as calling vortex_cli 's main function and we are all set. To separate this from demo program, let's wrap it on cli() and create another python scripts calling this cli() as our entry-point (see cli.py ) def cli(): vortex.development.vortex_cli.main()","title":"4. Integration with vortex CLI"},{"location":"developer-guides/custom_model_integration_detection/#5-demo-program","text":"Prepare some files run_demo = False if __name__ == '__main__': run_demo = True from pathlib import Path def download_image(url: str, filename: str): path = Path(filename) try: Image.open(path) except: with path.open('wb+') as f: f.write(requests.get(url).content) return filename images = [ dict( url='http://images.cocodataset.org/val2017/000000039769.jpg', filename='000000039769.jpg' ), dict( url='http://farm7.staticflickr.com/6153/6162098289_512fc331dc_z.jpg', filename='6162098289_512fc331dc_z.jpg' ), dict( url='http://farm9.staticflickr.com/8539/8640225449_73c750f686_z.jpg', filename='8640225449_73c750f686_z.jpg' ), dict( url='http://farm5.staticflickr.com/4008/4295843111_bee8c2f586_z.jpg', filename='4295843111_bee8c2f586_z.jpg' ) ] downloaded_images = list(map(lambda args: download_image(**args), images)) Create helper function for detr demo. You can run this demo using examples/custom_models/detr/detr.py or examples/custom_models/detr/detr.ipynb . def detr_demo(): def rescale_bboxes(out_bbox, size): img_w, img_h = size b = box_cxcywh_to_xyxy(out_bbox) b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32) return b def detect(im, model, transform): # mean-std normalize the input image (batch-size: 1) img = transform(im).unsqueeze(0) # propagate through the model outputs = model(img) # keep only predictions with 0.7+ confidence probas = outputs['pred_logits'].softmax(-1)[0, :, :-1] keep = probas.max(-1).values > 0.7 # convert boxes from [0; 1] to image scales bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size) return probas[keep], bboxes_scaled def plot_results(pil_img, prob, boxes): plt.figure(figsize=(16,10)) plt.imshow(pil_img) ax = plt.gca() for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100): ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3)) cl = p.argmax() text = f'{CLASSES[cl]}: {p[cl]:0.2f}' ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5)) plt.axis('off') plt.show() # COCO classes CLASSES = [ 'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush' ] # colors for visualization COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125], [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]] # standard PyTorch mean-std input image normalization transform = T.Compose([ T.Resize(800), T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) detr = coco_detr() url = 'http://images.cocodataset.org/val2017/000000039769.jpg' im = Image.open(requests.get(url, stream=True).raw) scores, boxes = detect(im, detr, transform) plot_results(im, scores, boxes) Create helper function for detr demo using vortex def load_config(cfg='detr.yml'): import yaml, os, pathlib, easydict with open(cfg) as f: config = easydict.EasyDict(yaml.load(f)) # prepare path TORCH_HOME = os.getenv('TORCH_HOME', os.environ['HOME'] + '/.cache/torch/') weights_file = pathlib.Path(TORCH_HOME) weights_file = weights_file / 'checkpoints' / 'detr_demo-da2a99e9.pth' return dict( config=config, weights_file=weights_file ) def visualize(results): for visualization, prediction in zip(results.visualization, results.prediction) : # visualization plt.figure() plt.imshow(cv2.cvtColor(visualization,cv2.COLOR_BGR2RGB)) # print results print('prediction results:') print(prediction) def vortex_demo(cfg='detr.yml',test_images='000000039769.jpg', score_threshold=0.7): \"\"\" helper function to run torch inference on vortex \"\"\" cfg = load_config(cfg) config = cfg['config'] weights_file = cfg['weights_file'] # create predictor predictor = Pipeline(config, weights_file, 'cuda' if torch.cuda.is_available() else 'cpu') # run prediction test_images = test_images if isinstance(test_images, str): test_images = [test_images] run_args = dict( dump_visual=True, visualize=True, score_threshold=score_threshold ) results = predictor.run(test_images, **run_args) visualize(results) def vortex_export(cfg='detr.yml', test_images='000000039769.jpg'): \"\"\" helper function to export detr to ONNX & TorchScript \"\"\" cfg = load_config(cfg) config = cfg['config'] weights_file = cfg['weights_file'] exporter = ExportPipeline(config, weights=weights_file) result = exporter.run(example_input=test_images) def torchscript_demo(model=None, runtime='cpu', test_images='000000039769.jpg', score_threshold=0.7): \"\"\" helper function to run torchscript inference on vortex \"\"\" if model is None: model = 'experiments/outputs/detr/detr-bs4.pt' # create predictor predictor = TorchScriptPipeline(model, runtime=runtime) # run prediction # run prediction test_images = test_images if isinstance(test_images, str): test_images = [test_images] run_args = dict( dump_visual=True, visualize=True, score_threshold=score_threshold ) results = predictor.run(test_images, **run_args) visualize(results) def onnx_demo(model=None, runtime='cpu', test_images='000000039769.jpg', score_threshold=0.7): \"\"\" helper function to run onnx inference on vortex \"\"\" if model is None: model = 'experiments/outputs/detr/detr.onnx' # create predictor predictor = ONNXPipeline(model, runtime=runtime) # run prediction test_images = test_images if isinstance(test_images, str): test_images = [test_images] run_args = dict( dump_visual=True, visualize=True, score_threshold=score_threshold, ) results = predictor.run(test_images, **run_args) visualize(results) DE:TR Demo using original demo code detr_demo() if run_demo else 'skip detr demo' DE:TR Demo using vortex vortex_demo(test_images=downloaded_images) if run_demo else 'skip detr demo' Export DE:TR to ONNX and TorchScript using vortex vortex_export() if run_demo else 'skip detr export' TorchScript Inference using exported graph torchscript_demo(test_images=downloaded_images) if run_demo else 'skip torchscript demo' ONNX Inference using exported graph # skip for now # onnx_demo() if run_demo else 'skip onnx demo'","title":"5. Demo program"},{"location":"modules/augmentation/","text":"Augmentations \u00b6 This section listed all available augmentations modules configurations. Part of dataset.train configurations in experiment file. Albumentations \u00b6 You can utilized integrated albumentations image augmentation library in Vortex. Originally users must code the following example to use it. E.g. : from albumentations.augmentations.transforms import ( HorizontalFlip, RandomScale, RandomBrightnessContrast, RandomSnow ) from albumentations.core.composition import ( Compose, OneOf, BboxParams ) bbox_params=BboxParams(format='coco',min_area=0.0, min_visibility=0.0) transforms=Compose([OneOf(transforms=[RandomBrightnessContrast(p=0.5), RandomSnow(p=0.5)],p=0.5), HorizontalFlip(p=0.5), RandomScale(p=0.5,scale_limit=0.1)], bbox_params=bbox_params) In Vortex we simplify it in the experiment file, and abstracted the process of each image and targets. So the analogous form of the above script in Vortex configuration is shown below. E.g. : augmentations: [ { module: albumentations, args: { transforms: [ { compose: OneOf, args: { transforms: [ { transform: RandomBrightnessContrast, args: { p: 0.5}}, { transform: RandomSnow, args: { p: 0.5}} ], p: 0.5} }, { transform: HorizontalFlip, args: { p: 0.5 } }, { transform: RandomScale, args: { scale_limit: 0.1, p: 0.5 } } ], bbox_params: { min_visibility: 0.0, min_area: 0.0 }, visual_debug: False } } ] Arguments : transforms (list[dict]) : list of augmentation transformation or compose to be sequentially added. Each member of the list is a dictionary with a sub-arguments shown below : compose OR transform (str) : denotes a compose or a transform from albumentation. Only support OneOf compose for compose key. Supported transform available in this link . args : the corresponding arguments for selected compose or transform for OneOf compose, the transforms arguments have same description with the previous transforms (list[dict]). Possible for nested compose inside transforms bbox_params (dict) : Parameter for bounding box target's annotation. See this link for further reading. Supported sub-args: min_visibility (float) : minimum fraction of area for a bounding box to remain this box in list min_area (float) : minimum area of a bounding box. All bounding boxes whose visible area in pixels is less than this value will be removed visual_debug (bool) : used for visualization debugging. It uses \u2018cv2.imshow\u2019 to visualize every augmentations result. Disable it for training, default False Nvidia DALI \u00b6 You can utilized integrated Nvidia DALI image augmentation library in Vortex. However, due to the low level nature of DALI's ops, we provide several high level transformation module that automatically handle label's transformation for several ops that's label sensitive. Similar like albumentations module, you can utilize Nvidia DALI augmentation by specifying the following configurations augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: HorizontalFlip, args: { p: 0.5 } }, { transform: RandomBrightnessContrast, args: { brightness_limit: 0.2, contrast_limit : 0.1, p: 0.5 } } ] } } ] Arguments : transforms (list[dict]) : list of augmentation transformation or compose to be sequentially added. Each member of the list is a dictionary with a sub-arguments shown below : transform (str) : denotes a transform function to be used. Supported transforms can be read in the following section args (dict) : the corresponding arguments for selected transform Supported Augmentations \u00b6 The supported Nvidia DALI transforms listed in here : Horizontal Flip \u00b6 Flip image in horizontal axis. Supports coordinates sensitive labels augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: HorizontalFlip, args: { p: 0.5 } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. Vertical Flip \u00b6 Flip image in vertical axis. Supports coordinates sensitive labels augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: VerticalFlip, args: { p: 0.5 } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. Random Brightness Contrast \u00b6 Apply random brightness and contrast adjustment of the image augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: RandomBrightnessContrast, args: { p: 0.5 , brightness_limit: 0.5, contrast_limit: 0.5 } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. brightness_limit (float,list) : Factor multiplier range for changing brightness in [min,max] value format. If provided as a single float, the range will be 1 + (-limit, limit). Defaults to 0.5. contrast_limit (float,list) : Factor multiplier range for changing contrast in [min,max] value format. If provided as a single float, the range will be 1 + (-limit, limit). Defaults to 0.5. Random Jitter \u00b6 Perform a random Jitter augmentation. The output image is produced by moving each pixel by a random amount bounded by half of nDegree parameter (in both x and y dimensions). augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: RandomJitter, args: { p: 0.5 , nDegree: 2, } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. nDegree (int) : Each pixel is moved by a random amount in range [-nDegree/2, nDegree/2]. Defaults to 2. Random Hue Saturation Value \u00b6 Apply random HSV manipulation. To change hue, saturation and/or value of the image, pass corresponding coefficients. Keep in mind, that hue has additive delta argument, while for saturation and value they are multiplicative. augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: RandomHueSaturationValue, args: { p: 0.5 , hue_limit: 20, saturation_limit: 0.5, value_limit: 0.5 } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. hue_limit (float,list) : Range for changing hue in [min,max] value format. If provided as a single float, the range will be (-limit, limit). Defaults to 20.. saturation_limit (float,list) : Factor multiplier range for changing saturation in [min,max] value format. If provided as a single float, the range will be 1 + (-limit, limit). Defaults to 0.5. value_limit (float,list) : Factor multiplier range for changing value in [min,max] value format. If provided as a single float, the range will be 1 + (-limit, limit). Defaults to 0.5. Random Water \u00b6 Apply random water augmentation (make image appear to be underwater). Can not support coordinates sensitive labels augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: RandomWater, args: { p: 0.5 , ampl_x: 10.0, ampl_y: 10.0, freq_x: 0.049087, freq_y: 0.049087, phase_x: 0.0, phase_y: 0.0, } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. ampl_x (float): Amplitude of the wave in x direction. Defaults to 10.0. ampl_y (float): Amplitude of the wave in y direction. Defaults to 10.0. freq_x (float): Frequency of the wave in x direction. Defaults to 0.049087. freq_y (float): Frequency of the wave in y direction. Defaults to 0.049087. phase_x (float): Phase of the wave in x direction. Defaults to 0.0. phase_y (float): Phase of the wave in y direction. Defaults to 0.0. Random Rotate \u00b6 Apply random rotation to the image, currently not support coordinates sensitive labels augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: RandomRotate, args: { p: 0.5 , angle_limit: 45., } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. angle_limit (float,list) : Range for changing angle in [min,max] value format. If provided as a single float, the range will be (-limit, limit). Defaults to 45..","title":"Augmentations"},{"location":"modules/augmentation/#augmentations","text":"This section listed all available augmentations modules configurations. Part of dataset.train configurations in experiment file.","title":"Augmentations"},{"location":"modules/augmentation/#albumentations","text":"You can utilized integrated albumentations image augmentation library in Vortex. Originally users must code the following example to use it. E.g. : from albumentations.augmentations.transforms import ( HorizontalFlip, RandomScale, RandomBrightnessContrast, RandomSnow ) from albumentations.core.composition import ( Compose, OneOf, BboxParams ) bbox_params=BboxParams(format='coco',min_area=0.0, min_visibility=0.0) transforms=Compose([OneOf(transforms=[RandomBrightnessContrast(p=0.5), RandomSnow(p=0.5)],p=0.5), HorizontalFlip(p=0.5), RandomScale(p=0.5,scale_limit=0.1)], bbox_params=bbox_params) In Vortex we simplify it in the experiment file, and abstracted the process of each image and targets. So the analogous form of the above script in Vortex configuration is shown below. E.g. : augmentations: [ { module: albumentations, args: { transforms: [ { compose: OneOf, args: { transforms: [ { transform: RandomBrightnessContrast, args: { p: 0.5}}, { transform: RandomSnow, args: { p: 0.5}} ], p: 0.5} }, { transform: HorizontalFlip, args: { p: 0.5 } }, { transform: RandomScale, args: { scale_limit: 0.1, p: 0.5 } } ], bbox_params: { min_visibility: 0.0, min_area: 0.0 }, visual_debug: False } } ] Arguments : transforms (list[dict]) : list of augmentation transformation or compose to be sequentially added. Each member of the list is a dictionary with a sub-arguments shown below : compose OR transform (str) : denotes a compose or a transform from albumentation. Only support OneOf compose for compose key. Supported transform available in this link . args : the corresponding arguments for selected compose or transform for OneOf compose, the transforms arguments have same description with the previous transforms (list[dict]). Possible for nested compose inside transforms bbox_params (dict) : Parameter for bounding box target's annotation. See this link for further reading. Supported sub-args: min_visibility (float) : minimum fraction of area for a bounding box to remain this box in list min_area (float) : minimum area of a bounding box. All bounding boxes whose visible area in pixels is less than this value will be removed visual_debug (bool) : used for visualization debugging. It uses \u2018cv2.imshow\u2019 to visualize every augmentations result. Disable it for training, default False","title":"Albumentations"},{"location":"modules/augmentation/#nvidia-dali","text":"You can utilized integrated Nvidia DALI image augmentation library in Vortex. However, due to the low level nature of DALI's ops, we provide several high level transformation module that automatically handle label's transformation for several ops that's label sensitive. Similar like albumentations module, you can utilize Nvidia DALI augmentation by specifying the following configurations augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: HorizontalFlip, args: { p: 0.5 } }, { transform: RandomBrightnessContrast, args: { brightness_limit: 0.2, contrast_limit : 0.1, p: 0.5 } } ] } } ] Arguments : transforms (list[dict]) : list of augmentation transformation or compose to be sequentially added. Each member of the list is a dictionary with a sub-arguments shown below : transform (str) : denotes a transform function to be used. Supported transforms can be read in the following section args (dict) : the corresponding arguments for selected transform","title":"Nvidia DALI"},{"location":"modules/augmentation/#supported-augmentations","text":"The supported Nvidia DALI transforms listed in here :","title":"Supported Augmentations"},{"location":"modules/augmentation/#horizontal-flip","text":"Flip image in horizontal axis. Supports coordinates sensitive labels augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: HorizontalFlip, args: { p: 0.5 } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5.","title":"Horizontal Flip"},{"location":"modules/augmentation/#vertical-flip","text":"Flip image in vertical axis. Supports coordinates sensitive labels augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: VerticalFlip, args: { p: 0.5 } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5.","title":"Vertical Flip"},{"location":"modules/augmentation/#random-brightness-contrast","text":"Apply random brightness and contrast adjustment of the image augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: RandomBrightnessContrast, args: { p: 0.5 , brightness_limit: 0.5, contrast_limit: 0.5 } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. brightness_limit (float,list) : Factor multiplier range for changing brightness in [min,max] value format. If provided as a single float, the range will be 1 + (-limit, limit). Defaults to 0.5. contrast_limit (float,list) : Factor multiplier range for changing contrast in [min,max] value format. If provided as a single float, the range will be 1 + (-limit, limit). Defaults to 0.5.","title":"Random Brightness Contrast"},{"location":"modules/augmentation/#random-jitter","text":"Perform a random Jitter augmentation. The output image is produced by moving each pixel by a random amount bounded by half of nDegree parameter (in both x and y dimensions). augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: RandomJitter, args: { p: 0.5 , nDegree: 2, } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. nDegree (int) : Each pixel is moved by a random amount in range [-nDegree/2, nDegree/2]. Defaults to 2.","title":"Random Jitter"},{"location":"modules/augmentation/#random-hue-saturation-value","text":"Apply random HSV manipulation. To change hue, saturation and/or value of the image, pass corresponding coefficients. Keep in mind, that hue has additive delta argument, while for saturation and value they are multiplicative. augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: RandomHueSaturationValue, args: { p: 0.5 , hue_limit: 20, saturation_limit: 0.5, value_limit: 0.5 } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. hue_limit (float,list) : Range for changing hue in [min,max] value format. If provided as a single float, the range will be (-limit, limit). Defaults to 20.. saturation_limit (float,list) : Factor multiplier range for changing saturation in [min,max] value format. If provided as a single float, the range will be 1 + (-limit, limit). Defaults to 0.5. value_limit (float,list) : Factor multiplier range for changing value in [min,max] value format. If provided as a single float, the range will be 1 + (-limit, limit). Defaults to 0.5.","title":"Random Hue Saturation Value"},{"location":"modules/augmentation/#random-water","text":"Apply random water augmentation (make image appear to be underwater). Can not support coordinates sensitive labels augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: RandomWater, args: { p: 0.5 , ampl_x: 10.0, ampl_y: 10.0, freq_x: 0.049087, freq_y: 0.049087, phase_x: 0.0, phase_y: 0.0, } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. ampl_x (float): Amplitude of the wave in x direction. Defaults to 10.0. ampl_y (float): Amplitude of the wave in y direction. Defaults to 10.0. freq_x (float): Frequency of the wave in x direction. Defaults to 0.049087. freq_y (float): Frequency of the wave in y direction. Defaults to 0.049087. phase_x (float): Phase of the wave in x direction. Defaults to 0.0. phase_y (float): Phase of the wave in y direction. Defaults to 0.0.","title":"Random Water"},{"location":"modules/augmentation/#random-rotate","text":"Apply random rotation to the image, currently not support coordinates sensitive labels augmentations: [ { module: nvidia_dali, args: { transforms: [ { transform: RandomRotate, args: { p: 0.5 , angle_limit: 45., } }, ] } } ] Arguments : p (float) : probability of applying the transform. Default: 0.5. angle_limit (float,list) : Range for changing angle in [min,max] value format. If provided as a single float, the range will be (-limit, limit). Defaults to 45..","title":"Random Rotate"},{"location":"modules/backbones/","text":"Backbones Network \u00b6 This section listed all available backbone configuration. Part of model.name configurations (if the model support it) in experiment file. Backbones refers to the network which takes as input the image and extracts the feature map upon which the rest of the network is based. Usually, a famous image classification network is used as a backbone for other computer vision tasks, such as object detection. For example we can see figure below : In the figure, Resnet is used as a backbone in the Feature Pyramid Network (FPN) model for object detection. In Vortex, we provide various backbone that can be combined in different models. Backbones params and MAC/FLOPS comparison ( to compare which backbones is the lightest or heaviest ) can be found on this spreadsheet link NOTES : Params : total parameters in the model\u2019s weight MAC / FLOPS : Multiply-Accumulate Operation divided by Floating Point Operations per Second CSPNet \u00b6 Variants from reference CSPNet: A New Backbone that can Enhance Learning Capability of CNN : cspresnet50 cspresnext50 cspdarknet53 Darknet \u00b6 Variants from reference YOLOv3: An Incremental Improvement : darknet53 EfficientNet \u00b6 The pretrained model for EfficientNet base model (b0 - l2) is trained using Noisy Student, except for b8 . Variants from reference EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks : efficientnet_b0 efficientnet_b1 efficientnet_b2 efficientnet_b3 efficientnet_b4 efficientnet_b5 efficientnet_b6 efficientnet_b7 efficientnet_b8 Variants from reference Self-training with Noisy Student improves ImageNet classification : efficientnet_l2 efficientnet_l2_475 Variants from reference EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML : efficientnet_edge_s efficientnet_edge_m efficientnet_edge_l Variants from reference Higher accuracy on vision models with EfficientNet-Lite : efficientnet_lite0 efficientnet_lite1 efficientnet_lite2 efficientnet_lite3 efficientnet_lite4 MobileNet \u00b6 Variants from reference MobileNetV2: Inverted Residuals and Linear Bottlenecks : mobilenet_v2 Variants from reference Searching for MobileNetV3 : mobilenetv3_large_075 mobilenetv3_large_100 mobilenetv3_large_minimal_100 mobilenetv3_small_075 mobilenetv3_small_100 mobilenetv3_small_minimal_100 mobilenetv3_rw RegNet \u00b6 Variants from reference Designing Network Design Spaces : regnetx_002 regnetx_004 regnetx_006 regnetx_008 regnetx_016 regnetx_032 regnetx_040 regnetx_064 regnetx_080 regnetx_120 regnetx_160 regnetx_320 regnety_002 regnety_004 regnety_006 regnety_008 regnety_016 regnety_032 regnety_040 regnety_064 regnety_080 regnety_120 regnety_160 regnety_320 ResNest \u00b6 Variants from reference ResNeSt: Split-Attention Networks : resnest14 resnest26 resnest50 resnest101 resnest200 resnest269 resnest50d_4s2x40d resnest50d_1s4x24d ResNet \u00b6 Variants from reference Deep Residual Learning for Image Recognition : resnet18 resnet34 resnet50 resnet101 resnet152 Variants from reference Aggregated Residual Transformations for Deep Neural Networks : resnext50_32x4d resnext101_32x8d Variants from reference Wide Residual Networks : wide_resnet50_2 wide_resnet101_2 RexNet \u00b6 Variants from reference ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network : rexnet_100 rexnet_130 rexnet_150 rexnet_200 ShuffleNet \u00b6 Variants from reference ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design : shufflenetv2_x0.5 shufflenetv2_x1.0 shufflenetv2_x1.5 shufflenetv2_x2.0 TResNet \u00b6 Variants from reference TResNet: High Performance GPU-Dedicated Architecture : tresnet_m tresnet_l tresnet_xl tresnet_m_448 tresnet_l_448 tresnet_xl_448 VGG \u00b6 Variants from reference Very Deep Convolutional Networks for Large-Scale Image Recognition : vgg11 vgg11_bn vgg13 vgg13_bn vgg16 vgg16_bn vgg19 vgg19_bn","title":"Backbones Network"},{"location":"modules/backbones/#backbones-network","text":"This section listed all available backbone configuration. Part of model.name configurations (if the model support it) in experiment file. Backbones refers to the network which takes as input the image and extracts the feature map upon which the rest of the network is based. Usually, a famous image classification network is used as a backbone for other computer vision tasks, such as object detection. For example we can see figure below : In the figure, Resnet is used as a backbone in the Feature Pyramid Network (FPN) model for object detection. In Vortex, we provide various backbone that can be combined in different models. Backbones params and MAC/FLOPS comparison ( to compare which backbones is the lightest or heaviest ) can be found on this spreadsheet link NOTES : Params : total parameters in the model\u2019s weight MAC / FLOPS : Multiply-Accumulate Operation divided by Floating Point Operations per Second","title":"Backbones Network"},{"location":"modules/backbones/#cspnet","text":"Variants from reference CSPNet: A New Backbone that can Enhance Learning Capability of CNN : cspresnet50 cspresnext50 cspdarknet53","title":"CSPNet"},{"location":"modules/backbones/#darknet","text":"Variants from reference YOLOv3: An Incremental Improvement : darknet53","title":"Darknet"},{"location":"modules/backbones/#efficientnet","text":"The pretrained model for EfficientNet base model (b0 - l2) is trained using Noisy Student, except for b8 . Variants from reference EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks : efficientnet_b0 efficientnet_b1 efficientnet_b2 efficientnet_b3 efficientnet_b4 efficientnet_b5 efficientnet_b6 efficientnet_b7 efficientnet_b8 Variants from reference Self-training with Noisy Student improves ImageNet classification : efficientnet_l2 efficientnet_l2_475 Variants from reference EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML : efficientnet_edge_s efficientnet_edge_m efficientnet_edge_l Variants from reference Higher accuracy on vision models with EfficientNet-Lite : efficientnet_lite0 efficientnet_lite1 efficientnet_lite2 efficientnet_lite3 efficientnet_lite4","title":"EfficientNet"},{"location":"modules/backbones/#mobilenet","text":"Variants from reference MobileNetV2: Inverted Residuals and Linear Bottlenecks : mobilenet_v2 Variants from reference Searching for MobileNetV3 : mobilenetv3_large_075 mobilenetv3_large_100 mobilenetv3_large_minimal_100 mobilenetv3_small_075 mobilenetv3_small_100 mobilenetv3_small_minimal_100 mobilenetv3_rw","title":"MobileNet"},{"location":"modules/backbones/#regnet","text":"Variants from reference Designing Network Design Spaces : regnetx_002 regnetx_004 regnetx_006 regnetx_008 regnetx_016 regnetx_032 regnetx_040 regnetx_064 regnetx_080 regnetx_120 regnetx_160 regnetx_320 regnety_002 regnety_004 regnety_006 regnety_008 regnety_016 regnety_032 regnety_040 regnety_064 regnety_080 regnety_120 regnety_160 regnety_320","title":"RegNet"},{"location":"modules/backbones/#resnest","text":"Variants from reference ResNeSt: Split-Attention Networks : resnest14 resnest26 resnest50 resnest101 resnest200 resnest269 resnest50d_4s2x40d resnest50d_1s4x24d","title":"ResNest"},{"location":"modules/backbones/#resnet","text":"Variants from reference Deep Residual Learning for Image Recognition : resnet18 resnet34 resnet50 resnet101 resnet152 Variants from reference Aggregated Residual Transformations for Deep Neural Networks : resnext50_32x4d resnext101_32x8d Variants from reference Wide Residual Networks : wide_resnet50_2 wide_resnet101_2","title":"ResNet"},{"location":"modules/backbones/#rexnet","text":"Variants from reference ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network : rexnet_100 rexnet_130 rexnet_150 rexnet_200","title":"RexNet"},{"location":"modules/backbones/#shufflenet","text":"Variants from reference ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design : shufflenetv2_x0.5 shufflenetv2_x1.0 shufflenetv2_x1.5 shufflenetv2_x2.0","title":"ShuffleNet"},{"location":"modules/backbones/#tresnet","text":"Variants from reference TResNet: High Performance GPU-Dedicated Architecture : tresnet_m tresnet_l tresnet_xl tresnet_m_448 tresnet_l_448 tresnet_xl_448","title":"TResNet"},{"location":"modules/backbones/#vgg","text":"Variants from reference Very Deep Convolutional Networks for Large-Scale Image Recognition : vgg11 vgg11_bn vgg13 vgg13_bn vgg16 vgg16_bn vgg19 vgg19_bn","title":"VGG"},{"location":"modules/builtin_dataset/","text":"Built-in Dataset \u00b6 In this module, we listed all the internally supported dataset to be used in Vortex. Part of dataset.train AND dataset.eval configurations in experiment file. Torchvision Dataset \u00b6 Several torchvision dataset is supported, they are listed below : MNIST FashionMNIST KMNIST EMNIST QMNIST ImageFolder CIFAR10 CIFAR100 SVHN STL10 To use these dataset, set the experiment file using the dataset identifier listed above, and pass the arguments like shown in their respective documentations in the args field. For example : dataset: { train: { dataset: CIFAR10, args: { root: external/datasets, train: True, download: True } }, eval: { dataset: CIFAR10, args: { root: external/datasets, train: False, download: True } } } **IMPORTANT NOTES : transform and target_transform arguments is not supported, augmentation will be supported by Vortex built-in augmentation mechanism specified in this step","title":"Built-in Dataset"},{"location":"modules/builtin_dataset/#built-in-dataset","text":"In this module, we listed all the internally supported dataset to be used in Vortex. Part of dataset.train AND dataset.eval configurations in experiment file.","title":"Built-in Dataset"},{"location":"modules/builtin_dataset/#torchvision-dataset","text":"Several torchvision dataset is supported, they are listed below : MNIST FashionMNIST KMNIST EMNIST QMNIST ImageFolder CIFAR10 CIFAR100 SVHN STL10 To use these dataset, set the experiment file using the dataset identifier listed above, and pass the arguments like shown in their respective documentations in the args field. For example : dataset: { train: { dataset: CIFAR10, args: { root: external/datasets, train: True, download: True } }, eval: { dataset: CIFAR10, args: { root: external/datasets, train: False, download: True } } } **IMPORTANT NOTES : transform and target_transform arguments is not supported, augmentation will be supported by Vortex built-in augmentation mechanism specified in this step","title":"Torchvision Dataset"},{"location":"modules/data_loader/","text":"Data Loader \u00b6 This section listed all available dataloader modules configurations. Vortex data loader module will normalize and resize the input image following preprocess_args configuration in model section. The resize function, will resize the image by the value provided in input_size configuration and pad the image to square by adding black pixel (0,0,0). However the padding implementation will be different depends on the dataloader module (this is subject to change in the future) Pytorch Data Loader \u00b6 A standard Pytorch data loader. If the provided image data is image path, it will decode the image using OpenCV in the BGR format This data loader will pad the image in the shortest side by adding pad pixel in both edge. (E.g. if the shortest side is image's width, it will pad pixel in the left and right side of the image). The implementation is provided by albumentations augmentation PadIfNeeded Cannot be used with nvidia_dali augmentation module dataloader: { module: PytorchDataLoader, args: { num_workers: 0, batch_size: 16, shuffle: True, }, } It\u2019s important to be noted that argument that expect function as its input is not supported, detailed arguments can be inspected in this page Common used arguments : num_workers (int) : how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process batch_size (int) : how many samples per batch to load (default: 1) shuffle (bool) : set to True to have the data reshuffled at every epoch (default: False). Nvidia DALI Data Loader \u00b6 A data loader used together with Nvidia DALI pipeline and must utilize a GPU. The input data provided must be the path to the image (cannot accept numpy array), and it will decode the image on the GPU using ops.ImageDecoder into BGR format. This data loader will pad the image in the right-bottom style. The implementation is provided by ops.Paste Can be used with nvidia_dali augmentation module by specifying the module in the first order of augmentations configuration list dataloader: { module: DALIDataLoader, args: { num_thread: 1, device_id: 0, batch_size: 16, shuffle: True, }, } Arguments : num_thread (int) : Number of CPU threads used by the Nvidia DALI pipeline. (default: 1) device_id (int) : Id of GPU used by the pipeline. (default: 0) batch_size (int) : how many samples per batch to load (default: 1) shuffle (bool) : set to True to have the data reshuffled at every epoch (default: False).","title":"Data Loader"},{"location":"modules/data_loader/#data-loader","text":"This section listed all available dataloader modules configurations. Vortex data loader module will normalize and resize the input image following preprocess_args configuration in model section. The resize function, will resize the image by the value provided in input_size configuration and pad the image to square by adding black pixel (0,0,0). However the padding implementation will be different depends on the dataloader module (this is subject to change in the future)","title":"Data Loader"},{"location":"modules/data_loader/#pytorch-data-loader","text":"A standard Pytorch data loader. If the provided image data is image path, it will decode the image using OpenCV in the BGR format This data loader will pad the image in the shortest side by adding pad pixel in both edge. (E.g. if the shortest side is image's width, it will pad pixel in the left and right side of the image). The implementation is provided by albumentations augmentation PadIfNeeded Cannot be used with nvidia_dali augmentation module dataloader: { module: PytorchDataLoader, args: { num_workers: 0, batch_size: 16, shuffle: True, }, } It\u2019s important to be noted that argument that expect function as its input is not supported, detailed arguments can be inspected in this page Common used arguments : num_workers (int) : how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process batch_size (int) : how many samples per batch to load (default: 1) shuffle (bool) : set to True to have the data reshuffled at every epoch (default: False).","title":"Pytorch Data Loader"},{"location":"modules/data_loader/#nvidia-dali-data-loader","text":"A data loader used together with Nvidia DALI pipeline and must utilize a GPU. The input data provided must be the path to the image (cannot accept numpy array), and it will decode the image on the GPU using ops.ImageDecoder into BGR format. This data loader will pad the image in the right-bottom style. The implementation is provided by ops.Paste Can be used with nvidia_dali augmentation module by specifying the module in the first order of augmentations configuration list dataloader: { module: DALIDataLoader, args: { num_thread: 1, device_id: 0, batch_size: 16, shuffle: True, }, } Arguments : num_thread (int) : Number of CPU threads used by the Nvidia DALI pipeline. (default: 1) device_id (int) : Id of GPU used by the pipeline. (default: 0) batch_size (int) : how many samples per batch to load (default: 1) shuffle (bool) : set to True to have the data reshuffled at every epoch (default: False).","title":"Nvidia DALI Data Loader"},{"location":"modules/exporter/","text":"Graph Exporter \u00b6 This section listed all available exporter configuration. Part of graph exporter section in experiment file. ONNX \u00b6 This module will produce ONNX IR from a trained Vortex model. Further reading : onnx.ai Currently, we provide export support for opset version 9,10, and 11. However it must be noted that not all models and backbones are supported to be converted to ONNX. The list of supported and not supported models and backbones can be found in the COMPATIBILITY REPORT on the repo : COMPATIBILITY REPORT Opset Version 9 COMPATIBILITY REPORT Opset Version 10 COMPATIBILITY REPORT Opset Version 11 E.g. : exporter: { module: onnx, args: { n_batch: 4, opset_version: 11, filename: somemodel_bs4 }, } Arguments : n_batch (int) : number of input batches that can be processed by this model at a time. An IR graph input batch size must be pre configured during export and cannot be modified afterwise opset_version (int) : selected ONNX opset version. For complete information check this link filename (str) : A new filename which will be given to the exported IR with .onnx suffix. If not given, the default is {experiment_name}.onnx Outputs : ONNX IR : ONNX file model (*.onnx) will be produced at experiment output directory Torchscript \u00b6 This module will produce Torchscript IR from a trained Vortex model. Further reading : torchscript E.g. : exporter: { module: torchscript, args: { n_batch: 4, filename: somemodel_bs4 }, } Arguments : n_batch (int) : number of input batches that can be processed by this model at a time. An IR graph input batch size must be pre configured during export and cannot be modified afterwise filename (str) : A new filename which will be given to the exported IR with .pt suffix. If not given, the default is {experiment_name}.pt Outputs : Torhscript IR : Torchscript file model (*.pt) will be produced at experiment output directory","title":"Graph Exporter"},{"location":"modules/exporter/#graph-exporter","text":"This section listed all available exporter configuration. Part of graph exporter section in experiment file.","title":"Graph Exporter"},{"location":"modules/exporter/#onnx","text":"This module will produce ONNX IR from a trained Vortex model. Further reading : onnx.ai Currently, we provide export support for opset version 9,10, and 11. However it must be noted that not all models and backbones are supported to be converted to ONNX. The list of supported and not supported models and backbones can be found in the COMPATIBILITY REPORT on the repo : COMPATIBILITY REPORT Opset Version 9 COMPATIBILITY REPORT Opset Version 10 COMPATIBILITY REPORT Opset Version 11 E.g. : exporter: { module: onnx, args: { n_batch: 4, opset_version: 11, filename: somemodel_bs4 }, } Arguments : n_batch (int) : number of input batches that can be processed by this model at a time. An IR graph input batch size must be pre configured during export and cannot be modified afterwise opset_version (int) : selected ONNX opset version. For complete information check this link filename (str) : A new filename which will be given to the exported IR with .onnx suffix. If not given, the default is {experiment_name}.onnx Outputs : ONNX IR : ONNX file model (*.onnx) will be produced at experiment output directory","title":"ONNX"},{"location":"modules/exporter/#torchscript","text":"This module will produce Torchscript IR from a trained Vortex model. Further reading : torchscript E.g. : exporter: { module: torchscript, args: { n_batch: 4, filename: somemodel_bs4 }, } Arguments : n_batch (int) : number of input batches that can be processed by this model at a time. An IR graph input batch size must be pre configured during export and cannot be modified afterwise filename (str) : A new filename which will be given to the exported IR with .pt suffix. If not given, the default is {experiment_name}.pt Outputs : Torhscript IR : Torchscript file model (*.pt) will be produced at experiment output directory","title":"Torchscript"},{"location":"modules/logging_provider/","text":"Logging Provider \u00b6 This section listed all available logging configuration. Part of logging section in experiment file. No Logging \u00b6 This configuration will disable any logging logging: None Comet-ML \u00b6 This configuration will enable experiment logging to comet.ml . Make sure that you have an account there. logging: { module: 'comet_ml', args: { api_key: HG65hasJHGFshuasg67, project_name: vortex-classification, workspace: hyperion-rg }, pytz_timezone: 'Asia/Jakarta' } Required Arguments : api_key : comet.ml user personal API key project_name : the experiment\u2019s project group workspace : the user\u2019s workspace Additional Arguments : See this link","title":"Logging Provider"},{"location":"modules/logging_provider/#logging-provider","text":"This section listed all available logging configuration. Part of logging section in experiment file.","title":"Logging Provider"},{"location":"modules/logging_provider/#no-logging","text":"This configuration will disable any logging logging: None","title":"No Logging"},{"location":"modules/logging_provider/#comet-ml","text":"This configuration will enable experiment logging to comet.ml . Make sure that you have an account there. logging: { module: 'comet_ml', args: { api_key: HG65hasJHGFshuasg67, project_name: vortex-classification, workspace: hyperion-rg }, pytz_timezone: 'Asia/Jakarta' } Required Arguments : api_key : comet.ml user personal API key project_name : the experiment\u2019s project group workspace : the user\u2019s workspace Additional Arguments : See this link","title":"Comet-ML"},{"location":"modules/models_zoo/","text":"Models Zoo \u00b6 This section listed all available model configuration. Part of model section in experiment file. Classification \u00b6 This task is about predicting a category for an input image. Softmax \u00b6 This classification model supports single-category multi-class classification objectives. The output will be a class with the highest prediction probability score (between 0 and 1) Reference : Softmax and probabilities E.g. : model: { name: softmax, network_args: { backbone: shufflenetv2_x1.0, n_classes: 10, pretrained_backbone: True, freeze_backbone: False },[ preprocess_args: { input_size: 224, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, loss_args: { loss: ce_loss, reduction: mean, additional_args: {} }, postprocess_args: {}, init_state_dict: somepath.pth } Arguments \u00b6 preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section n_classes (int) : number of classes pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training loss_args : loss (str) : classification loss to be used, options : ce_loss - will use nn.NLLLoss from PyTorch, see this documentations for details. focal_loss - will use Vortex implementation of Focal loss, referenced from this repo reduction (str) : reduction of loss array, either sum or mean additional_args (dict) : additional arguments to be forwarded to the loss function, options: for ce_loss , see the documentation for additional arguments for focal_loss , see this link for further details, sub-arguments : gamma (float) : focusing parameter for modulating factor (1-p) alpha (List[float]) : per-class weighting array. For example, if you have a classification task with 3 class, you need to provide the weight in an array of size 3 = [0.1 ,0.2, 0.7] postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section Outputs \u00b6 List of dictionary of np.ndarray pair, output key : class_label ( np.ndarray ) : array with size of 1, each column represents class label class_confidence ( np.ndarray ) : array with size of 1, each column represents class confidence NOTES : row orders are consistent : class_confidence[i] is associated with class_label[i] Detection \u00b6 This task is about predicting multiple objects location as pixel coordinates and its category from input image. Detection models\u2019 params and MAC/FLOPS comparison ( to compare which models is the lightest or heaviest ) can be found on this spreadsheet link FPN-SSD \u00b6 This implementation is basically RetinaNet without (not yet) focal loss implemented. DISCLAIMER : THIS MODEL IS NOT VERIFIED YET, MAY PRODUCE BAD RESULT Reference : Focal Loss for Dense Object Detection E.g. model: { name: FPNSSD, preprocess_args: { input_size: 512, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, network_args: { backbone: shufflenetv2_x1.0, n_classes: 20, pyramid_channels: 256, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, freeze_backbone: False }, loss_args: { neg_pos: 3, overlap_thresh: 0.5, }, postprocess_args: {}, init_state_dict: somepath.pth } Arguments \u00b6 preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section n_classes (int) : number of classes pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training pyramid_channels (int) : number of channels for FPN top-down pyramid, default to 256, could be modified if necessary (e.g. for accuracy vs speed trade-off) (the lower the number, speed will increase)(the lower the number, speed will increase) aspect_ratios (list) : aspect ratio for anchor box loss_args : neg_pos (int) : negative (background) to positive (object) ratio for Hard Negative Mining overlap_thresh (float) : minimum iou threshold to be considered as positive during training postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section Outputs \u00b6 List of dictionary of np.ndarray pair, output key : bounding_box ( np.ndarray ) : array with size n_detections * 4 , each column on x_min,y_min,x_max,y_max format class_label ( np.ndarray ) : array with size of n_detectionx * 1 , each column represents class label class_confidence ( np.ndarray ) : array with size of n_detections * 1 , each column represents class confidence NOTES : row orders are consistent : bounding_box[i] is associated with class_label[i] , etc.. Detection (with Landmarks/Keypoints) \u00b6 This task is about predicting multiple objects location as pixel coordinates, its category, and objects landmarks/keypoints in [x,y] coordinates from input image. Detection models\u2019 params and MAC/FLOPS comparison ( to compare which models is the lightest or heaviest ) can be found on this spreadsheet link RetinaFace \u00b6 This model perform 1 class object detection with 5 key points estimation, originally used for face detection and face landmarks prediction DISCLAIMER : THIS MODEL IS NOT VERIFIED YET, MAY PRODUCE BAD RESULT Reference : RetinaFace: Single-stage Dense Face Localisation in the Wild E.g. : model: { name: RetinaFace, preprocess_args: { input_size: 640, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ] } }, network_args: { backbone: shufflenetv2_x1.0, pyramid_channels: 64, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, freeze_backbone: False }, loss_args: { neg_pos: 7, overlap_thresh: 0.35, cls: 2.0, box: 1.0, ldm: 1.0, }, postprocess_args: {}, init_state_dict: somepath.pth } Arguments \u00b6 preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training pyramid_channels (int) : number of channels for FPN top-down pyramid, default to 256, could be modified if necessary (e.g. for accuracy vs speed trade-off) (the lower the number, speed will increase)(the lower the number, speed will increase) aspect_ratios (list) : aspect ratio for anchor box loss_args : neg_pos (int) : negative (background) to positive (object) ratio for Hard Negative Mining overlap_thresh (float) : minimum iou threshold to be considered as positive during training cls (float) : weight for classification loss box (float) : weight for bounding box loss ldm (float) : weight for landmark regression loss postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section Outputs \u00b6 List of dictionary of np.ndarray pair, output key : bounding_box ( np.ndarray ) : array with size n_detections * 4 , each column on x_min,y_min,x_max,y_max format class_label ( np.ndarray ) : array with size of n_detectionx * 1 , each column represents class label class_confidence ( np.ndarray ) : array with size of n_detections * 1 , each column represents class confidence landmarks ( np.ndarray ) : array with size of n_detections x 10 , each column represents landmark position with xy format, e.g. [p1x, p1y, p2x, p2y, \u2026] NOTES : row orders are consistent : bounding_box[i] is associated with class_label[i] , etc.. DETR \u00b6 Implementation of facebook research's DETR model DISCLAIMER : THIS MODEL STILL COULD NOT BE EXPORTED Reference : End-to-End Object Detection with Transformers Example config: model : { name : DETR, preprocess_args : { input_size : 800, input_normalization : { mean : [0.485, 0.456, 0.406], std : [0.229, 0.224, 0.225], scaler : 255 } }, network_args : { backbone : resnet50, n_classes : 20, pretrained_backbone: True, num_decoder_layers: 6, aux_loss: True, lr_backbone: 0.00001, }, loss_args : {}, postprocess_args : {} } Arguments \u00b6 preprocess_args : see this section network_args : backbone (str): backbone name. supported backbones network is provided at backbones network section n_classes (int): number of object classes. pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) train_backbone (bool, optional) : whether to train the backbones or not. Default: True num_queries (int, optional): number of object queries, i.e. detection slot. This is the maximum number of objects DETR can detect in a single image. Default: 100 aux_loss (bool, optional): whether to use auxiliary decoding losses (loss at each decoder layer). Default: True position_embedding (str, optional): position embedding layer to be used. Available: [ sine , learned ]. Default: sine hidden_dim (int, optional): number of hidden dimension of position embedding, transformer, and MLP layer. Default: 256 nhead (int, optional): number of heads in the multiheadattention models. Default: 8 num_encoder_layers (int, optional): number of sub-encoder-layers of encoder in transformer layer. Default: 6 num_decoder_layers (int, optional): number of sub-decoder-layers of decoder in transformer layer. Deafult: 6 dim_feedforward (int, optional): dimension of the feedforward transformer layer. Default: 2048 dropout (float, optional): dropout value of transformer model. Deafult: 0.1 activation (str, optional): activation layer name. Available: ['relu', 'gelu', 'glu']. Deafult: 'relu' lr_backbone (float, optional): backbone network learning rate value. Default: 1e-5 loss_args : matcher (str, optional): matcher name to compute a matching between targets and proposals. Available: 'hungairan'. Default: 'hungarian' eos_coef (float, optional): relative classification weight applied to the no-object category. Default: 0.1 weight_ce (float, optional): cross entropy loss weight. Default: 1.0 weight_bbox : bounding box loss weight. Default: 5.0 weight_giou : generalized iou loss weight. Default: 2.0 postprocess_args : No available arguments Outputs \u00b6 List of dictionary of np.ndarray pair, output key : bounding_box ( np.ndarray ) : array with size n_detections * 4 , each column on x_min,y_min,x_max,y_max format class_label ( np.ndarray ) : array with size of n_detectionx * 1 , each column represents class label class_confidence ( np.ndarray ) : array with size of n_detections * 1 , each column represents class confidence NOTES : row orders are consistent : bounding_box[i] is associated with class_label[i] , etc..","title":"Models Zoo"},{"location":"modules/models_zoo/#models-zoo","text":"This section listed all available model configuration. Part of model section in experiment file.","title":"Models Zoo"},{"location":"modules/models_zoo/#classification","text":"This task is about predicting a category for an input image.","title":"Classification"},{"location":"modules/models_zoo/#softmax","text":"This classification model supports single-category multi-class classification objectives. The output will be a class with the highest prediction probability score (between 0 and 1) Reference : Softmax and probabilities E.g. : model: { name: softmax, network_args: { backbone: shufflenetv2_x1.0, n_classes: 10, pretrained_backbone: True, freeze_backbone: False },[ preprocess_args: { input_size: 224, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, loss_args: { loss: ce_loss, reduction: mean, additional_args: {} }, postprocess_args: {}, init_state_dict: somepath.pth }","title":"Softmax"},{"location":"modules/models_zoo/#arguments","text":"preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section n_classes (int) : number of classes pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training loss_args : loss (str) : classification loss to be used, options : ce_loss - will use nn.NLLLoss from PyTorch, see this documentations for details. focal_loss - will use Vortex implementation of Focal loss, referenced from this repo reduction (str) : reduction of loss array, either sum or mean additional_args (dict) : additional arguments to be forwarded to the loss function, options: for ce_loss , see the documentation for additional arguments for focal_loss , see this link for further details, sub-arguments : gamma (float) : focusing parameter for modulating factor (1-p) alpha (List[float]) : per-class weighting array. For example, if you have a classification task with 3 class, you need to provide the weight in an array of size 3 = [0.1 ,0.2, 0.7] postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section","title":"Arguments"},{"location":"modules/models_zoo/#outputs","text":"List of dictionary of np.ndarray pair, output key : class_label ( np.ndarray ) : array with size of 1, each column represents class label class_confidence ( np.ndarray ) : array with size of 1, each column represents class confidence NOTES : row orders are consistent : class_confidence[i] is associated with class_label[i]","title":"Outputs"},{"location":"modules/models_zoo/#detection","text":"This task is about predicting multiple objects location as pixel coordinates and its category from input image. Detection models\u2019 params and MAC/FLOPS comparison ( to compare which models is the lightest or heaviest ) can be found on this spreadsheet link","title":"Detection"},{"location":"modules/models_zoo/#fpn-ssd","text":"This implementation is basically RetinaNet without (not yet) focal loss implemented. DISCLAIMER : THIS MODEL IS NOT VERIFIED YET, MAY PRODUCE BAD RESULT Reference : Focal Loss for Dense Object Detection E.g. model: { name: FPNSSD, preprocess_args: { input_size: 512, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, network_args: { backbone: shufflenetv2_x1.0, n_classes: 20, pyramid_channels: 256, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, freeze_backbone: False }, loss_args: { neg_pos: 3, overlap_thresh: 0.5, }, postprocess_args: {}, init_state_dict: somepath.pth }","title":"FPN-SSD"},{"location":"modules/models_zoo/#arguments_1","text":"preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section n_classes (int) : number of classes pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training pyramid_channels (int) : number of channels for FPN top-down pyramid, default to 256, could be modified if necessary (e.g. for accuracy vs speed trade-off) (the lower the number, speed will increase)(the lower the number, speed will increase) aspect_ratios (list) : aspect ratio for anchor box loss_args : neg_pos (int) : negative (background) to positive (object) ratio for Hard Negative Mining overlap_thresh (float) : minimum iou threshold to be considered as positive during training postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section","title":"Arguments"},{"location":"modules/models_zoo/#outputs_1","text":"List of dictionary of np.ndarray pair, output key : bounding_box ( np.ndarray ) : array with size n_detections * 4 , each column on x_min,y_min,x_max,y_max format class_label ( np.ndarray ) : array with size of n_detectionx * 1 , each column represents class label class_confidence ( np.ndarray ) : array with size of n_detections * 1 , each column represents class confidence NOTES : row orders are consistent : bounding_box[i] is associated with class_label[i] , etc..","title":"Outputs"},{"location":"modules/models_zoo/#detection-with-landmarkskeypoints","text":"This task is about predicting multiple objects location as pixel coordinates, its category, and objects landmarks/keypoints in [x,y] coordinates from input image. Detection models\u2019 params and MAC/FLOPS comparison ( to compare which models is the lightest or heaviest ) can be found on this spreadsheet link","title":"Detection (with Landmarks/Keypoints)"},{"location":"modules/models_zoo/#retinaface","text":"This model perform 1 class object detection with 5 key points estimation, originally used for face detection and face landmarks prediction DISCLAIMER : THIS MODEL IS NOT VERIFIED YET, MAY PRODUCE BAD RESULT Reference : RetinaFace: Single-stage Dense Face Localisation in the Wild E.g. : model: { name: RetinaFace, preprocess_args: { input_size: 640, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ] } }, network_args: { backbone: shufflenetv2_x1.0, pyramid_channels: 64, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, freeze_backbone: False }, loss_args: { neg_pos: 7, overlap_thresh: 0.35, cls: 2.0, box: 1.0, ldm: 1.0, }, postprocess_args: {}, init_state_dict: somepath.pth }","title":"RetinaFace"},{"location":"modules/models_zoo/#arguments_2","text":"preprocess_args : see this section network_args : backbone (str) : backbones network name, supported backbones network is provided at backbones network section pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) freeze_backbone (bool) : freeze the backbone weight, if the backbone is frozen the weight in backbone model will not be updated during training pyramid_channels (int) : number of channels for FPN top-down pyramid, default to 256, could be modified if necessary (e.g. for accuracy vs speed trade-off) (the lower the number, speed will increase)(the lower the number, speed will increase) aspect_ratios (list) : aspect ratio for anchor box loss_args : neg_pos (int) : negative (background) to positive (object) ratio for Hard Negative Mining overlap_thresh (float) : minimum iou threshold to be considered as positive during training cls (float) : weight for classification loss box (float) : weight for bounding box loss ldm (float) : weight for landmark regression loss postprocess_args : you can leave this field with empty dict {} init_state_dict : see this section","title":"Arguments"},{"location":"modules/models_zoo/#outputs_2","text":"List of dictionary of np.ndarray pair, output key : bounding_box ( np.ndarray ) : array with size n_detections * 4 , each column on x_min,y_min,x_max,y_max format class_label ( np.ndarray ) : array with size of n_detectionx * 1 , each column represents class label class_confidence ( np.ndarray ) : array with size of n_detections * 1 , each column represents class confidence landmarks ( np.ndarray ) : array with size of n_detections x 10 , each column represents landmark position with xy format, e.g. [p1x, p1y, p2x, p2y, \u2026] NOTES : row orders are consistent : bounding_box[i] is associated with class_label[i] , etc..","title":"Outputs"},{"location":"modules/models_zoo/#detr","text":"Implementation of facebook research's DETR model DISCLAIMER : THIS MODEL STILL COULD NOT BE EXPORTED Reference : End-to-End Object Detection with Transformers Example config: model : { name : DETR, preprocess_args : { input_size : 800, input_normalization : { mean : [0.485, 0.456, 0.406], std : [0.229, 0.224, 0.225], scaler : 255 } }, network_args : { backbone : resnet50, n_classes : 20, pretrained_backbone: True, num_decoder_layers: 6, aux_loss: True, lr_backbone: 0.00001, }, loss_args : {}, postprocess_args : {} }","title":"DETR"},{"location":"modules/models_zoo/#arguments_3","text":"preprocess_args : see this section network_args : backbone (str): backbone name. supported backbones network is provided at backbones network section n_classes (int): number of object classes. pretrained_backbone (bool) : using the provided pretrained backbone for weight initialization (transfer learning) train_backbone (bool, optional) : whether to train the backbones or not. Default: True num_queries (int, optional): number of object queries, i.e. detection slot. This is the maximum number of objects DETR can detect in a single image. Default: 100 aux_loss (bool, optional): whether to use auxiliary decoding losses (loss at each decoder layer). Default: True position_embedding (str, optional): position embedding layer to be used. Available: [ sine , learned ]. Default: sine hidden_dim (int, optional): number of hidden dimension of position embedding, transformer, and MLP layer. Default: 256 nhead (int, optional): number of heads in the multiheadattention models. Default: 8 num_encoder_layers (int, optional): number of sub-encoder-layers of encoder in transformer layer. Default: 6 num_decoder_layers (int, optional): number of sub-decoder-layers of decoder in transformer layer. Deafult: 6 dim_feedforward (int, optional): dimension of the feedforward transformer layer. Default: 2048 dropout (float, optional): dropout value of transformer model. Deafult: 0.1 activation (str, optional): activation layer name. Available: ['relu', 'gelu', 'glu']. Deafult: 'relu' lr_backbone (float, optional): backbone network learning rate value. Default: 1e-5 loss_args : matcher (str, optional): matcher name to compute a matching between targets and proposals. Available: 'hungairan'. Default: 'hungarian' eos_coef (float, optional): relative classification weight applied to the no-object category. Default: 0.1 weight_ce (float, optional): cross entropy loss weight. Default: 1.0 weight_bbox : bounding box loss weight. Default: 5.0 weight_giou : generalized iou loss weight. Default: 2.0 postprocess_args : No available arguments","title":"Arguments"},{"location":"modules/models_zoo/#outputs_3","text":"List of dictionary of np.ndarray pair, output key : bounding_box ( np.ndarray ) : array with size n_detections * 4 , each column on x_min,y_min,x_max,y_max format class_label ( np.ndarray ) : array with size of n_detectionx * 1 , each column represents class label class_confidence ( np.ndarray ) : array with size of n_detections * 1 , each column represents class confidence NOTES : row orders are consistent : bounding_box[i] is associated with class_label[i] , etc..","title":"Outputs"},{"location":"modules/scheduler/","text":"Learning Rates Scheduler \u00b6 This section listed all available lr_scheduler configuration. Part of trainer configurations in experiment file. Additionally, we have a utility script scripts/visualize_learning_rate.py on our repository to help user visualize their learning rate scheduler. usage: visualize_learning_rate.py [-h] [-c CONFIG] [--epochs EPOCHS] [CFG] Visualize learning rate scheduler positional arguments: CFG experiment configuration file path (.yml) optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG experiment configuration file path (choose either one of this or positional argument) --epochs EPOCHS number of epoch of learning rate to be calculated, override epoxh in experiment file E.g. : python3 scripts/visualize_learning_rate.py -c experiments/configs/shufflenetv2x100_retinaface_frontal_fddb_640.yml --epochs 300 Pytorch Scheduler \u00b6 Vortex support several Pytorch original scheduler implementation. User only need to specify the arguments beside the optimizer arguments which have already handled internally StepLR \u00b6 StepLR Documentation E.g. : lr_scheduler: { method: StepLR, args: { step_size: 100, gamma: .1, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300, MultiStepLR \u00b6 MultiStepLR Documentation E.g. : lr_scheduler: { method: MultiStepLR, args: { milestones: [225,275], gamma: .1, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300, ExponentialLR \u00b6 ExponentialLR Documentation E.g. : lr_scheduler: { method: ExponentialLR, args: { gamma: .98, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300, CosineAnnealingLR \u00b6 CosineAnnealingLR Documentation E.g. : lr_scheduler: { method: CosineAnnealingLR, args: { T_max: 100, eta_min: 0.00001, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300, CosineAnnealingWarmRestarts \u00b6 CosineAnnealingWarmRestarts Documentation E.g. : lr_scheduler: { method: CosineAnnealingWarmRestarts, args: { T_0: 50, T_mult: 2, eta_min: .00001, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300, CyclicLR \u00b6 CyclicLR Documentation E.g. : lr_scheduler: { method: CyclicLR, args: { base_lr: 0.00001, max_lr: 0.001, step_size_up: 2000, mode: triangular, gamma: 1.0, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300, OneCycleLR \u00b6 OneCycleLR Documentation E.g. : lr_scheduler: { method: OneCycleLR, args: { max_lr: 0.001, steps_per_epoch: 200, epochs: 300, pct_start: .3, anneal_strategy: cos, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300, Custom Implementation \u00b6 CosineLR With Warm Up \u00b6 Implement Cosine decay scheduler with warm restarts Reference : SGDR: Stochastic Gradient Descent with Warm Restarts allennlp/cosine.py lr_scheduler: { method: CosineLRWithWarmUp, args: { t_initial: 100, t_mul: 1.0, lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 5, cycle_limit: 3, t_in_epochs: True, decay_rate: 0.75, } }, Arguments : t_initial (int) : the number of iterations (epochs) within the first cycle t_mul (float) : determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . default : 1 lr_min (float) : minimum learning rate after decay. default : 0. warmup_lr_init (float) : starting learning rate on warmup stage. default : 0 warmup_t (int) : number of epoch of warmup stage. default : 0 cycle_limit (int) : number of cosine cycle. default : 0 t_in_epochs (bool) : if True, update learning rate per epoch, if not, update per step. default : True decay_rate (float) : learning rate decay rate. default : 1 Example Visualization : Using above configuration, and epoch of 300, TanhLR With Warm Up \u00b6 Implement Hyperbolic-Tangent decay with warm restarts Reference : Stochastic Gradient Descent with Hyperbolic-Tangent Decay on Classification lr_scheduler: { method: TanhLRWithWarmUp, args: { t_initial: 100, t_mul: 1.0, lb: -6., ub: 4., lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 5, cycle_limit: 3, t_in_epochs: True, decay_rate: 0.75, } } Arguments : t_initial (int) : the number of iterations (epochs) within the first cycle t_mul (float) : determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . default : 1 lb (float) : tanh function lower bound value ub (float) : tanh function upper bound value lr_min (float) : Minimum learning rate after decay. default : 0. warmup_lr_init (float) : starting learning rate on warmup stage. default : 0 warmup_t (int) : number of epoch of warmup stage. default : 0 cycle_limit (int) : number of cosine cycle. default : 0 t_in_epochs (bool) : if True, update learning rate per epoch, if not, update per step. default : True decay_rate (float) : learning rate decay rate. default : 1 Example Visualization : Using above configuration, and epoch of 300, StepLR With Warm Up \u00b6 Implement StepLR scheduler with burn in (warm start), adapted from YOLOv3 training method Reference : DeNA/PyTorch_YOLOv3: Implementation of YOLOv3 in PyTorch lr_scheduler: { method: StepLRWithWarmUp, args: { warm_up: 5, steps: [180,190], scales: [.1,.1], last_epoch: -1 } } Arguments : warm_up (int) : number of epochs for warm up steps (list) : list of epoch when the learning rate will be reduced, e.g. [180,190] --> learning rate will be reduced on epoch 180 and epoch 190 scales (list) : scale of the reduced learning rate, e.g. [0.1,0.1] --> e.g. initial lr == 0.01 , on epoch 180 will be reduced to 0.1 * 0.01 = 0.001 and on epoch 190 will be reduced to 0.1 * 0.001 = 0.0001 last_epoch (int) : last epoch number. default : -1 Example Visualization : Using above configuration, and epoch of 200,","title":"Learning Rates Scheduler"},{"location":"modules/scheduler/#learning-rates-scheduler","text":"This section listed all available lr_scheduler configuration. Part of trainer configurations in experiment file. Additionally, we have a utility script scripts/visualize_learning_rate.py on our repository to help user visualize their learning rate scheduler. usage: visualize_learning_rate.py [-h] [-c CONFIG] [--epochs EPOCHS] [CFG] Visualize learning rate scheduler positional arguments: CFG experiment configuration file path (.yml) optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG experiment configuration file path (choose either one of this or positional argument) --epochs EPOCHS number of epoch of learning rate to be calculated, override epoxh in experiment file E.g. : python3 scripts/visualize_learning_rate.py -c experiments/configs/shufflenetv2x100_retinaface_frontal_fddb_640.yml --epochs 300","title":"Learning Rates Scheduler"},{"location":"modules/scheduler/#pytorch-scheduler","text":"Vortex support several Pytorch original scheduler implementation. User only need to specify the arguments beside the optimizer arguments which have already handled internally","title":"Pytorch Scheduler"},{"location":"modules/scheduler/#steplr","text":"StepLR Documentation E.g. : lr_scheduler: { method: StepLR, args: { step_size: 100, gamma: .1, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300,","title":"StepLR"},{"location":"modules/scheduler/#multisteplr","text":"MultiStepLR Documentation E.g. : lr_scheduler: { method: MultiStepLR, args: { milestones: [225,275], gamma: .1, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300,","title":"MultiStepLR"},{"location":"modules/scheduler/#exponentiallr","text":"ExponentialLR Documentation E.g. : lr_scheduler: { method: ExponentialLR, args: { gamma: .98, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300,","title":"ExponentialLR"},{"location":"modules/scheduler/#cosineannealinglr","text":"CosineAnnealingLR Documentation E.g. : lr_scheduler: { method: CosineAnnealingLR, args: { T_max: 100, eta_min: 0.00001, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300,","title":"CosineAnnealingLR"},{"location":"modules/scheduler/#cosineannealingwarmrestarts","text":"CosineAnnealingWarmRestarts Documentation E.g. : lr_scheduler: { method: CosineAnnealingWarmRestarts, args: { T_0: 50, T_mult: 2, eta_min: .00001, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300,","title":"CosineAnnealingWarmRestarts"},{"location":"modules/scheduler/#cycliclr","text":"CyclicLR Documentation E.g. : lr_scheduler: { method: CyclicLR, args: { base_lr: 0.00001, max_lr: 0.001, step_size_up: 2000, mode: triangular, gamma: 1.0, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300,","title":"CyclicLR"},{"location":"modules/scheduler/#onecyclelr","text":"OneCycleLR Documentation E.g. : lr_scheduler: { method: OneCycleLR, args: { max_lr: 0.001, steps_per_epoch: 200, epochs: 300, pct_start: .3, anneal_strategy: cos, last_epoch: -1 } }, Example Visualization : Using above configuration, and epoch of 300,","title":"OneCycleLR"},{"location":"modules/scheduler/#custom-implementation","text":"","title":"Custom Implementation"},{"location":"modules/scheduler/#cosinelr-with-warm-up","text":"Implement Cosine decay scheduler with warm restarts Reference : SGDR: Stochastic Gradient Descent with Warm Restarts allennlp/cosine.py lr_scheduler: { method: CosineLRWithWarmUp, args: { t_initial: 100, t_mul: 1.0, lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 5, cycle_limit: 3, t_in_epochs: True, decay_rate: 0.75, } }, Arguments : t_initial (int) : the number of iterations (epochs) within the first cycle t_mul (float) : determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . default : 1 lr_min (float) : minimum learning rate after decay. default : 0. warmup_lr_init (float) : starting learning rate on warmup stage. default : 0 warmup_t (int) : number of epoch of warmup stage. default : 0 cycle_limit (int) : number of cosine cycle. default : 0 t_in_epochs (bool) : if True, update learning rate per epoch, if not, update per step. default : True decay_rate (float) : learning rate decay rate. default : 1 Example Visualization : Using above configuration, and epoch of 300,","title":"CosineLR With Warm Up"},{"location":"modules/scheduler/#tanhlr-with-warm-up","text":"Implement Hyperbolic-Tangent decay with warm restarts Reference : Stochastic Gradient Descent with Hyperbolic-Tangent Decay on Classification lr_scheduler: { method: TanhLRWithWarmUp, args: { t_initial: 100, t_mul: 1.0, lb: -6., ub: 4., lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 5, cycle_limit: 3, t_in_epochs: True, decay_rate: 0.75, } } Arguments : t_initial (int) : the number of iterations (epochs) within the first cycle t_mul (float) : determines the number of iterations (epochs) in the i-th decay cycle, which is the length of the last cycle multiplied by t_mul . default : 1 lb (float) : tanh function lower bound value ub (float) : tanh function upper bound value lr_min (float) : Minimum learning rate after decay. default : 0. warmup_lr_init (float) : starting learning rate on warmup stage. default : 0 warmup_t (int) : number of epoch of warmup stage. default : 0 cycle_limit (int) : number of cosine cycle. default : 0 t_in_epochs (bool) : if True, update learning rate per epoch, if not, update per step. default : True decay_rate (float) : learning rate decay rate. default : 1 Example Visualization : Using above configuration, and epoch of 300,","title":"TanhLR With Warm Up"},{"location":"modules/scheduler/#steplr-with-warm-up","text":"Implement StepLR scheduler with burn in (warm start), adapted from YOLOv3 training method Reference : DeNA/PyTorch_YOLOv3: Implementation of YOLOv3 in PyTorch lr_scheduler: { method: StepLRWithWarmUp, args: { warm_up: 5, steps: [180,190], scales: [.1,.1], last_epoch: -1 } } Arguments : warm_up (int) : number of epochs for warm up steps (list) : list of epoch when the learning rate will be reduced, e.g. [180,190] --> learning rate will be reduced on epoch 180 and epoch 190 scales (list) : scale of the reduced learning rate, e.g. [0.1,0.1] --> e.g. initial lr == 0.01 , on epoch 180 will be reduced to 0.1 * 0.01 = 0.001 and on epoch 190 will be reduced to 0.1 * 0.001 = 0.0001 last_epoch (int) : last epoch number. default : -1 Example Visualization : Using above configuration, and epoch of 200,","title":"StepLR With Warm Up"},{"location":"modules/train_driver/","text":"Training Driver \u00b6 This section listed all available driver configuration. Part of trainer configurations in experiment file. Default Trainer \u00b6 The basic training driver. A model will be forwarded batched inputs, loss will be calculated and back propagated. This trainer supports gradient accumulation in which backpropagation gradient can be bigger than batch size. For example : Let's say batch_size in dataloader is 16 , accumulation_step is 4 . The gradient calculation will come from batch_size * accumulation_step which is equivalent to 16*4=64 . Thus even if with limited resource, training with simulated larger batch size is possible. E.g. : driver: { module: DefaultTrainer, args: { accumulation_step: 4, } } Arguments : accumulation_step (int) : number of iterations before gradient is back propagated","title":"Training Driver"},{"location":"modules/train_driver/#training-driver","text":"This section listed all available driver configuration. Part of trainer configurations in experiment file.","title":"Training Driver"},{"location":"modules/train_driver/#default-trainer","text":"The basic training driver. A model will be forwarded batched inputs, loss will be calculated and back propagated. This trainer supports gradient accumulation in which backpropagation gradient can be bigger than batch size. For example : Let's say batch_size in dataloader is 16 , accumulation_step is 4 . The gradient calculation will come from batch_size * accumulation_step which is equivalent to 16*4=64 . Thus even if with limited resource, training with simulated larger batch size is possible. E.g. : driver: { module: DefaultTrainer, args: { accumulation_step: 4, } } Arguments : accumulation_step (int) : number of iterations before gradient is back propagated","title":"Default Trainer"},{"location":"user-guides/dataset_integration/","text":"Dataset Integration \u00b6 To integrate external datasets into vortex, you need to follow several standards. This section will describe those standards. Directory Structure Standards \u00b6 The dataset directory must be placed under directory external/datasets . By default, Vortex will search this folder in the current working directory. However, should you place this directory elsewhere, you can set the environment variable VORTEX_DATASET_ROOT so Vortex can find it. Example : export VORTEX_DATASET_ROOT = /home/alvinprayuda /* Means that the dataset is in /home/alvinprayuda/external/datasets */ So, the directory structure will looked like this external/ datasets/ {dataset-directory} Each dataset directory must provide a python module /utils/dataset.py which will act as interface {dataset-directory}/ utils/ dataset.py Python Module Standards \u00b6 The python module can have several dataset interface classes. However it must be noted that a class represents a dataset. E.g. : class VOC0712DetectionDataset : def __init__(self): \"\"\" Implement something here \"\"\" pass All available dataset classes in the utils/dataset.py module must be listed in a variable named supported_datasets . E.g. : supported_dataset = [ 'VOC0712DetectionDataset' ] class VOC0712DetectionDataset : def __init__(self): \"\"\" Implement something here \"\"\" pass The utils/dataset.py interface python module must implement a function create_dataset . This function will receive args from the experiment file. E.g. : class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): \"\"\" Implement something here \"\"\" pass def create_dataset(*args, **kwargs) : return VOC0712DetectionDataset(*args, **kwargs) Dataset Class Standards \u00b6 Each dataset class must implement several mandatory method and attributes: Method __getitem__ and __len__ similar to Pytorch dataset implementation : __len__ function must return the number (int) of dataset e.g. the number of images. E.g. : class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] def __len__(self): return len(self.images) __getitem__ function returned value must be a tuple of image path (str) and its annotations (numpy array). E.g. Classification task, if you choose not to use torchvision's ImageFolder , the returned target's array dimension is 1 class ImageClassificationDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [[0],[2],[1]] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = ImageClassificationDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([0], dtype=float32) \"\"\" Detection task, the returned target's array dimension is 2 import numpy as np class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [ [[0. , 0.5 , 0.5 , 0.3 , 0.2],[0. , 0.2 , 0.3 , 0.4 , 0.5]], [[0. , 0.1 , 0.2 , 0.3 , 0.4]], [[1. , 0.7 , 0.5 , 0.2 , 0.3],[2. , 0.4 , 0.4 , 0.3 , 0.3]], ] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = VOC0712DetectionDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([[0. , 0.5, 0.5, 0.3, 0.2], [0. , 0.2, 0.3, 0.4, 0.5]], dtype=float32) \"\"\" Attribute self.class_names and self.data_format self.class_names contains information about class index to string . The value must be a list with string members which the sequence corresponds to its integer class index. The returned class labels in dataset's target must correspond to this list. E.g. class ImageClassificationDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [[0],[2],[1]] self.class_names = ['cat','dog','bird'] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = ImageClassificationDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([0], dtype=float32) \"\"\" class_label = dataset[0][1] class_label_string_name = dataset.class_names[class_label[0]] print(class_label_string_name) \"\"\" 'cat' This means that class_label = 0 correspond to string 'cat' in the self.class_names \"\"\" self.data_format which explains the format of dataset's target array and will be used to extract information from it. This attribute is specifically different between different tasks. Vortex utilizes numpy.take to slice the information from the dataset's target array. E.g. \"\"\" Example self.data_format = { 'bounding_box' : { 'indices' : [0, 1, 2, 3], 'axis' : 1 }, 'class_label' : { 'indices' : [4], 'axis' : 1 } } 'indices' : [0, 1, 2, 3] -> indicate x,y,w,h index of bounding box notation from labels array 'axis' : 1 -> specify the axis in which we slice the labels array --Example 'x' 'y' 'w' 'h' 'class' target_array=np.array([[ 0.75 , 0.6 , 0.1 , 0.2 , 8 ] [ 0.5 , 0.22 , 0.3 , 0.4 , 7 ]]) Using above data format we can slice the array to get only the bounding boxes coords bbox_array = np.array([[ 0.75 , 0.6 , 0.1 , 0.28] [ 0.5 , 0.22 , 0.3 , 0.4]]) class_array = np.array([[8] [7]]) \"\"\" Classification Task Class Label Data Format \"\"\" Class label data format \"\"\" self.data_format = {'class_label' : None} \"\"\" Because the annotations array size only 1, no need to specify indices and axis However, `self.data_format` is still mandatory \"\"\" Detection Task Class Label Data Format \"\"\" Class label data format \"\"\" \"\"\" Option 1 Indicate a single class notation for object detection \"\"\" self.data_format = {'class_label' : None} \"\"\" Option 2 Indicate a single-category multi-class notation \"\"\" self.data_format = { 'class_label' : { 'indices' : [4], 'axis' : 1 } } \"\"\" Option 3 (FUTURE-PLAN,NOT SUPPORTED YET) Indicate a multi-category multi-class notation \"\"\" self.data_format = { 'class_label' : { 'indices' : [4,5,6], 'axis' : 1 } } \"\"\" Option 4 (FUTURE-PLAN,NOT SUPPORTED YET) Indicate a multi-category multi-class notation with sequential long indexes \"\"\" self.data_format = { 'class_label' : { 'indices' : { 'start' : 4, 'end' : 6 }, 'axis' : 1 } } \"\"\" Explanation 'indices' with dict format and keys 'start' and 'end' will be converted to indices sequence internally \"\"\" Bounding Box Data Format It must be noted that VORTEX utilize [x,y,w,h] bounding box format in a normalized style (range 0 - 1 , [x] and [w] are normalized to image\u2019s width, whereas [y] and [h] normalized to image\u2019s height ) \"\"\" Bounding box data format \"\"\" self.data_format = { 'bounding_box' : { 'indices' : [0, 1, 2, 3], 'axis' : 1 }, } Landmarks (Key Points) Data Format (OPTIONAL) This data format is 'optional' in the sense that not all detection models that support landmark (key points) prediction. Thus if you want to utilize model that predict landmarks, such as RetinaFace, this data format is mandatory Landmarks annotation is presented as a 1-dimensional array which has an even length . E.g. [ x1,y1, x2,y2, x3,y3, x4,y4, x5,y5 ] The given example means that we have 5 landmarks with the coordinates of (x1,y1),(x2,y2),(x3,y3),(x4,y4), and (x5,y5) and also in normalized style (range 0 - 1 , [x] are normalized to image\u2019s width, whereas [y] normalized to image\u2019s height ) \"\"\" Landmarks data format \"\"\" \"\"\" Option 1 Standard implementation \"\"\" self.data_format = { 'landmarks' : { 'indices' : [7,8,9,10,11,12,13,14,15,16], 'axis' : 1 } } \"\"\" Option 2 With asymmetric keypoint declaration \"\"\" self.data_format = { 'landmarks' : { 'indices' : [7,8,9,10,11,12,13,14,15,16], 'asymm_pairs' : [[0,1],[3,4]], 'axis' : 1 } } \"\"\" Option 3 Implementation with long sequences \"\"\" self.data_format = { 'landmarks' : { 'indices' : { 'start' : 7, 'end' : 16 } 'asymm_pairs' : [[0,1],[3,4]], 'axis' : 1 } } \"\"\" Explanation 'indices' : [7,8,9,10,11,12,13,14,15,16] or 'indices' : { 'start' : 7, 'end' : 16 } The implementatiom above indicates a sequence of x,y coordinates (e.g index 7,9,11,13,15 -> x coordinates , index 8,10,12,14,16 -> y coordinates) Indices length must be even number 'asymm_pairs' : [[0,1],[3,4]] Indicates asymmetric key points which can be affected by vertical/ horizontal-flip data augmentation For example : Internally, indices [7,8,9,10,11,12,13,14,15,16] will be converted to [(7,8),(9,10),(11,12),(13,14),(15,16)] which means that the key points indexes are : keypoint 0 -> (7,8) keypoint 1 -> (9,10) keypoint 2 -> (11,12) keypoint 3 -> (13,14) keypoint 4 -> (15,16) In this example, we follow 5 facial landmarks example in which left and right landmarks sequence is crucial keypoint 0 -> (7,8) -> left eye keypoint 1 -> (9,10) -> right eye keypoint 2 -> (11,12) -> nose keypoint 3 -> (13,14) -> left mouth keypoint 4 -> (15,16) -> right mouth To handle this, the data format should specify which key points index have asymmetric relation, in this case keypoint 0-1 and keypoint 3-4, so we annotate them as in a list as [[0,1],[3,4]] \"\"\"","title":"Dataset Integration"},{"location":"user-guides/dataset_integration/#dataset-integration","text":"To integrate external datasets into vortex, you need to follow several standards. This section will describe those standards.","title":"Dataset Integration"},{"location":"user-guides/dataset_integration/#directory-structure-standards","text":"The dataset directory must be placed under directory external/datasets . By default, Vortex will search this folder in the current working directory. However, should you place this directory elsewhere, you can set the environment variable VORTEX_DATASET_ROOT so Vortex can find it. Example : export VORTEX_DATASET_ROOT = /home/alvinprayuda /* Means that the dataset is in /home/alvinprayuda/external/datasets */ So, the directory structure will looked like this external/ datasets/ {dataset-directory} Each dataset directory must provide a python module /utils/dataset.py which will act as interface {dataset-directory}/ utils/ dataset.py","title":"Directory Structure Standards"},{"location":"user-guides/dataset_integration/#python-module-standards","text":"The python module can have several dataset interface classes. However it must be noted that a class represents a dataset. E.g. : class VOC0712DetectionDataset : def __init__(self): \"\"\" Implement something here \"\"\" pass All available dataset classes in the utils/dataset.py module must be listed in a variable named supported_datasets . E.g. : supported_dataset = [ 'VOC0712DetectionDataset' ] class VOC0712DetectionDataset : def __init__(self): \"\"\" Implement something here \"\"\" pass The utils/dataset.py interface python module must implement a function create_dataset . This function will receive args from the experiment file. E.g. : class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): \"\"\" Implement something here \"\"\" pass def create_dataset(*args, **kwargs) : return VOC0712DetectionDataset(*args, **kwargs)","title":"Python Module Standards"},{"location":"user-guides/dataset_integration/#dataset-class-standards","text":"Each dataset class must implement several mandatory method and attributes: Method __getitem__ and __len__ similar to Pytorch dataset implementation : __len__ function must return the number (int) of dataset e.g. the number of images. E.g. : class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] def __len__(self): return len(self.images) __getitem__ function returned value must be a tuple of image path (str) and its annotations (numpy array). E.g. Classification task, if you choose not to use torchvision's ImageFolder , the returned target's array dimension is 1 class ImageClassificationDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [[0],[2],[1]] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = ImageClassificationDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([0], dtype=float32) \"\"\" Detection task, the returned target's array dimension is 2 import numpy as np class VOC0712DetectionDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [ [[0. , 0.5 , 0.5 , 0.3 , 0.2],[0. , 0.2 , 0.3 , 0.4 , 0.5]], [[0. , 0.1 , 0.2 , 0.3 , 0.4]], [[1. , 0.7 , 0.5 , 0.2 , 0.3],[2. , 0.4 , 0.4 , 0.3 , 0.3]], ] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = VOC0712DetectionDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([[0. , 0.5, 0.5, 0.3, 0.2], [0. , 0.2, 0.3, 0.4, 0.5]], dtype=float32) \"\"\" Attribute self.class_names and self.data_format self.class_names contains information about class index to string . The value must be a list with string members which the sequence corresponds to its integer class index. The returned class labels in dataset's target must correspond to this list. E.g. class ImageClassificationDataset : def __init__(self,*args,**kwargs): self.images = ['images/image1.jpg','images/image2.jpg','images/image3.jpg'] self.labels = [[0],[2],[1]] self.class_names = ['cat','dog','bird'] def __getitem__(self, index): img_path = self.images[index] target = np.array(self.labels[index]) return img_path, target dataset = ImageClassificationDataset() print(dataset[0][0]) \"\"\" 'images/image1.jpg' \"\"\" print(dataset[0][1]) \"\"\" array([0], dtype=float32) \"\"\" class_label = dataset[0][1] class_label_string_name = dataset.class_names[class_label[0]] print(class_label_string_name) \"\"\" 'cat' This means that class_label = 0 correspond to string 'cat' in the self.class_names \"\"\" self.data_format which explains the format of dataset's target array and will be used to extract information from it. This attribute is specifically different between different tasks. Vortex utilizes numpy.take to slice the information from the dataset's target array. E.g. \"\"\" Example self.data_format = { 'bounding_box' : { 'indices' : [0, 1, 2, 3], 'axis' : 1 }, 'class_label' : { 'indices' : [4], 'axis' : 1 } } 'indices' : [0, 1, 2, 3] -> indicate x,y,w,h index of bounding box notation from labels array 'axis' : 1 -> specify the axis in which we slice the labels array --Example 'x' 'y' 'w' 'h' 'class' target_array=np.array([[ 0.75 , 0.6 , 0.1 , 0.2 , 8 ] [ 0.5 , 0.22 , 0.3 , 0.4 , 7 ]]) Using above data format we can slice the array to get only the bounding boxes coords bbox_array = np.array([[ 0.75 , 0.6 , 0.1 , 0.28] [ 0.5 , 0.22 , 0.3 , 0.4]]) class_array = np.array([[8] [7]]) \"\"\" Classification Task Class Label Data Format \"\"\" Class label data format \"\"\" self.data_format = {'class_label' : None} \"\"\" Because the annotations array size only 1, no need to specify indices and axis However, `self.data_format` is still mandatory \"\"\" Detection Task Class Label Data Format \"\"\" Class label data format \"\"\" \"\"\" Option 1 Indicate a single class notation for object detection \"\"\" self.data_format = {'class_label' : None} \"\"\" Option 2 Indicate a single-category multi-class notation \"\"\" self.data_format = { 'class_label' : { 'indices' : [4], 'axis' : 1 } } \"\"\" Option 3 (FUTURE-PLAN,NOT SUPPORTED YET) Indicate a multi-category multi-class notation \"\"\" self.data_format = { 'class_label' : { 'indices' : [4,5,6], 'axis' : 1 } } \"\"\" Option 4 (FUTURE-PLAN,NOT SUPPORTED YET) Indicate a multi-category multi-class notation with sequential long indexes \"\"\" self.data_format = { 'class_label' : { 'indices' : { 'start' : 4, 'end' : 6 }, 'axis' : 1 } } \"\"\" Explanation 'indices' with dict format and keys 'start' and 'end' will be converted to indices sequence internally \"\"\" Bounding Box Data Format It must be noted that VORTEX utilize [x,y,w,h] bounding box format in a normalized style (range 0 - 1 , [x] and [w] are normalized to image\u2019s width, whereas [y] and [h] normalized to image\u2019s height ) \"\"\" Bounding box data format \"\"\" self.data_format = { 'bounding_box' : { 'indices' : [0, 1, 2, 3], 'axis' : 1 }, } Landmarks (Key Points) Data Format (OPTIONAL) This data format is 'optional' in the sense that not all detection models that support landmark (key points) prediction. Thus if you want to utilize model that predict landmarks, such as RetinaFace, this data format is mandatory Landmarks annotation is presented as a 1-dimensional array which has an even length . E.g. [ x1,y1, x2,y2, x3,y3, x4,y4, x5,y5 ] The given example means that we have 5 landmarks with the coordinates of (x1,y1),(x2,y2),(x3,y3),(x4,y4), and (x5,y5) and also in normalized style (range 0 - 1 , [x] are normalized to image\u2019s width, whereas [y] normalized to image\u2019s height ) \"\"\" Landmarks data format \"\"\" \"\"\" Option 1 Standard implementation \"\"\" self.data_format = { 'landmarks' : { 'indices' : [7,8,9,10,11,12,13,14,15,16], 'axis' : 1 } } \"\"\" Option 2 With asymmetric keypoint declaration \"\"\" self.data_format = { 'landmarks' : { 'indices' : [7,8,9,10,11,12,13,14,15,16], 'asymm_pairs' : [[0,1],[3,4]], 'axis' : 1 } } \"\"\" Option 3 Implementation with long sequences \"\"\" self.data_format = { 'landmarks' : { 'indices' : { 'start' : 7, 'end' : 16 } 'asymm_pairs' : [[0,1],[3,4]], 'axis' : 1 } } \"\"\" Explanation 'indices' : [7,8,9,10,11,12,13,14,15,16] or 'indices' : { 'start' : 7, 'end' : 16 } The implementatiom above indicates a sequence of x,y coordinates (e.g index 7,9,11,13,15 -> x coordinates , index 8,10,12,14,16 -> y coordinates) Indices length must be even number 'asymm_pairs' : [[0,1],[3,4]] Indicates asymmetric key points which can be affected by vertical/ horizontal-flip data augmentation For example : Internally, indices [7,8,9,10,11,12,13,14,15,16] will be converted to [(7,8),(9,10),(11,12),(13,14),(15,16)] which means that the key points indexes are : keypoint 0 -> (7,8) keypoint 1 -> (9,10) keypoint 2 -> (11,12) keypoint 3 -> (13,14) keypoint 4 -> (15,16) In this example, we follow 5 facial landmarks example in which left and right landmarks sequence is crucial keypoint 0 -> (7,8) -> left eye keypoint 1 -> (9,10) -> right eye keypoint 2 -> (11,12) -> nose keypoint 3 -> (13,14) -> left mouth keypoint 4 -> (15,16) -> right mouth To handle this, the data format should specify which key points index have asymmetric relation, in this case keypoint 0-1 and keypoint 3-4, so we annotate them as in a list as [[0,1],[3,4]] \"\"\"","title":"Dataset Class Standards"},{"location":"user-guides/experiment_file_config/","text":"Experiment File Configuration \u00b6 A YAML experiment file is needed to navigate all of Vortex pipelines. This experiment file will contain and track all configurations during all pipelines and can be a single source of information on how a model is developed. Several examples of experiment file can be inspected on this link . In this guide, we will cover all available sections that can be configured. All available configurations is listed below : Experiment Name \u00b6 Flagged with experiment_name key (str) in the experiment file. This field acts as an experiment identifier and is related to the experiment output directory where the trained model,reports, backups,and etc. will be dumped. E.g. : experiment_name: shufflenetv2x100_softmax_cifar10 Logging \u00b6 Flagged with logging key (dict or str( None )) in the experiment file. This field denotes logging providers that can be used for experiment logging, such as tensorboard, comet-ml, etc. The supported logging provider listed in the logging provider section . E.g. : logging: { module: comet_ml, args: { api_key: HGhusyd76hsGiSbt27688, project_name: vortex-classification, workspace: hyperion-rg }, pytz_timezone: Asia/Jakarta } Arguments : module (str) : denotes a specific logging provider module args (dict) : the corresponding arguments for selected module pytz_timezone (str) : the setting of the recorded experiment run timezone in Vortex local system. All timezone settings can be found in this link Output Directory \u00b6 Flagged with output_directory key (str) in the experiment file. This configuration set the location where experiment's output directory will be created. E.g. : output_directory: experiments/outputs Device \u00b6 Flagged with device key (str) in the experiment file. This configuration set device to run experiment. E.g. : device: 'cuda:0' Arguments : device (str) : set the device to run the experiment in pipelines, whether using CPU cpu or cuda GPU cuda . To use specific GPU device, append :{i} to cuda , E.g. cuda:0 for GPU index 0, cuda:1 for GPU index 1 Reproducibility and cuDNN auto-tune \u00b6 THIS CONFIGURATION IS OPTIONAL (MAY IMPACT TRAINING PERFORMANCE) Flagged with seed key (dict) in the experiment file. This configuration setting can be set if the user wants a model training experiment reproducibility. However as noted in this reference , completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms, even with identical seeds. However we still can make the computation deterministic, to produce similar results. For further information, you can check this link . E.g. : seed: { torch: 0, numpy: 0, cudnn: { deterministic: True, benchmark: False, } } Arguments : torch (int) : set the Pytorch seed numpy (int) : set the Numpy seed cudnn (dict) : cuDNN configurations, sub-arguments : deterministic (bool) : set the Pytorch computation to be deterministic benchmark (bool) : set the cuDNN auto-tuner The cudnn args only need to be specified if the user trains using NVIDIA GPU (CuDNN backend). Additionally, by using this seed configuration you also use cuDNN auto-tune capability by using the following configuration seed: { cudnn: { benchmark: True, } } Dataset \u00b6 Flagged with dataset key (dict) in the experiment file. This is the configurations of the dataset to be used in the pipeline, which includes the dataset name for train and eval dataset. E.g. : dataset: { train: { name: VOC0712DetectionDataset, args: { image_set: train, }, augmentations: [ { module: albumentations, args: { transforms: [ { transform: HorizontalFlip, args: { p: 0.5}}, ], bbox_params: { min_visibility: 0.0, min_area: 0.0 }, visual_debug: False } }, # Example if you want to add another augmentation module # { # module: imgaug, # NOTES : `imgaug` module is not implemented yet, just example # args: { # transforms: [] # } # } ] }, eval: { name: VOC0712DetectionDataset, args: { image_set: val } }, } Arguments : train AND eval (str) ( eval is Optional): denotes the configuration of training and validation dataset respectively, if eval is not provided, in-loop validation process will be skipped. eval is mandatory for validation pipeline. Sub-arguments : dataset (str) : the dataset class names which will be used, mentioned in getting started section step 1. args (dict) : the corresponding arguments to the respective dataset class initialization augmentations (list[dict]) ( train only) : the augmentation configurations for training dataset. Augmentation modules provided in the list will be executed sequentially. sub-arguments (list members as dict) : module (str) : selected augmentation module, see augmentation module section for supported augmentation modules args (dict) : the corresponding arguments for selected module Data Loader \u00b6 Flagged with dataloader key (dict) in the experiment file. This is the configurations of dataloader to be used in the training. E.g. : dataloader: { module: PytorchDataLoader, args: { num_workers: 0, batch_size: 16, shuffle: True, }, } Arguments : dataloader (dict) : denotes the configuration of the dataset iterator module (str) : specify the dataloader module to be used, supported data loader modules is provided at data loader module section args (dict) : the corresponding arguments for selected module Trainer \u00b6 Flagged with trainer key (dict) in the experiment file. This configuration set how we train and several other configurations related to training iterations. E.g. : trainer: { optimizer: { method: SGD, args: { lr: 0.001, momentum: 0.9, weight_decay: 0.0005 } }, lr_scheduler: { method: CosineLRWithWarmUp, args: { t_initial: 200, t_mul: 1.0, lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 3, cycle_limit: 1, t_in_epochs: True, decay_rate: 0.1 } }, epoch: 200, save_epoch: 1, save_best_metrics: [loss,mean_ap], driver: { module: DefaultTrainer, args: { accumulation_step: 4, } } } Arguments : optimizer (dict) : configuration for optimization algorithm. Sub-arguments : method (str) : optimization method identifier, currently support all optimizers supported by Pytorch listed in this link args (dict) : the corresponding arguments to the respective optimizer method lr_scheduler (dict) : methods to adjust the learning rate based on the number of epochs method (str) : scheduler method identifier, supported scheduler methods is provided at learning rate scheduler section args (dict) : the corresponding arguments to the respective scheduler method epoch (int) : number of dataset iteration (epoch) being done on the training dataset. 1 epoch is 1 dataset iteration save_epoch (int, optional) : number of epoch before a model checkpoint being saved for backup. save_best_metrics (str, list, optional) : list of metrics or single metrics name to be monitored to save the best weight. Available options: loss : per-epoch training loss For detection task : mean_ap : validation mean-average precision metrics For classification task : accuracy : validation accuracy metrics precision (micro) : validation micro-average precision metrics precision (macro) : validation macro-average precision metrics precision (weighted) : validation weighted-average precision metrics recall (micro) : validation micro-average recall metrics recall (macro) : validation macro-average recall metrics recall (weighted) : validation weighted-average recall metrics f1_score (micro) : validation micro-average f1_score metrics f1_score (macro) : validation macro-average f1_score metrics f1_score (weighted) : validation weighted-average f1_score metrics driver (dict) : the mechanism on how a training is done in a loop ( iterated over n epochs ). Sub-arguments : module (str) : training driver identifier. Supported training driver methods is provided at training driver section args (dict) : the corresponding arguments to the respective training driver module Validator \u00b6 Flagged with validator key (dict) in the experiment file. This configuration set the validation process in the training iteration. E.g. : validator: { args: { score_threshold: 0.9, iou_threshold: 0.2, }, val_epoch: 10 } Arguments : args (dict) : additional arguments needed for validation process, including but not limited to arguments for model postprocessing which dependent on the model itself. For example : For several detection models ( if needed ): score_threshold (float) : threshold applied to the model\u2019s predicted object confidence score. Only objects with prediction scores higher than this threshold will be considered true objects, else considered as background. iou_threshold (float) : threshold for non-maxima suppression (NMS) intersection over union (IoU) For classification : No additional arguments for this task, you can leave args with empty dict {} val_epoch (int) : periodic number of epoch when the validation process will be executed in the training loop Model \u00b6 Flagged with model key (dict) in the experiment file. This configuration set the selected deep learning model architecture for the specific task. E.g. : model: { name: FPNSSD, preprocess_args: { input_size: 512, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, network_args: { backbone: shufflenetv2_x1.0, n_classes: 20, pyramid_channels: 256, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, }, loss_args: { neg_pos: 3, overlap_thresh: 0.5, }, postprocess_args: { nms: True, } } Arguments : name (str) : model name identifier. List of supported models can be found in models zoo section . E.g. : preprocess_args (dict) : configurations for input data preprocessing. Sub-arguments : input_size (int) : input data size, input image will be resized to square while maintaining aspect ratio, by padding the data with black pixel (0,0,0) input_normalization (dict) : determine how input data will be normalized. Input data will be divided by scaler value accross all channel i.e. output = input / scaler . Then, given mean : [M1,...,Mn] and std : [S1,..,Sn] for n channels, this transform will normalize each channel of the input torch.*Tensor i.e. output[channel] = (input[channel] - mean[channel]) / std[channel] mean (list) : sequence of means for each channel. std (list) : sequence of standard deviations for each channel. scaler (list) : scale value for each pixel, default = 255 network_args (dict) : configuration related to the models architecture hyperparameter correspond to the respective model name identifier loss_args (dict) : configuration related to loss calculation hyperparameter which will be used in the training pipeline, which correspond to the respective model name identifier postprocess_args (dict) : configuration related to postprocess arguments, which correspond to the respective model name identifier Checkpoint \u00b6 Flagged with checkpoint key (str) in the experiment file. This configuration describe the path to Vortex model checkpoint which will be used for continue interrupted training or transfer learning to different cases with same model configuration. E.g. : checkpoint: experiments/outputs/shufflenetv2x100_fpn_ssd_voc2007_512/16575bd31b364539817177ca14147b5d/shufflenetv2x100_fpn_ssd_voc2007_512-epoch-10.pth Graph Exporter \u00b6 Flagged with exporter key (list or dict) in the experiment file. This configuration describe the graph exporter which will be used to convert the Pytorch graph into Intermediate Representation (IR) format. To export graph into multiple IR, use list of dict configurations. E.g. : Single IR exporter : exporter: { module: onnx, args: { opset_version: 11, }, } Multiple IR exporter : exporter: [ { module: onnx, args: { opset_version: 11, }, }, { module: torchscript, args: {}, }, ] Arguments : module (str) : selected exporter module, see exporter section for supported exporter modules args (dict) : the corresponding arguments to the respective exporter module","title":"Experiment File Configuration"},{"location":"user-guides/experiment_file_config/#experiment-file-configuration","text":"A YAML experiment file is needed to navigate all of Vortex pipelines. This experiment file will contain and track all configurations during all pipelines and can be a single source of information on how a model is developed. Several examples of experiment file can be inspected on this link . In this guide, we will cover all available sections that can be configured. All available configurations is listed below :","title":"Experiment File Configuration"},{"location":"user-guides/experiment_file_config/#experiment-name","text":"Flagged with experiment_name key (str) in the experiment file. This field acts as an experiment identifier and is related to the experiment output directory where the trained model,reports, backups,and etc. will be dumped. E.g. : experiment_name: shufflenetv2x100_softmax_cifar10","title":"Experiment Name"},{"location":"user-guides/experiment_file_config/#logging","text":"Flagged with logging key (dict or str( None )) in the experiment file. This field denotes logging providers that can be used for experiment logging, such as tensorboard, comet-ml, etc. The supported logging provider listed in the logging provider section . E.g. : logging: { module: comet_ml, args: { api_key: HGhusyd76hsGiSbt27688, project_name: vortex-classification, workspace: hyperion-rg }, pytz_timezone: Asia/Jakarta } Arguments : module (str) : denotes a specific logging provider module args (dict) : the corresponding arguments for selected module pytz_timezone (str) : the setting of the recorded experiment run timezone in Vortex local system. All timezone settings can be found in this link","title":"Logging"},{"location":"user-guides/experiment_file_config/#output-directory","text":"Flagged with output_directory key (str) in the experiment file. This configuration set the location where experiment's output directory will be created. E.g. : output_directory: experiments/outputs","title":"Output Directory"},{"location":"user-guides/experiment_file_config/#device","text":"Flagged with device key (str) in the experiment file. This configuration set device to run experiment. E.g. : device: 'cuda:0' Arguments : device (str) : set the device to run the experiment in pipelines, whether using CPU cpu or cuda GPU cuda . To use specific GPU device, append :{i} to cuda , E.g. cuda:0 for GPU index 0, cuda:1 for GPU index 1","title":"Device"},{"location":"user-guides/experiment_file_config/#reproducibility-and-cudnn-auto-tune","text":"THIS CONFIGURATION IS OPTIONAL (MAY IMPACT TRAINING PERFORMANCE) Flagged with seed key (dict) in the experiment file. This configuration setting can be set if the user wants a model training experiment reproducibility. However as noted in this reference , completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms, even with identical seeds. However we still can make the computation deterministic, to produce similar results. For further information, you can check this link . E.g. : seed: { torch: 0, numpy: 0, cudnn: { deterministic: True, benchmark: False, } } Arguments : torch (int) : set the Pytorch seed numpy (int) : set the Numpy seed cudnn (dict) : cuDNN configurations, sub-arguments : deterministic (bool) : set the Pytorch computation to be deterministic benchmark (bool) : set the cuDNN auto-tuner The cudnn args only need to be specified if the user trains using NVIDIA GPU (CuDNN backend). Additionally, by using this seed configuration you also use cuDNN auto-tune capability by using the following configuration seed: { cudnn: { benchmark: True, } }","title":"Reproducibility and cuDNN auto-tune"},{"location":"user-guides/experiment_file_config/#dataset","text":"Flagged with dataset key (dict) in the experiment file. This is the configurations of the dataset to be used in the pipeline, which includes the dataset name for train and eval dataset. E.g. : dataset: { train: { name: VOC0712DetectionDataset, args: { image_set: train, }, augmentations: [ { module: albumentations, args: { transforms: [ { transform: HorizontalFlip, args: { p: 0.5}}, ], bbox_params: { min_visibility: 0.0, min_area: 0.0 }, visual_debug: False } }, # Example if you want to add another augmentation module # { # module: imgaug, # NOTES : `imgaug` module is not implemented yet, just example # args: { # transforms: [] # } # } ] }, eval: { name: VOC0712DetectionDataset, args: { image_set: val } }, } Arguments : train AND eval (str) ( eval is Optional): denotes the configuration of training and validation dataset respectively, if eval is not provided, in-loop validation process will be skipped. eval is mandatory for validation pipeline. Sub-arguments : dataset (str) : the dataset class names which will be used, mentioned in getting started section step 1. args (dict) : the corresponding arguments to the respective dataset class initialization augmentations (list[dict]) ( train only) : the augmentation configurations for training dataset. Augmentation modules provided in the list will be executed sequentially. sub-arguments (list members as dict) : module (str) : selected augmentation module, see augmentation module section for supported augmentation modules args (dict) : the corresponding arguments for selected module","title":"Dataset"},{"location":"user-guides/experiment_file_config/#data-loader","text":"Flagged with dataloader key (dict) in the experiment file. This is the configurations of dataloader to be used in the training. E.g. : dataloader: { module: PytorchDataLoader, args: { num_workers: 0, batch_size: 16, shuffle: True, }, } Arguments : dataloader (dict) : denotes the configuration of the dataset iterator module (str) : specify the dataloader module to be used, supported data loader modules is provided at data loader module section args (dict) : the corresponding arguments for selected module","title":"Data Loader"},{"location":"user-guides/experiment_file_config/#trainer","text":"Flagged with trainer key (dict) in the experiment file. This configuration set how we train and several other configurations related to training iterations. E.g. : trainer: { optimizer: { method: SGD, args: { lr: 0.001, momentum: 0.9, weight_decay: 0.0005 } }, lr_scheduler: { method: CosineLRWithWarmUp, args: { t_initial: 200, t_mul: 1.0, lr_min: 0.00001, warmup_lr_init: 0.00001, warmup_t: 3, cycle_limit: 1, t_in_epochs: True, decay_rate: 0.1 } }, epoch: 200, save_epoch: 1, save_best_metrics: [loss,mean_ap], driver: { module: DefaultTrainer, args: { accumulation_step: 4, } } } Arguments : optimizer (dict) : configuration for optimization algorithm. Sub-arguments : method (str) : optimization method identifier, currently support all optimizers supported by Pytorch listed in this link args (dict) : the corresponding arguments to the respective optimizer method lr_scheduler (dict) : methods to adjust the learning rate based on the number of epochs method (str) : scheduler method identifier, supported scheduler methods is provided at learning rate scheduler section args (dict) : the corresponding arguments to the respective scheduler method epoch (int) : number of dataset iteration (epoch) being done on the training dataset. 1 epoch is 1 dataset iteration save_epoch (int, optional) : number of epoch before a model checkpoint being saved for backup. save_best_metrics (str, list, optional) : list of metrics or single metrics name to be monitored to save the best weight. Available options: loss : per-epoch training loss For detection task : mean_ap : validation mean-average precision metrics For classification task : accuracy : validation accuracy metrics precision (micro) : validation micro-average precision metrics precision (macro) : validation macro-average precision metrics precision (weighted) : validation weighted-average precision metrics recall (micro) : validation micro-average recall metrics recall (macro) : validation macro-average recall metrics recall (weighted) : validation weighted-average recall metrics f1_score (micro) : validation micro-average f1_score metrics f1_score (macro) : validation macro-average f1_score metrics f1_score (weighted) : validation weighted-average f1_score metrics driver (dict) : the mechanism on how a training is done in a loop ( iterated over n epochs ). Sub-arguments : module (str) : training driver identifier. Supported training driver methods is provided at training driver section args (dict) : the corresponding arguments to the respective training driver module","title":"Trainer"},{"location":"user-guides/experiment_file_config/#validator","text":"Flagged with validator key (dict) in the experiment file. This configuration set the validation process in the training iteration. E.g. : validator: { args: { score_threshold: 0.9, iou_threshold: 0.2, }, val_epoch: 10 } Arguments : args (dict) : additional arguments needed for validation process, including but not limited to arguments for model postprocessing which dependent on the model itself. For example : For several detection models ( if needed ): score_threshold (float) : threshold applied to the model\u2019s predicted object confidence score. Only objects with prediction scores higher than this threshold will be considered true objects, else considered as background. iou_threshold (float) : threshold for non-maxima suppression (NMS) intersection over union (IoU) For classification : No additional arguments for this task, you can leave args with empty dict {} val_epoch (int) : periodic number of epoch when the validation process will be executed in the training loop","title":"Validator"},{"location":"user-guides/experiment_file_config/#model","text":"Flagged with model key (dict) in the experiment file. This configuration set the selected deep learning model architecture for the specific task. E.g. : model: { name: FPNSSD, preprocess_args: { input_size: 512, input_normalization: { mean: [ 0.5, 0.5, 0.5 ], std: [ 0.5, 0.5, 0.5 ], scaler: 255 } }, network_args: { backbone: shufflenetv2_x1.0, n_classes: 20, pyramid_channels: 256, aspect_ratios: [ 1, 2., 3. ], pretrained_backbone: True, }, loss_args: { neg_pos: 3, overlap_thresh: 0.5, }, postprocess_args: { nms: True, } } Arguments : name (str) : model name identifier. List of supported models can be found in models zoo section . E.g. : preprocess_args (dict) : configurations for input data preprocessing. Sub-arguments : input_size (int) : input data size, input image will be resized to square while maintaining aspect ratio, by padding the data with black pixel (0,0,0) input_normalization (dict) : determine how input data will be normalized. Input data will be divided by scaler value accross all channel i.e. output = input / scaler . Then, given mean : [M1,...,Mn] and std : [S1,..,Sn] for n channels, this transform will normalize each channel of the input torch.*Tensor i.e. output[channel] = (input[channel] - mean[channel]) / std[channel] mean (list) : sequence of means for each channel. std (list) : sequence of standard deviations for each channel. scaler (list) : scale value for each pixel, default = 255 network_args (dict) : configuration related to the models architecture hyperparameter correspond to the respective model name identifier loss_args (dict) : configuration related to loss calculation hyperparameter which will be used in the training pipeline, which correspond to the respective model name identifier postprocess_args (dict) : configuration related to postprocess arguments, which correspond to the respective model name identifier","title":"Model"},{"location":"user-guides/experiment_file_config/#checkpoint","text":"Flagged with checkpoint key (str) in the experiment file. This configuration describe the path to Vortex model checkpoint which will be used for continue interrupted training or transfer learning to different cases with same model configuration. E.g. : checkpoint: experiments/outputs/shufflenetv2x100_fpn_ssd_voc2007_512/16575bd31b364539817177ca14147b5d/shufflenetv2x100_fpn_ssd_voc2007_512-epoch-10.pth","title":"Checkpoint"},{"location":"user-guides/experiment_file_config/#graph-exporter","text":"Flagged with exporter key (list or dict) in the experiment file. This configuration describe the graph exporter which will be used to convert the Pytorch graph into Intermediate Representation (IR) format. To export graph into multiple IR, use list of dict configurations. E.g. : Single IR exporter : exporter: { module: onnx, args: { opset_version: 11, }, } Multiple IR exporter : exporter: [ { module: onnx, args: { opset_version: 11, }, }, { module: torchscript, args: {}, }, ] Arguments : module (str) : selected exporter module, see exporter section for supported exporter modules args (dict) : the corresponding arguments to the respective exporter module","title":"Graph Exporter"},{"location":"user-guides/hypopt_file_config/","text":"Hyperparameter Optimization File Configuration \u00b6 An additional YAML configuration file is needed for hyperparameter optimization configuration. This hypopt configuration file will describe the parameters and objective to be optimized. Several examples of hypopt config file can be inspected on this link In this guide, we will describe the configurations needed for Optuna-based hyperparameter optimization All available configurations is listed below : Study Name \u00b6 Flagged with study_name key (str) in the hypopt config file. This field is used to identify the name of the hypopt attempt. Will be combined with the experiment_name field in the experiment file to identify an Optuna study . E.g. : study_name: detection_param_search Parameters \u00b6 Flagged with parameters key (list[dict]) in the hypopt config file. This field will contain a list of hyperparameters that will be searched. However, each parameter mentioned in here must be declared first in the experiment file using any initial value. To declare a parameter we use a flattened XML structure with dot ( . ) reducer. For example we want to search the parameter of score_threshold and iou_threshold in which their structure in the experiment file is shown below : ## optional field for validation step validator: { ## passed to validator class args: { score_threshold: 0.9, iou_threshold: 0.2, }, val_epoch: 5, }, Thus, we declare these parameters in the hypopt config as shown below: parameters: [ validator.args.score_threshold: { suggestion: suggest_discrete_uniform, args: { low: 0.05, high: 0.95, q: 0.025, } }, validator.args.iou_threshold: { suggestion: suggest_discrete_uniform, args: { low: 0.05, high: 0.5, q: 0.05, } }, ] Each parameter presented as (dict) type and have the following key arguments: suggestion (str) : define the Optuna trial object\u2019s suggestion method which will be used, any suggest_* method should be supported. For full reference see this link args (dict) : the corresponding arguments to the respective suggestion function Objective \u00b6 Flagged with objective key (dict) in the hypopt config file. This field denotes the function which we want to optimize, e.g. minimizing training loss or maximizing validation metric. In Vortex, we provide two general objective settings related to development pipelines. E.g. : objective: { module: TrainObjective, args: { metric_type: val, ## [val, loss] metric_name: mean_ap, ## if metric_type==val ## final objective value is the reduced validation metrics reduction: average, # reduction is based on numpy function (e.g. np.average, np.max, etc.) reduction_args: { weights: [ 1, 2, 3, 4, 5 ] } } } Arguments : module (str) : denotes a specific objective function module. Supported objective function is available in the next sub-section args (dict) : the corresponding arguments for selected module Train Objective Optimization \u00b6 This objective aim to optimize hyperparameters on train pipeline . E.g. : objective: { module: TrainObjective, args: { metric_type: val, ## [val, loss] metric_name: mean_ap, ## if metric_type==val ## final objective value is the reduced validation metrics reduction: average, # reduction is based on numpy function (e.g. np.average, np.max, etc.) reduction_args: { weights: [ 1, 2, 3, 4, 5 ] } } } Arguments : metric_type (str) : type of metric to be optimized : val or loss . val metric is extracted from in-training-loop validation process. So the provided experiment file must be valid for validation. loss metric is extracted from training process metric_name (str) : only used if metric_type is set to val . This argument denotes the name of the metric which want to be optimized. The available settings for this argument are also related to the model's task Detection task (see this link for further reading) : mean_ap : using the mean-average precision metrics Classification task (see this link for further reading): accuracy : using the accuracy metrics precision (micro) : using the micro-average precision metrics precision (macro) : using the macro-average precision metrics precision (weighted) : using the weighted-average precision metrics recall (micro) : using the micro-average recall metrics recall (macro) : using the macro-average recall metrics recall (weighted) : using the weighted-average recall metrics f1_score (micro) : using the micro-average f1_score metrics f1_score (macro) : using the macro-average f1_score metrics f1_score (weighted) : using the weighted-average f1_score metrics reduction (str) : the reduction function used to averaged the returned value from training pipeline. Supported reduction : latest : select the last value ( index [-1] ), mean : see numpy.mean sum : see numpy.sum min : see numpy.min max : see numpy.max median : see numpy.median average : see numpy.average percentile : see numpy.percentile quantile : see numpy.quantile reduction_args (dict) : the corresponding arguments for selected reduction . Additional Explanation : This objective utilize training pipelines which will return a list of values ( not singular value ), either a sequence of recorded loss value on each epoch, ( 100 epoch means list of 100 loss values) or a sequence of validation metrics ( val_epoch set to 5 and 100 epoch means list of 100/5 = 20 metric values ). However, Optuna expect a singular value to be optimized. Thus, we apply a reduction method using numpy functionality to pick the most representative value for that trial. Important Notes : Be careful when using weighted average, because the weights args expect the user to input a weight array with the same length compared to the input. So you must calculate the length yourself. E.g. : You want to use weighted average reduction on metric_type = loss. epoch is set to 50, so the input length to reduction function is 50 ( each epoch will dump 1 loss value ). Hence you need to provide weights in the reduction_args with the same length You want to use average reduction on metric_type = val. epoch is set to 50 , val_epoch is set to 5 , so the input length to reduction function is 50/5 = 10 ( epoch / val_epoch , each val_epoch will dump 1 validation metrics value). Hence you need to provide weights in the reduction_args with the same length Validation Objective Optimization \u00b6 This objective aim to optimize hyperparameters post-training pipeline (validate, predict) and utilize validation pipeline E.g. : objective: { module: ValidationObjective, args: { metric_name: mean_ap } } Arguments : metric_name (str) : denotes the name of the metric which want to be optimized. The available settings for this argument are also related to the model's task Detection task (see this link for further reading) : mean_ap : using the mean-average precision metrics Classification task (see this link for further reading): accuracy : using the accuracy metrics precision (micro) : using the micro-average precision metrics precision (macro) : using the macro-average precision metrics precision (weighted) : using the weighted-average precision metrics recall (micro) : using the micro-average recall metrics recall (macro) : using the macro-average recall metrics recall (weighted) : using the weighted-average recall metrics f1_score (micro) : using the micro-average f1_score metrics f1_score (macro) : using the macro-average f1_score metrics f1_score (weighted) : using the weighted-average f1_score metrics Study \u00b6 Flagged with objective key (dict) in the hypopt config file. This field set the Optuna study initialization. As stated in this link , a study corresponds to an optimization task, i.e. a set of trials. E.g. : study: { n_trials: 5, direction: maximize, pruner: { method: MedianPruner, args: {}, }, sampler: { method: TPESampler, args: {}, }, args: { storage: sqlite://anysqldb_url.db, load_if_exists: True, } } Arguments : n_trials (int) : number of trials attempted in a study direction (str) : either maximize or minimize the objective value pruner (dict) (Optional): enable Optuna pruner configuration, to judge whether the trial should be pruned based on the reported values, full reference in this link . Sub-arguments : method (str) : specify the Pruner\u2019s method which is going to be used args (dict) : the corresponding arguments to the respective pruner method sampler (dict) (Optional) : enable Optuna different sampler to sample the combination of hyperparameters value in a search space, full reference in this link . Sub-arguments : method (str) : specify the Sampler\u2019s method which is going to be used args (dict) : the corresponding arguments to the respective sampler method args (dict) (Optional) : Additional Optuna Study\u2019s arguments to use database to save studies. Sub-arguments : storage (str) : database URL. If this argument is set to None, in-memory storage is used, and the Study will not be persistent, full reference in this link load_if_exists (str) : flag to control the behavior to handle a conflict of study names, full reference in this link Additional Parameters' Override \u00b6 This config is used to override any experiment file configuration besides the search parameters mentioned in parameters section . For example, using epoch = 200 in initial experiment file config is too long if we want to set the n_trials = 20 ( means 20 * 200 epoch without any sampler ), so for hypopt we may only use 10 epochs. In order to do that we assign the overridden parameter in this field with flattened XML structure similar to parameters section E.g.: override: { trainer.epoch: 10, validator.val_epoch: 2, } Additional Experiment Configuration \u00b6 Additional experiment configuration which wants to be added to the original experiment file while optimized. Mandatory but can be empty E.g. : additional_config: {}","title":"HypOpt File Configuration"},{"location":"user-guides/hypopt_file_config/#hyperparameter-optimization-file-configuration","text":"An additional YAML configuration file is needed for hyperparameter optimization configuration. This hypopt configuration file will describe the parameters and objective to be optimized. Several examples of hypopt config file can be inspected on this link In this guide, we will describe the configurations needed for Optuna-based hyperparameter optimization All available configurations is listed below :","title":"Hyperparameter Optimization File Configuration"},{"location":"user-guides/hypopt_file_config/#study-name","text":"Flagged with study_name key (str) in the hypopt config file. This field is used to identify the name of the hypopt attempt. Will be combined with the experiment_name field in the experiment file to identify an Optuna study . E.g. : study_name: detection_param_search","title":"Study Name"},{"location":"user-guides/hypopt_file_config/#parameters","text":"Flagged with parameters key (list[dict]) in the hypopt config file. This field will contain a list of hyperparameters that will be searched. However, each parameter mentioned in here must be declared first in the experiment file using any initial value. To declare a parameter we use a flattened XML structure with dot ( . ) reducer. For example we want to search the parameter of score_threshold and iou_threshold in which their structure in the experiment file is shown below : ## optional field for validation step validator: { ## passed to validator class args: { score_threshold: 0.9, iou_threshold: 0.2, }, val_epoch: 5, }, Thus, we declare these parameters in the hypopt config as shown below: parameters: [ validator.args.score_threshold: { suggestion: suggest_discrete_uniform, args: { low: 0.05, high: 0.95, q: 0.025, } }, validator.args.iou_threshold: { suggestion: suggest_discrete_uniform, args: { low: 0.05, high: 0.5, q: 0.05, } }, ] Each parameter presented as (dict) type and have the following key arguments: suggestion (str) : define the Optuna trial object\u2019s suggestion method which will be used, any suggest_* method should be supported. For full reference see this link args (dict) : the corresponding arguments to the respective suggestion function","title":"Parameters"},{"location":"user-guides/hypopt_file_config/#objective","text":"Flagged with objective key (dict) in the hypopt config file. This field denotes the function which we want to optimize, e.g. minimizing training loss or maximizing validation metric. In Vortex, we provide two general objective settings related to development pipelines. E.g. : objective: { module: TrainObjective, args: { metric_type: val, ## [val, loss] metric_name: mean_ap, ## if metric_type==val ## final objective value is the reduced validation metrics reduction: average, # reduction is based on numpy function (e.g. np.average, np.max, etc.) reduction_args: { weights: [ 1, 2, 3, 4, 5 ] } } } Arguments : module (str) : denotes a specific objective function module. Supported objective function is available in the next sub-section args (dict) : the corresponding arguments for selected module","title":"Objective"},{"location":"user-guides/hypopt_file_config/#train-objective-optimization","text":"This objective aim to optimize hyperparameters on train pipeline . E.g. : objective: { module: TrainObjective, args: { metric_type: val, ## [val, loss] metric_name: mean_ap, ## if metric_type==val ## final objective value is the reduced validation metrics reduction: average, # reduction is based on numpy function (e.g. np.average, np.max, etc.) reduction_args: { weights: [ 1, 2, 3, 4, 5 ] } } } Arguments : metric_type (str) : type of metric to be optimized : val or loss . val metric is extracted from in-training-loop validation process. So the provided experiment file must be valid for validation. loss metric is extracted from training process metric_name (str) : only used if metric_type is set to val . This argument denotes the name of the metric which want to be optimized. The available settings for this argument are also related to the model's task Detection task (see this link for further reading) : mean_ap : using the mean-average precision metrics Classification task (see this link for further reading): accuracy : using the accuracy metrics precision (micro) : using the micro-average precision metrics precision (macro) : using the macro-average precision metrics precision (weighted) : using the weighted-average precision metrics recall (micro) : using the micro-average recall metrics recall (macro) : using the macro-average recall metrics recall (weighted) : using the weighted-average recall metrics f1_score (micro) : using the micro-average f1_score metrics f1_score (macro) : using the macro-average f1_score metrics f1_score (weighted) : using the weighted-average f1_score metrics reduction (str) : the reduction function used to averaged the returned value from training pipeline. Supported reduction : latest : select the last value ( index [-1] ), mean : see numpy.mean sum : see numpy.sum min : see numpy.min max : see numpy.max median : see numpy.median average : see numpy.average percentile : see numpy.percentile quantile : see numpy.quantile reduction_args (dict) : the corresponding arguments for selected reduction . Additional Explanation : This objective utilize training pipelines which will return a list of values ( not singular value ), either a sequence of recorded loss value on each epoch, ( 100 epoch means list of 100 loss values) or a sequence of validation metrics ( val_epoch set to 5 and 100 epoch means list of 100/5 = 20 metric values ). However, Optuna expect a singular value to be optimized. Thus, we apply a reduction method using numpy functionality to pick the most representative value for that trial. Important Notes : Be careful when using weighted average, because the weights args expect the user to input a weight array with the same length compared to the input. So you must calculate the length yourself. E.g. : You want to use weighted average reduction on metric_type = loss. epoch is set to 50, so the input length to reduction function is 50 ( each epoch will dump 1 loss value ). Hence you need to provide weights in the reduction_args with the same length You want to use average reduction on metric_type = val. epoch is set to 50 , val_epoch is set to 5 , so the input length to reduction function is 50/5 = 10 ( epoch / val_epoch , each val_epoch will dump 1 validation metrics value). Hence you need to provide weights in the reduction_args with the same length","title":"Train Objective Optimization"},{"location":"user-guides/hypopt_file_config/#validation-objective-optimization","text":"This objective aim to optimize hyperparameters post-training pipeline (validate, predict) and utilize validation pipeline E.g. : objective: { module: ValidationObjective, args: { metric_name: mean_ap } } Arguments : metric_name (str) : denotes the name of the metric which want to be optimized. The available settings for this argument are also related to the model's task Detection task (see this link for further reading) : mean_ap : using the mean-average precision metrics Classification task (see this link for further reading): accuracy : using the accuracy metrics precision (micro) : using the micro-average precision metrics precision (macro) : using the macro-average precision metrics precision (weighted) : using the weighted-average precision metrics recall (micro) : using the micro-average recall metrics recall (macro) : using the macro-average recall metrics recall (weighted) : using the weighted-average recall metrics f1_score (micro) : using the micro-average f1_score metrics f1_score (macro) : using the macro-average f1_score metrics f1_score (weighted) : using the weighted-average f1_score metrics","title":"Validation Objective Optimization"},{"location":"user-guides/hypopt_file_config/#study","text":"Flagged with objective key (dict) in the hypopt config file. This field set the Optuna study initialization. As stated in this link , a study corresponds to an optimization task, i.e. a set of trials. E.g. : study: { n_trials: 5, direction: maximize, pruner: { method: MedianPruner, args: {}, }, sampler: { method: TPESampler, args: {}, }, args: { storage: sqlite://anysqldb_url.db, load_if_exists: True, } } Arguments : n_trials (int) : number of trials attempted in a study direction (str) : either maximize or minimize the objective value pruner (dict) (Optional): enable Optuna pruner configuration, to judge whether the trial should be pruned based on the reported values, full reference in this link . Sub-arguments : method (str) : specify the Pruner\u2019s method which is going to be used args (dict) : the corresponding arguments to the respective pruner method sampler (dict) (Optional) : enable Optuna different sampler to sample the combination of hyperparameters value in a search space, full reference in this link . Sub-arguments : method (str) : specify the Sampler\u2019s method which is going to be used args (dict) : the corresponding arguments to the respective sampler method args (dict) (Optional) : Additional Optuna Study\u2019s arguments to use database to save studies. Sub-arguments : storage (str) : database URL. If this argument is set to None, in-memory storage is used, and the Study will not be persistent, full reference in this link load_if_exists (str) : flag to control the behavior to handle a conflict of study names, full reference in this link","title":"Study"},{"location":"user-guides/hypopt_file_config/#additional-parameters-override","text":"This config is used to override any experiment file configuration besides the search parameters mentioned in parameters section . For example, using epoch = 200 in initial experiment file config is too long if we want to set the n_trials = 20 ( means 20 * 200 epoch without any sampler ), so for hypopt we may only use 10 epochs. In order to do that we assign the overridden parameter in this field with flattened XML structure similar to parameters section E.g.: override: { trainer.epoch: 10, validator.val_epoch: 2, }","title":"Additional Parameters' Override"},{"location":"user-guides/hypopt_file_config/#additional-experiment-configuration","text":"Additional experiment configuration which wants to be added to the original experiment file while optimized. Mandatory but can be empty E.g. : additional_config: {}","title":"Additional Experiment Configuration"},{"location":"user-guides/pipelines/","text":"Vortex Pipelines \u00b6 This section will describe how to easily run each of Vortex pipeline in details. For complete pipelines flow please see Vortex overview section Training Pipeline \u00b6 This pipeline purpose is to train a deep learning model using the provided dataset. If you need to integrate the training into your own script you can see the training pipeline API section . To run this pipeline, make sure you've already prepared : Dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one and additionally, if you want to resume previous training or load pretrained model , you also need : Vortex model's file *.pth : obtained from previously executed training pipeline which corresponds to the previous mentioned experiment file. Specific checkpoint file on several epoch can be found under run directory ( see outputs of this training pipeline section ). This file must be configured in experiment file under checkpoint section You only need to run this command from the command line interface : usage: vortex train [-h] -c CONFIG [--resume] [--no-log] Vortex training pipeline; will generate a Pytorch model file optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config file --resume vortex-saved model path for resume training --no-log disable logging, ignore experiment file config E.g. : vortex train -c experiments/config/efficientnet_b0_classification_cifar10.yml This pipeline will generate several outputs : Local runs log file : every time a user runs a VORTEX training experiment, the experiment logger module will write a local file experiments/local_runs.log which will record all experimental training runs which have already executed sequentially for easier tracking. Example of the content inside experiments/local_runs.log is shown below : ############################################################################### Timestamp : 03/27/2020, 09:24:00 Experiment Name : test_torchvision_dataset Output Path : experiments/outputs/test_torchvision_dataset/601f45782a884286be310b1ffe562597 Logging Provider : comet_ml Experiment Log URL : https://www.comet.ml/hyperion-rg/vortex-dev/601f45782a884286be310b1ffe562597 ############################################################################### Experiment directory : If not exist yet, training script will make a directory under the configured output_directory path. The created directory will be named after the experiment_name configuration and will be the directory to dump training (final weight), validation result (if any), backup, etc. Run directory : Everytime user runs the training script, it will be tagged as a new experiment run. Vortex (or third party logger) will generate a unique key which will be an identifier for that specific experiment run. And thus, Vortex will make a new directory under the experiment directory which will act as a backup directory. It will store the duplicate of the executed experiment file (as a backup) and will be the directory which store intermediate model\u2019s weight path (weight that saved every n-epoch). For example, in the previous example log the output path is : Output Path : experiments/outputs/test_torchvision_dataset/601f45782a884286be310b1ffe562597 The experiment directory is test_torchvision_dataset and the run directory is 601f45782a884286be310b1ffe562597 Backup experiment file : Experiment file will be duplicated and stored under run directory Intermediate model file : File containing model\u2019s weight and training checkpoint will be dumped into run directory with .pth extension,which can be controlled from 2 experiment file trainer parameter, save_epoch and save_best_metrics : save_epoch : save checkpoint every n -epoch save_best_metrics : save checkpoint based on monitored metrics Final model file : File containing model's weight and training checkpoint after all training epoch is completed will be dumped in the run directory and experiment directory with .pth extension. WARNING : if you have multiple experiment run with the same experiment_name , each finished run will overwrite this final model file in the experiment_directory , however the original final model file will still exist on the run_directory Experiment log : If logging is enabled, training metrics will be collected by the logging provider. Additionally if the config file is valid for validation, the validation metrics will also be collected. Validation Pipeline \u00b6 This pipeline handle the evaluation of the Vortex model (Pytorch state dict .pth ) in term of model's performance and resource usage. In addition, this pipeline also generate a visual report. If you need to integrate the validation into your own script you can see the validation pipeline API section . To run this pipeline, make sure you've already prepared : Validation dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one. Must be valid for validation, make sure dataset.eval and validator is set Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file You only need to run this command from the command line interface : usage: vortex validate [-h] -c CONFIG [-w WEIGHTS] [-v] [--quiet] [-d [DEVICES [DEVICES ...]]] [-b BATCH_SIZE] Vortex Pytorch model validation pipeline; successful runs will produce autogenerated reports optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config -w WEIGHTS, --weights WEIGHTS path to selected weights(optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -v, --verbose verbose prediction output --quiet -d [DEVICES [DEVICES ...]], --devices [DEVICES [DEVICES ...]] computation device to be used for prediction, possible to list multiple devices -b BATCH_SIZE, --batch-size BATCH_SIZE batch size for validation NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex validate -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -b 8 \\ -d cpu cuda This pipeline will generate several outputs : Report file : after successful evaluation, report file will be generated under directory reports in the experiment directory based on experiment_name under output_directory . Pro Tip : the generated report could be easily converted to pdf using pandoc or vscode markdown-pdf extension . Prediction Pipeline \u00b6 This pipeline is used to test and visualize your Vortex model's prediction. If you need to integrate the prediction into your own script you can see the prediction pipeline API section . To run this pipeline, make sure you've already prepared : Experiment file : see this section to create one Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Input image(s) : image file(s) (tested with *.jpg , *.jpeg , *.png extension) You only need to run this command from the command line interface : usage: vortex predict [-h] -c CONFIG [-w WEIGHTS] [-o OUTPUT_DIR] -i IMAGE [IMAGE ...] [-d DEVICE] [--score_threshold SCORE_THRESHOLD] [--iou_threshold IOU_THRESHOLD] Vortex Pytorch model prediction pipeline; may receive multiple image(s) for batched prediction optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config -w WEIGHTS, --weights WEIGHTS path to selected weights(optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -o OUTPUT_DIR, --output-dir OUTPUT_DIR directory to dump prediction visualization -i IMAGE [IMAGE ...], --image IMAGE [IMAGE ...] path to test image(s) -d DEVICE, --device DEVICE the device in which the inference will be performed --score_threshold SCORE_THRESHOLD score threshold for detection, only used if model is detection, ignored otherwise --iou_threshold IOU_THRESHOLD iou threshold for nms, , only used if model is detection, ignored otherwise NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex predict -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -i image1.jpg image2.jpg \\ -d cuda \\ -o output_vis NOTES : Provided multiple input images will be treated as batch input This pipeline will generate several outputs : Output Visualization Directory : if --output_dir is provided, it will create the directory in your current working directory Prediction Visualization : prediction visualization will be generated in the --output_dir if provided, or in the current working dir if not. The generated file will have prediction_ name prefix. Hyperparameters Optimization Pipeline \u00b6 This pipeline is used to search for optimum hyperparameter to be used for either training pipeline or validation pipeline (parameter in validation pipeline also can be used for prediction pipeline). Basically this pipeline is Optuna wrapper for Vortex components. If you need to integrate the prediction into your own script you can see the hyperparameters optimization pipeline API section . To run this pipeline, make sure you've already prepared : Hypopt config file : see this section to create one Experiment file : see this section to create one Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Related objective's pipeline requirement You only need to run this command from the command line interface : usage: vortex hypopt [-h] -c CONFIG -o OPTCONFIG [-w WEIGHTS] Vortex hyperparameter optimization experiment optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config file -o OPTCONFIG, --optconfig OPTCONFIG path to hypopt config file -w WEIGHTS, --weights WEIGHTS path to selected weights (optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified, valid only for ValidationObjective, ignored otherwise NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex hypopt -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -o experiments/hypopt/learning_rate_search.yml This pipeline will generate several outputs : Hypopt Output Dir : hypopt/{hypopt_study_name} will be created under experiment directory Best Parameters : file *.txt containing best parameters will be created in hypopt output dir Hypopt Visualization : graph visualization of parameter search (visualization extension must be installed. see installation section ) will be created in hypopt output dir Graph Export Pipeline \u00b6 This pipeline is used to export trained Vortex model (or graph) into another graph representation (or Intermediate Representation (IR)). If you need to integrate the graph export pipeline into your own script you can see the graph export pipeline API section . To run this pipeline, make sure you've already prepared : Experiment file : see this section to create one and make sure the exporter section is already configured Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Example Input Image : example input image for correct graph tracing. Recommended for using image from training dataset and strongly recommended for model with detection task You only need to run this command from the command line interface : usage: vortex export [-h] -c CONFIG [-w WEIGHTS] [-i EXAMPLE_INPUT] export model to specific IR specified in config, output IR are stored in the experiment directory based on `experiment_name` under `output_directory` config field, after successful export, you should be able to visualize the network using [netron](https://lutzroeder.github.io/netron/) optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG export experiment config file -w WEIGHTS, --weights WEIGHTS path to selected weights (optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -i EXAMPLE_INPUT, --example-input EXAMPLE_INPUT path to example input for tracing (optional, may be necessary for correct tracing, especially for detection model) NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex export -c experiments/config/efficientnet_b0_classification_cifar10.yml -i image1.jpg This pipeline will generate several outputs : IR model file : IR model file will be created under experiment directory , with file extension that correspond to exporter settings IR Validation Pipeline \u00b6 This pipeline handle the evaluation of the IR model ( *.pt or *.onnx ) in term of model's performance and resource usage. In addition, this pipeline also generate a visual report. If you need to integrate the validation into your own script you can see the IR validation pipeline API section . To run this pipeline, make sure you've already prepared : Validation dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one. Must be valid for validation, make sure dataset.eval and validator is set IR model file *.pt or *.onnx : obtained from graph export pipeline IR runtime library and environment : make sure runtime library and environment is installed (currently runtime library installed together with vortex) You only need to run this command from the command line interface : usage: vortex ir_runtime_validate [-h] -c CONFIG -m MODEL [-r [RUNTIME [RUNTIME ...]]] [-v] [--quiet] [--batch-size BATCH_SIZE] Vortex exported IR graph validation pipeline; successful runs will produce autogenerated reports optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config including dataset fields, must be valid for validation, dataset.eval will be used for evaluation -m MODEL, --model MODEL path to IR model -r [RUNTIME [RUNTIME ...]], --runtime [RUNTIME [RUNTIME ...]] runtime backend device -v, --verbose verbose prediction output --quiet -b BATCH_SIZE, --batch-size BATCH_SIZE batch size for validation; NOTE : passed value should be matched with exported model batch size E.g. : vortex ir_runtime_validate -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -m experiments/outputs/efficientnet_b0_classification_cifar10/efficientnet_b0_classification_cifar10.pt \\ -b 8 \\ -r cpu cuda This pipeline will generate several outputs : Report file : after successful evaluation, report file will be generated under directory reports in the experiment directory based on experiment_name under output_directory . Pro Tip : the generated report could be easily converted to pdf using pandoc or vscode markdown-pdf extension . IR Prediction Pipeline \u00b6 This pipeline is used to test and visualize your IR model's ( *.pt or *.onnx ) prediction. If you need to integrate the prediction into your own script you can see the IR prediction pipeline API section . To run this pipeline, make sure you've already prepared : IR model file *.pt or *.onnx : obtained from graph export pipeline IR runtime library and environment : make sure runtime library and environment is installed (currently runtime library installed together with vortex) Input image(s) : image file(s) (tested with *.jpg , *.jpeg , *.png extension) You only need to run this command from the command line interface : usage: vortex ir_runtime_predict [-h] -m MODEL -i IMAGE [IMAGE ...] [-o OUTPUT_DIR] [--score_threshold SCORE_THRESHOLD] [--iou_threshold IOU_THRESHOLD] [-r RUNTIME] Vortex IR model prediction pipeline; may receive multiple image(s) for batched prediction optional arguments: -h, --help show this help message and exit -m MODEL, --model MODEL path to IR model -i IMAGE [IMAGE ...], --image IMAGE [IMAGE ...] path to test image(s); at least 1 path should be provided, supports up to model batch_size -o OUTPUT_DIR, --output-dir OUTPUT_DIR directory to dump prediction visualization --score_threshold SCORE_THRESHOLD score threshold for detection, only used if model is detection, ignored otherwise --iou_threshold IOU_THRESHOLD iou threshold for nms, only used if model is detection, ignored otherwise -r RUNTIME, --runtime RUNTIME runtime device E.g. : vortex ir_runtime_predict -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -m experiments/outputs/efficientnet_b0_classification_cifar10/efficientnet_b0_classification_cifar10.pt \\ -i image1.jpg image2.jpg \\ -r cuda \\ -o output_vis NOTES : Provided multiple input images will be treated as batch input. Vortex IR model is strict with batch size, means that provided input batch size must match with Vortex IR exporter batch size configuration. This pipeline will generate several outputs : Output Visualization Directory : if --output_dir is provided, it will create the directory in your current working directory Prediction Visualization : prediction visualization will be generated in the --output_dir if provided, or in the current working dir if not. The generated file will have prediction_ name prefix.","title":"Pipelines"},{"location":"user-guides/pipelines/#vortex-pipelines","text":"This section will describe how to easily run each of Vortex pipeline in details. For complete pipelines flow please see Vortex overview section","title":"Vortex Pipelines"},{"location":"user-guides/pipelines/#training-pipeline","text":"This pipeline purpose is to train a deep learning model using the provided dataset. If you need to integrate the training into your own script you can see the training pipeline API section . To run this pipeline, make sure you've already prepared : Dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one and additionally, if you want to resume previous training or load pretrained model , you also need : Vortex model's file *.pth : obtained from previously executed training pipeline which corresponds to the previous mentioned experiment file. Specific checkpoint file on several epoch can be found under run directory ( see outputs of this training pipeline section ). This file must be configured in experiment file under checkpoint section You only need to run this command from the command line interface : usage: vortex train [-h] -c CONFIG [--resume] [--no-log] Vortex training pipeline; will generate a Pytorch model file optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config file --resume vortex-saved model path for resume training --no-log disable logging, ignore experiment file config E.g. : vortex train -c experiments/config/efficientnet_b0_classification_cifar10.yml This pipeline will generate several outputs : Local runs log file : every time a user runs a VORTEX training experiment, the experiment logger module will write a local file experiments/local_runs.log which will record all experimental training runs which have already executed sequentially for easier tracking. Example of the content inside experiments/local_runs.log is shown below : ############################################################################### Timestamp : 03/27/2020, 09:24:00 Experiment Name : test_torchvision_dataset Output Path : experiments/outputs/test_torchvision_dataset/601f45782a884286be310b1ffe562597 Logging Provider : comet_ml Experiment Log URL : https://www.comet.ml/hyperion-rg/vortex-dev/601f45782a884286be310b1ffe562597 ############################################################################### Experiment directory : If not exist yet, training script will make a directory under the configured output_directory path. The created directory will be named after the experiment_name configuration and will be the directory to dump training (final weight), validation result (if any), backup, etc. Run directory : Everytime user runs the training script, it will be tagged as a new experiment run. Vortex (or third party logger) will generate a unique key which will be an identifier for that specific experiment run. And thus, Vortex will make a new directory under the experiment directory which will act as a backup directory. It will store the duplicate of the executed experiment file (as a backup) and will be the directory which store intermediate model\u2019s weight path (weight that saved every n-epoch). For example, in the previous example log the output path is : Output Path : experiments/outputs/test_torchvision_dataset/601f45782a884286be310b1ffe562597 The experiment directory is test_torchvision_dataset and the run directory is 601f45782a884286be310b1ffe562597 Backup experiment file : Experiment file will be duplicated and stored under run directory Intermediate model file : File containing model\u2019s weight and training checkpoint will be dumped into run directory with .pth extension,which can be controlled from 2 experiment file trainer parameter, save_epoch and save_best_metrics : save_epoch : save checkpoint every n -epoch save_best_metrics : save checkpoint based on monitored metrics Final model file : File containing model's weight and training checkpoint after all training epoch is completed will be dumped in the run directory and experiment directory with .pth extension. WARNING : if you have multiple experiment run with the same experiment_name , each finished run will overwrite this final model file in the experiment_directory , however the original final model file will still exist on the run_directory Experiment log : If logging is enabled, training metrics will be collected by the logging provider. Additionally if the config file is valid for validation, the validation metrics will also be collected.","title":"Training Pipeline"},{"location":"user-guides/pipelines/#validation-pipeline","text":"This pipeline handle the evaluation of the Vortex model (Pytorch state dict .pth ) in term of model's performance and resource usage. In addition, this pipeline also generate a visual report. If you need to integrate the validation into your own script you can see the validation pipeline API section . To run this pipeline, make sure you've already prepared : Validation dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one. Must be valid for validation, make sure dataset.eval and validator is set Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file You only need to run this command from the command line interface : usage: vortex validate [-h] -c CONFIG [-w WEIGHTS] [-v] [--quiet] [-d [DEVICES [DEVICES ...]]] [-b BATCH_SIZE] Vortex Pytorch model validation pipeline; successful runs will produce autogenerated reports optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config -w WEIGHTS, --weights WEIGHTS path to selected weights(optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -v, --verbose verbose prediction output --quiet -d [DEVICES [DEVICES ...]], --devices [DEVICES [DEVICES ...]] computation device to be used for prediction, possible to list multiple devices -b BATCH_SIZE, --batch-size BATCH_SIZE batch size for validation NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex validate -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -b 8 \\ -d cpu cuda This pipeline will generate several outputs : Report file : after successful evaluation, report file will be generated under directory reports in the experiment directory based on experiment_name under output_directory . Pro Tip : the generated report could be easily converted to pdf using pandoc or vscode markdown-pdf extension .","title":"Validation Pipeline"},{"location":"user-guides/pipelines/#prediction-pipeline","text":"This pipeline is used to test and visualize your Vortex model's prediction. If you need to integrate the prediction into your own script you can see the prediction pipeline API section . To run this pipeline, make sure you've already prepared : Experiment file : see this section to create one Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Input image(s) : image file(s) (tested with *.jpg , *.jpeg , *.png extension) You only need to run this command from the command line interface : usage: vortex predict [-h] -c CONFIG [-w WEIGHTS] [-o OUTPUT_DIR] -i IMAGE [IMAGE ...] [-d DEVICE] [--score_threshold SCORE_THRESHOLD] [--iou_threshold IOU_THRESHOLD] Vortex Pytorch model prediction pipeline; may receive multiple image(s) for batched prediction optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config -w WEIGHTS, --weights WEIGHTS path to selected weights(optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -o OUTPUT_DIR, --output-dir OUTPUT_DIR directory to dump prediction visualization -i IMAGE [IMAGE ...], --image IMAGE [IMAGE ...] path to test image(s) -d DEVICE, --device DEVICE the device in which the inference will be performed --score_threshold SCORE_THRESHOLD score threshold for detection, only used if model is detection, ignored otherwise --iou_threshold IOU_THRESHOLD iou threshold for nms, , only used if model is detection, ignored otherwise NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex predict -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -i image1.jpg image2.jpg \\ -d cuda \\ -o output_vis NOTES : Provided multiple input images will be treated as batch input This pipeline will generate several outputs : Output Visualization Directory : if --output_dir is provided, it will create the directory in your current working directory Prediction Visualization : prediction visualization will be generated in the --output_dir if provided, or in the current working dir if not. The generated file will have prediction_ name prefix.","title":"Prediction Pipeline"},{"location":"user-guides/pipelines/#hyperparameters-optimization-pipeline","text":"This pipeline is used to search for optimum hyperparameter to be used for either training pipeline or validation pipeline (parameter in validation pipeline also can be used for prediction pipeline). Basically this pipeline is Optuna wrapper for Vortex components. If you need to integrate the prediction into your own script you can see the hyperparameters optimization pipeline API section . To run this pipeline, make sure you've already prepared : Hypopt config file : see this section to create one Experiment file : see this section to create one Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Related objective's pipeline requirement You only need to run this command from the command line interface : usage: vortex hypopt [-h] -c CONFIG -o OPTCONFIG [-w WEIGHTS] Vortex hyperparameter optimization experiment optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config file -o OPTCONFIG, --optconfig OPTCONFIG path to hypopt config file -w WEIGHTS, --weights WEIGHTS path to selected weights (optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified, valid only for ValidationObjective, ignored otherwise NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex hypopt -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -o experiments/hypopt/learning_rate_search.yml This pipeline will generate several outputs : Hypopt Output Dir : hypopt/{hypopt_study_name} will be created under experiment directory Best Parameters : file *.txt containing best parameters will be created in hypopt output dir Hypopt Visualization : graph visualization of parameter search (visualization extension must be installed. see installation section ) will be created in hypopt output dir","title":"Hyperparameters Optimization Pipeline"},{"location":"user-guides/pipelines/#graph-export-pipeline","text":"This pipeline is used to export trained Vortex model (or graph) into another graph representation (or Intermediate Representation (IR)). If you need to integrate the graph export pipeline into your own script you can see the graph export pipeline API section . To run this pipeline, make sure you've already prepared : Experiment file : see this section to create one and make sure the exporter section is already configured Vortex model's file *.pth : obtained from training pipeline which corresponds to the previous mentioned experiment file Example Input Image : example input image for correct graph tracing. Recommended for using image from training dataset and strongly recommended for model with detection task You only need to run this command from the command line interface : usage: vortex export [-h] -c CONFIG [-w WEIGHTS] [-i EXAMPLE_INPUT] export model to specific IR specified in config, output IR are stored in the experiment directory based on `experiment_name` under `output_directory` config field, after successful export, you should be able to visualize the network using [netron](https://lutzroeder.github.io/netron/) optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG export experiment config file -w WEIGHTS, --weights WEIGHTS path to selected weights (optional, will be inferred from `output_directory` and `experiment_name` field from config) if not specified -i EXAMPLE_INPUT, --example-input EXAMPLE_INPUT path to example input for tracing (optional, may be necessary for correct tracing, especially for detection model) NOTES : if --weights is not provided, Vortex will assume final weights exist in the experiment directory E.g. : vortex export -c experiments/config/efficientnet_b0_classification_cifar10.yml -i image1.jpg This pipeline will generate several outputs : IR model file : IR model file will be created under experiment directory , with file extension that correspond to exporter settings","title":"Graph Export Pipeline"},{"location":"user-guides/pipelines/#ir-validation-pipeline","text":"This pipeline handle the evaluation of the IR model ( *.pt or *.onnx ) in term of model's performance and resource usage. In addition, this pipeline also generate a visual report. If you need to integrate the validation into your own script you can see the IR validation pipeline API section . To run this pipeline, make sure you've already prepared : Validation dataset : see this section for built-in datasets, or this section for external datasets Experiment file : see this section to create one. Must be valid for validation, make sure dataset.eval and validator is set IR model file *.pt or *.onnx : obtained from graph export pipeline IR runtime library and environment : make sure runtime library and environment is installed (currently runtime library installed together with vortex) You only need to run this command from the command line interface : usage: vortex ir_runtime_validate [-h] -c CONFIG -m MODEL [-r [RUNTIME [RUNTIME ...]]] [-v] [--quiet] [--batch-size BATCH_SIZE] Vortex exported IR graph validation pipeline; successful runs will produce autogenerated reports optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG path to experiment config including dataset fields, must be valid for validation, dataset.eval will be used for evaluation -m MODEL, --model MODEL path to IR model -r [RUNTIME [RUNTIME ...]], --runtime [RUNTIME [RUNTIME ...]] runtime backend device -v, --verbose verbose prediction output --quiet -b BATCH_SIZE, --batch-size BATCH_SIZE batch size for validation; NOTE : passed value should be matched with exported model batch size E.g. : vortex ir_runtime_validate -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -m experiments/outputs/efficientnet_b0_classification_cifar10/efficientnet_b0_classification_cifar10.pt \\ -b 8 \\ -r cpu cuda This pipeline will generate several outputs : Report file : after successful evaluation, report file will be generated under directory reports in the experiment directory based on experiment_name under output_directory . Pro Tip : the generated report could be easily converted to pdf using pandoc or vscode markdown-pdf extension .","title":"IR Validation Pipeline"},{"location":"user-guides/pipelines/#ir-prediction-pipeline","text":"This pipeline is used to test and visualize your IR model's ( *.pt or *.onnx ) prediction. If you need to integrate the prediction into your own script you can see the IR prediction pipeline API section . To run this pipeline, make sure you've already prepared : IR model file *.pt or *.onnx : obtained from graph export pipeline IR runtime library and environment : make sure runtime library and environment is installed (currently runtime library installed together with vortex) Input image(s) : image file(s) (tested with *.jpg , *.jpeg , *.png extension) You only need to run this command from the command line interface : usage: vortex ir_runtime_predict [-h] -m MODEL -i IMAGE [IMAGE ...] [-o OUTPUT_DIR] [--score_threshold SCORE_THRESHOLD] [--iou_threshold IOU_THRESHOLD] [-r RUNTIME] Vortex IR model prediction pipeline; may receive multiple image(s) for batched prediction optional arguments: -h, --help show this help message and exit -m MODEL, --model MODEL path to IR model -i IMAGE [IMAGE ...], --image IMAGE [IMAGE ...] path to test image(s); at least 1 path should be provided, supports up to model batch_size -o OUTPUT_DIR, --output-dir OUTPUT_DIR directory to dump prediction visualization --score_threshold SCORE_THRESHOLD score threshold for detection, only used if model is detection, ignored otherwise --iou_threshold IOU_THRESHOLD iou threshold for nms, only used if model is detection, ignored otherwise -r RUNTIME, --runtime RUNTIME runtime device E.g. : vortex ir_runtime_predict -c experiments/config/efficientnet_b0_classification_cifar10.yml \\ -m experiments/outputs/efficientnet_b0_classification_cifar10/efficientnet_b0_classification_cifar10.pt \\ -i image1.jpg image2.jpg \\ -r cuda \\ -o output_vis NOTES : Provided multiple input images will be treated as batch input. Vortex IR model is strict with batch size, means that provided input batch size must match with Vortex IR exporter batch size configuration. This pipeline will generate several outputs : Output Visualization Directory : if --output_dir is provided, it will create the directory in your current working directory Prediction Visualization : prediction visualization will be generated in the --output_dir if provided, or in the current working dir if not. The generated file will have prediction_ name prefix.","title":"IR Prediction Pipeline"}]}