<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Custom Model Integration : Detection - Vortex</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ir-black.min.css">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/console.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">Vortex</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../.." class="nav-link">Home</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">User Guides <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../user-guides/dataset_integration/" class="dropdown-item">Dataset Integration</a>
</li>
                                    
<li>
    <a href="../../user-guides/experiment_file_config/" class="dropdown-item">Experiment File Configuration</a>
</li>
                                    
<li>
    <a href="../../user-guides/hypopt_file_config/" class="dropdown-item">HypOpt File Configuration</a>
</li>
                                    
<li>
    <a href="../../user-guides/pipelines/" class="dropdown-item">Pipelines</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Developer Guides <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../custom_backbone_integration/" class="dropdown-item">Custom Backbone Integration</a>
</li>
                                    
<li>
    <a href="../custom_model_integration_classification/" class="dropdown-item">Custom Model Integration : Classification</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active">Custom Model Integration : Detection</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Modules <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../modules/builtin_dataset/" class="dropdown-item">Built-in Dataset</a>
</li>
                                    
<li>
    <a href="../../modules/logging_provider/" class="dropdown-item">Logging Provider</a>
</li>
                                    
<li>
    <a href="../../modules/augmentation/" class="dropdown-item">Augmentations</a>
</li>
                                    
<li>
    <a href="../../modules/data_loader/" class="dropdown-item">Data Loader</a>
</li>
                                    
<li>
    <a href="../../modules/scheduler/" class="dropdown-item">Learning Rates Scheduler</a>
</li>
                                    
<li>
    <a href="../../modules/train_driver/" class="dropdown-item">Training Driver</a>
</li>
                                    
<li>
    <a href="../../modules/models_zoo/" class="dropdown-item">Models Zoo</a>
</li>
                                    
<li>
    <a href="../../modules/backbones/" class="dropdown-item">Backbones Network</a>
</li>
                                    
<li>
    <a href="../../modules/exporter/" class="dropdown-item">Graph Exporter</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../api/vortex.development.core.pipelines/" class="dropdown-item">vortex.development.core.pipelines</a>
</li>
                                    
<li>
    <a href="../../api/vortex.development.core.factory/" class="dropdown-item">vortex.development.core.factory</a>
</li>
                                    
<li>
    <a href="../../api/vortex.runtime/" class="dropdown-item">vortex.runtime</a>
</li>
                                </ul>
                            </li>
                            <li class="navitem">
                                <a href="https://github.com/nodefluxio/vortex" class="nav-link">Repository</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../custom_model_integration_classification/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../modules/builtin_dataset/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#integrating-custom-detection-model-to-vortex" class="nav-link">Integrating Custom Detection Model to Vortex</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#1-model-architecture" class="nav-link">1. Model Architecture</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#2-create-postprocess-module" class="nav-link">2. Create PostProcess module</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#3-registering-our-model-to-vortex" class="nav-link">3. Registering our model to vortex</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#4-integration-with-vortex-cli" class="nav-link">4. Integration with vortex CLI</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#5-demo-program" class="nav-link">5. Demo program</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="integrating-custom-detection-model-to-vortex">Integrating Custom Detection Model to Vortex<a class="headerlink" href="#integrating-custom-detection-model-to-vortex" title="Permanent link">&para;</a></h1>
<p>This tutorial shows how to create custom detection model and integrate to <code>vortex</code>.
In this particular case, we will integrate <strong><code>DE:TR</code> (Detection Transformer)</strong>, and
constrained to inference only.</p>
<pre><code class="language-python">from PIL import Image
import requests
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torchvision as vision
import torchvision.transforms as T
from torchvision.models import resnet50

torch.set_grad_enabled(False)

import cv2
import numpy as np

import warnings
warnings.filterwarnings(&quot;ignore&quot;)

import vortex.development
import vortex.development.networks as networks
import vortex.development.networks.modules as vortex_modules
import vortex.development.exporter.utils.onnx.graph_ops as graph_ops

Pipeline = vortex.development.core.pipelines.PytorchPredictionPipeline
ExportPipeline = vortex.development.core.pipelines.GraphExportPipeline
ONNXPipeline = vortex.development.core.pipelines.IRPredictionPipeline
TorchScriptPipeline = vortex.development.core.pipelines.IRPredictionPipeline
np.set_printoptions(precision=2, suppress=True)
</code></pre>
<hr />
<h2 id="1-model-architecture">1. Model Architecture<a class="headerlink" href="#1-model-architecture" title="Permanent link">&para;</a></h2>
<p>DETR Demo taken from <a href="https://github.com/facebookresearch/detr">facebookresearch/detr</a>'s 
<a href="https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb">google colab demo</a>.
Modified for <code>vortex</code> compatibility by adding <code>task</code> and <code>output_format</code> atrributes to
our model.</p>
<p><code>output_format</code> is a nested <code>dict</code>, with signature of <code>Dict[str,dict]</code>,
with inner <code>dict</code> with signature of <code>Dict[str,Union[list,int]]</code>.  The inner
dictionary is a mapping from output name to arguments that will be used for tensor slicing.
The tensor slicing operation will be performed using <code>np.take</code>, <code>Onnx.Gather</code>, or
<code>torch.index_select</code>; the arguments' naming use numpy's take, that is <code>indices</code> and <code>axis</code>;
check numpy, onnx, or torch docs for more details.</p>
<p>For detection models, we need <code>class_label</code>, <code>class_confidence</code>, and <code>bounding_box</code>.
<code>output_format</code> will be used to slice single sample (image). In case of detection models,
there might be multiple objects for single sample, so the result from single sample is 
2D tensor. Hence, we take the tensor's values at <code>axis=1</code>.</p>
<p>Note that in the official DETR demo, the model's postprocess returned the full probability tensor
but in our case, we want to take only 1D tensor, so we set <code>indices</code> of <code>class_label</code> to <code>[4]</code>
and <code>class_confidence</code> to <code>[5]</code>, these values should be corresponding to our postprocess module's output.</p>
<pre><code class="language-python">class DETRdemo(nn.Module):
    &quot;&quot;&quot;
    Demo DETR implementation.

    Demo implementation of DETR in minimal number of lines, with the
    following differences wrt DETR in the paper:
    * learned positional encoding (instead of sine)
    * positional encoding is passed at input (instead of attention)
    * fc bbox predictor (instead of MLP)
    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.
    Only batch size 1 supported.
    &quot;&quot;&quot;
    def __init__(self, num_classes, hidden_dim=256, nheads=8,
                 num_encoder_layers=6, num_decoder_layers=6):
        super().__init__()

        # create ResNet-50 backbone
        self.backbone = resnet50()
        del self.backbone.fc

        # create conversion layer
        self.conv = nn.Conv2d(2048, hidden_dim, 1)

        # create a default PyTorch transformer
        self.transformer = nn.Transformer(
            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)

        # prediction heads, one extra class for predicting non-empty slots
        # note that in baseline DETR linear_bbox layer is 3-layer MLP
        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)
        self.linear_bbox = nn.Linear(hidden_dim, 4)

        # output positional encodings (object queries)
        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))

        # spatial positional encodings
        # note that in baseline DETR we use sine positional encodings
        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))
        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))

        # attributes for vortex
        self.task = &quot;detection&quot;
        self.output_format = dict(
            class_label=dict(
                indices=[4], axis=1
            ),
            class_confidence=dict(
                indices=[5], axis=1
            ),
            bounding_box=dict(
                indices=[0,1,2,3], axis=1
            )
        )

    def forward(self, inputs):
        # propagate inputs through ResNet-50 up to avg-pool layer
        x = self.backbone.conv1(inputs)
        x = self.backbone.bn1(x)
        x = self.backbone.relu(x)
        x = self.backbone.maxpool(x)

        x = self.backbone.layer1(x)
        x = self.backbone.layer2(x)
        x = self.backbone.layer3(x)
        x = self.backbone.layer4(x)

        # convert from 2048 to 256 feature planes for the transformer
        h = self.conv(x)

        # construct positional encodings
        H, W = h.shape[-2:]
        pos = torch.cat([
            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),
            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),
        ], dim=-1).flatten(0, 1).unsqueeze(1)

        # propagate through the transformer
        # workaround to handle batched input
        src = pos + 0.1 * h.flatten(2).permute(2, 0, 1)
        tgt = self.query_pos.unsqueeze(1).repeat(1, src.size(1), 1)
        h = self.transformer(src,tgt).transpose(0, 1)

        # finally project transformer outputs to class labels and bounding boxes
        return {'pred_logits': self.linear_class(h), 
                'pred_boxes': self.linear_bbox(h).sigmoid()}
</code></pre>
<p>helper function to get pretrained DETR on COCO</p>
<pre><code class="language-python">def coco_detr():
    detr = DETRdemo(num_classes=91)
    state_dict = torch.hub.load_state_dict_from_url(
        url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',
        map_location='cpu', check_hash=True)
    detr.load_state_dict(state_dict)
    detr.eval()
    return detr

# for output bounding box post-processing
def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=1)
</code></pre>
<hr />
<h2 id="2-create-postprocess-module">2. Create PostProcess module<a class="headerlink" href="#2-create-postprocess-module" title="Permanent link">&para;</a></h2>
<p>Next, let's define our post-process module. As the name suggests, this module
will be called after model inference. In this module we will format the model's
output tensor to our format, as well as, decoding the model's output tensor to
perform score thresholding, and bounding box conversion.</p>
<p>Our postprocess module will take two input, the network output 
and additional <code>score_threshold</code> as input. By default, the network output will
be forwarded to postprocess. For our <code>score_threshold</code>, we need to tell vortex
that our postprocess requires additional input, by adding attribute <code>additional_inputs</code>
to our postprocess. The signature is <code>Tuple[Tuple[str,Tuple[int,...]],...]</code>, with
first elements of the sequences are the input names and the seconds are the input shapes.
By doing this, we can change the value even after export to TorchScript or ONNX.</p>
<p>Also note that we introduce workaround for <code>torch==1.4</code>, <code>onnx==1.6</code> and <code>onnxruntime==1.2</code>.
Onnx operator <code>Greater</code> can't have <code>tensor(bool)</code> as input, but torch onnx exporter
won't complain. <code>squeeze</code> and <code>nonzero</code> also inserted to match appropriate dimension
and type for <code>index_select</code> (for slicing).</p>
<pre><code class="language-python">class DETRPostProcess(nn.Module):
    def __init__(self):
        super().__init__()
        # tell vortex that our postprocess has additional input
        # that is score threshold with shape of torch.Size([1])
        self.additional_inputs = (
            ('score_threshold', (1,)),
        )

    def _forward(self, input: torch.Tensor, score_threshold: torch.Tensor) -&gt; torch.Tensor:
        # keep only predictions &gt; score_threshold
        probas = input['pred_logits'].softmax(-1)[:, :-1]

        # workaround for onnx inference (torch 1.4, onnx 1.6, onnxruntime 1.2)
        # op `Greater` can't have tensor(bool) as input
        values, indices = probas.max(-1, keepdim=True)
        keep = torch.gt(values, score_threshold).squeeze(-1).nonzero().squeeze(-1)

        # convert to xyxy
        out_bbox = input['pred_boxes'].index_select(0,keep)
        out_bbox = box_cxcywh_to_xyxy(out_bbox)

        # take confidence and classes and match bbox dim
        # probas = probas[keep]
        probas = probas.index_select(0, keep)
        conf_label, cls_label = probas.max(dim=1, keepdim=False)
        conf_label.unsqueeze_(-1)
        cls_label.unsqueeze_(-1)

        # concat input
        output = torch.cat((out_bbox,cls_label.float(),conf_label),-1)
        return output

    def forward(self, input: torch.Tensor, score_threshold: torch.Tensor) -&gt; torch.Tensor:
        num_batch = input['pred_logits'].size(0)
        results = []
        # workaround to handle batched input,
        # `num_batch` will be static after export
        for i in range(num_batch):
            pred_input = {key: value[i] for key, value in input.items()}
            result = self._forward(pred_input, score_threshold)
            results.append(result)
        return tuple(results)
</code></pre>
<hr />
<h2 id="3-registering-our-model-to-vortex">3. Registering our model to vortex<a class="headerlink" href="#3-registering-our-model-to-vortex" title="Permanent link">&para;</a></h2>
<p>After defining necessary components, we need to define <em>builder</em> function
that will create instances of necessary components, then we register it to vortex
by decorating the function and providing the model name.</p>
<p>This function serves as entry point and customization point for our loss, model,
and post-process, and optionally preprocess and dataset collater. For example, you
need to reuse parameter from networks to postprocess, you can do it here.</p>
<p>The required components are <code>preprocess</code>, <code>network</code>, and <code>postprocess</code>.
For training, we need additional components, <code>loss</code> and optional <code>collate_fn</code>.
For preprocess, it is recommended to use modules from <code>vortex_modules.preprocess</code> to 
make sure it is exportable.</p>
<p>Here, we will also perform some <em>hack</em> to keep pretrained weights. And note that since
we will implements inference only (for now), we will just reject training stage.</p>
<pre><code class="language-python">@networks.models.register_model(model_name='detr')
def create_model_components(
    model_name, preprocess_args,
    network_args, loss_args,
    postprocess_args, stage) -&gt; dict:

    pretrained = network_args.pop('pretrained',False)
    num_classes = network_args.pop('num_classes',91)

    if pretrained:

        # make sure our config match available pretrained
        assert network_args.get('nheads',8)==8 and          \
            network_args.get('hidden_dim',256)==256 and     \
            network_args.get('num_encoder_layers',6)==6 and \
            network_args.get('num_decoder_layers',6)==6,    \
            &quot;pretrained not available&quot;

        # start from pretrained
        model = coco_detr()

        # looking at the implementation, we restore number of class if necessary
        if num_classes != 91:
            model.linear_class = nn.Linear(hidden_dim, num_classes + 1)

        # override preprocess to match with pretrained preprocess
        preprocess_args = {
                'input_size': 800,
                'input_normalization': {
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225],
                'scaler': 255,
            }
        }
    else:
        model = DETRdemo(**network_args)

    # using vortex' preprocess module
    normalizer = 'normalizer'
    if pretrained:
        normalizer = 'flip_normalizer' # rgb2bgr

    preprocess = vortex_modules.preprocess.get_preprocess(
        normalizer, **preprocess_args
    )

    # instantiate our DETR postprocess
    postprocess = DETRPostProcess(**postprocess_args)

    # wrap-up core components
    components = dict(
        preprocess=preprocess,
        network=model,
        postprocess=postprocess,
    )

    # current implemenetation doesn't support training
    if stage == 'train':
        raise NotImplementedError(&quot;current implementation doesn't support training yet&quot;)

    return components
</code></pre>
<hr />
<h2 id="4-integration-with-vortex-cli">4. Integration with vortex CLI<a class="headerlink" href="#4-integration-with-vortex-cli" title="Permanent link">&para;</a></h2>
<p>We'll provide entry-point for our experiments to get more vortex' features
in a <em>configuratble</em> way. Doing this is as simple as calling <code>vortex_cli</code>'s
main function and we are all set. To separate this from demo program, let's
wrap it on <code>cli()</code> and create another python scripts calling this <code>cli()</code> as
our entry-point (see <code>cli.py</code>)</p>
<pre><code class="language-python">def cli():
    vortex.development.vortex_cli.main()
</code></pre>
<hr />
<h2 id="5-demo-program">5. Demo program<a class="headerlink" href="#5-demo-program" title="Permanent link">&para;</a></h2>
<p>Prepare some files</p>
<pre><code class="language-python">run_demo = False
if __name__ == '__main__':
    run_demo = True

from pathlib import Path

def download_image(url: str, filename: str):
    path = Path(filename)
    try:
        Image.open(path)
    except:
        with path.open('wb+') as f:
            f.write(requests.get(url).content)
    return filename

images = [
    dict(
        url='http://images.cocodataset.org/val2017/000000039769.jpg',
        filename='000000039769.jpg'
    ),
    dict(
        url='http://farm7.staticflickr.com/6153/6162098289_512fc331dc_z.jpg',
        filename='6162098289_512fc331dc_z.jpg'
    ),
    dict(
        url='http://farm9.staticflickr.com/8539/8640225449_73c750f686_z.jpg',
        filename='8640225449_73c750f686_z.jpg'
    ),
    dict(
        url='http://farm5.staticflickr.com/4008/4295843111_bee8c2f586_z.jpg',
        filename='4295843111_bee8c2f586_z.jpg'
    )
]

downloaded_images = list(map(lambda args: download_image(**args), images))
</code></pre>
<p>Create helper function for detr demo. You can run this demo using 
<code>examples/custom_models/detr/detr.py</code> or <code>examples/custom_models/detr/detr.ipynb</code>.</p>
<pre><code class="language-python">def detr_demo():
    def rescale_bboxes(out_bbox, size):
        img_w, img_h = size
        b = box_cxcywh_to_xyxy(out_bbox)
        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)
        return b

    def detect(im, model, transform):
        # mean-std normalize the input image (batch-size: 1)
        img = transform(im).unsqueeze(0)

        # propagate through the model
        outputs = model(img)

        # keep only predictions with 0.7+ confidence
        probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]
        keep = probas.max(-1).values &gt; 0.7

        # convert boxes from [0; 1] to image scales
        bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)
        return probas[keep], bboxes_scaled

    def plot_results(pil_img, prob, boxes):
        plt.figure(figsize=(16,10))
        plt.imshow(pil_img)
        ax = plt.gca()
        for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):
            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                    fill=False, color=c, linewidth=3))
            cl = p.argmax()
            text = f'{CLASSES[cl]}: {p[cl]:0.2f}'
            ax.text(xmin, ymin, text, fontsize=15,
                    bbox=dict(facecolor='yellow', alpha=0.5))
        plt.axis('off')
        plt.show()
    # COCO classes
    CLASSES = [
        'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
        'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',
        'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',
        'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',
        'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',
        'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
        'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',
        'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',
        'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
        'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',
        'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',
        'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',
        'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
        'toothbrush'
    ]
    # colors for visualization
    COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],
            [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]

    # standard PyTorch mean-std input image normalization
    transform = T.Compose([
        T.Resize(800),
        T.ToTensor(),
        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    detr = coco_detr()

    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
    im = Image.open(requests.get(url, stream=True).raw)

    scores, boxes = detect(im, detr, transform)

    plot_results(im, scores, boxes)
</code></pre>
<p>Create helper function for detr demo using <code>vortex</code></p>
<pre><code class="language-python">def load_config(cfg='detr.yml'):
    import yaml, os, pathlib, easydict

    with open(cfg) as f:
        config = easydict.EasyDict(yaml.load(f))

    # prepare path
    TORCH_HOME = os.getenv('TORCH_HOME', os.environ['HOME'] + '/.cache/torch/')
    weights_file = pathlib.Path(TORCH_HOME)
    weights_file = weights_file / 'checkpoints' / 'detr_demo-da2a99e9.pth'

    return dict(
        config=config,
        weights_file=weights_file
    )

def visualize(results):
    for visualization, prediction in zip(results.visualization, results.prediction) :
        # visualization
        plt.figure()
        plt.imshow(cv2.cvtColor(visualization,cv2.COLOR_BGR2RGB))

        # print results
        print('prediction results:')
        print(prediction)

def vortex_demo(cfg='detr.yml',test_images='000000039769.jpg', score_threshold=0.7):
    &quot;&quot;&quot;
    helper function to run torch inference on vortex
    &quot;&quot;&quot;

    cfg = load_config(cfg)
    config = cfg['config']
    weights_file = cfg['weights_file']

    # create predictor
    predictor = Pipeline(config, weights_file, 'cuda' if torch.cuda.is_available() else 'cpu')

    # run prediction
    test_images = test_images
    if isinstance(test_images, str):
        test_images = [test_images]
    run_args = dict(
        dump_visual=True, visualize=True, 
        score_threshold=score_threshold
    )
    results = predictor.run(test_images, **run_args)

    visualize(results)

def vortex_export(cfg='detr.yml', test_images='000000039769.jpg'):
    &quot;&quot;&quot;
    helper function to export detr to ONNX &amp; TorchScript
    &quot;&quot;&quot;
    cfg = load_config(cfg)
    config = cfg['config']
    weights_file = cfg['weights_file']

    exporter = ExportPipeline(config, weights=weights_file)
    result = exporter.run(example_input=test_images)

def torchscript_demo(model=None, runtime='cpu', test_images='000000039769.jpg', score_threshold=0.7):
    &quot;&quot;&quot;
    helper function to run torchscript inference on vortex
    &quot;&quot;&quot;

    if model is None:
        model = 'experiments/outputs/detr/detr-bs4.pt'

    # create predictor
    predictor = TorchScriptPipeline(model, runtime=runtime)

    # run prediction
    # run prediction
    test_images = test_images
    if isinstance(test_images, str):
        test_images = [test_images]
    run_args = dict(
        dump_visual=True, visualize=True, 
        score_threshold=score_threshold
    )
    results = predictor.run(test_images, **run_args)

    visualize(results)

def onnx_demo(model=None, runtime='cpu', test_images='000000039769.jpg', score_threshold=0.7):
    &quot;&quot;&quot;
    helper function to run onnx inference on vortex
    &quot;&quot;&quot;

    if model is None:
        model = 'experiments/outputs/detr/detr.onnx'

    # create predictor
    predictor = ONNXPipeline(model, runtime=runtime)

    # run prediction
    test_images = test_images
    if isinstance(test_images, str):
        test_images = [test_images]
    run_args = dict(
        dump_visual=True, visualize=True, 
        score_threshold=score_threshold,
    )
    results = predictor.run(test_images, **run_args)

    visualize(results)
</code></pre>
<p><code>DE:TR</code> Demo using original demo code</p>
<pre><code class="language-python">detr_demo() if run_demo else 'skip detr demo'
</code></pre>
<p><code>DE:TR</code> Demo using <code>vortex</code></p>
<pre><code class="language-python">vortex_demo(test_images=downloaded_images) if run_demo else 'skip detr demo'
</code></pre>
<p>Export <code>DE:TR</code> to <code>ONNX</code> and <code>TorchScript</code> using vortex</p>
<pre><code class="language-python">vortex_export() if run_demo else 'skip detr export'
</code></pre>
<p>TorchScript Inference using exported graph</p>
<pre><code class="language-python">torchscript_demo(test_images=downloaded_images) if run_demo else 'skip torchscript demo'
</code></pre>
<p>ONNX Inference using exported graph</p>
<pre><code class="language-python"># skip for now
# onnx_demo() if run_demo else 'skip onnx demo'
</code></pre></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
